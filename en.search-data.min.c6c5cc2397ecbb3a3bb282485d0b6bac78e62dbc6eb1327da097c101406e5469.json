[{"id":0,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E8%B5%84%E6%BA%90%E5%9B%9E%E6%94%B6/redisLazyFree/","title":"Redis LazyFree","section":"资源回收","content":"\n场景 [1]\r#\r场景一：客户端执行的显示删除/清除命令，比如 del，flushdb 等； 场景二：某些指令带有的隐式删除命令，比如 move , rename 等； 场景三：到达过期时间的数据需要删除； 场景四：使用内存达到 maxmemory 后被选出来要淘汰的数据需要删除； 场景五：在主从同步全量同步阶段，从库收到主库的 RDB 文件后要先删除现有的数据再加载 RDB 文件； 相关配置\r#\r惰性删除相 关的配置项 lazyfree-lazy-eviction：对应缓存淘汰时的数据删除场景。 lazyfree-lazy-expire：对应过期 key 的删除场景。 lazyfree-lazy-server-del：对应会隐式进行删除操作的 server 命令执行场景。 replica-lazy-flush：对应从节点完成全量同步后，删除原有旧数据的场景。 源代码\r#\revict.c int freeMemoryIfNeeded(void) { /*......*/ /* Finally remove the selected key. */ if (bestkey) { db = server.db+bestdbid; robj *keyobj = createStringObject(bestkey,sdslen(bestkey)); propagateExpire(db,keyobj,server.lazyfree_lazy_eviction); ### 1 /* We compute the amount of memory freed by db*Delete() alone. * It is possible that actually the memory needed to propagate * the DEL in AOF and replication link is greater than the one * we are freeing removing the key, but we can\u0026#39;t account for * that otherwise we would never exit the loop. * * AOF and Output buffer memory will be freed eventually so * we only care about memory used by the key space. */ delta = (long long) zmalloc_used_memory(); //获取当前内存使用量 latencyStartMonitor(eviction_latency); if (server.lazyfree_lazy_eviction) /////. 如果 lazyfree_lazy_eviction 被设置为 1，也就是启用了缓存淘汰时的惰性删除， dbAsyncDelete(db,keyobj); /////. 那么，删除操作对应的命令就是 UNLINK； else dbSyncDelete(db,keyobj); /////. 否则的话，命令就是 DEL。 /*......*/ delta -= (long long) zmalloc_used_memory(); ///根据当前内存使用量计算数据删除前后释放.... mem_freed += delta; //更新已释放的内存量 /*......*/ } db.c ### 1 /* Propagate expires into slaves and the AOF file. * When a key expires in the master, a DEL operation for this key is sent * to all the slaves and the AOF file if enabled. * * This way the key expiry is centralized in one place, and since both * AOF and the master-\u0026gt;slave link guarantee operation ordering, everything * will be consistent even if we allow write operations against expiring * keys. */ void propagateExpire(redisDb *db, robj *key, int lazy) { robj *argv[2]; argv[0] = lazy ? shared.unlink : shared.del; // 如果server启用了lazyfree-lazy argv[1] = key; //被淘汰的key对象 incrRefCount(argv[0]); incrRefCount(argv[1]); if (server.aof_state != AOF_OFF) /// 是否启用了 AOF 日志 /// 如果启用了AOF日志 feedAppendOnlyFile(server.delCommand,db-\u0026gt;id,argv,2); // 把被淘汰 key 的删除操作记录到 AOF 文件中，以保证后续使用 AOF 文件进行 Redis 数据库恢复时，可以和恢复前保持一致 replicationFeedSlaves(server.slaves,db-\u0026gt;id,argv,2); //// 把删除操作同步给从节点，以保证主从节点的数据一致 decrRefCount(argv[0]); decrRefCount(argv[1]); } 子操作一 子操作一：将被淘汰的键值对从哈希表中去除，这里的哈希表既可能是设置了过期 key 的哈希表，也可能是全局哈希表。 子操作二：释放被淘汰键值对所占用的内存空间。 子操作一 /* Search and remove an element. This is an helper function for * dictDelete() and dictUnlink(), please check the top comment * of those functions. */ static dictEntry *dictGenericDelete(dict *d, const void *key, int nofree) { ... ... h = dictHashKey(d, key); //计算key的哈希值 for (table = 0; table \u0026lt;= 1; table++) { idx = h \u0026amp; d-\u0026gt;ht[table].sizemask; //根据key的哈希值获取它所在的哈希桶编号 he = d-\u0026gt;ht[table].table[idx]; //获取key所在哈希桶的第一个哈希项 prevHe = NULL; while(he) { //在哈希桶中逐一查找被删除的key是否存在 if (key==he-\u0026gt;key || dictCompareKeys(d, key, he-\u0026gt;key)) { /* Unlink the element from the list */ //如果找见被删除key了，那么将它从哈希桶的链表中去除 if (prevHe) prevHe-\u0026gt;next = he-\u0026gt;next; else d-\u0026gt;ht[table].table[idx] = he-\u0026gt;next; if (!nofree) { //如果要同步删除，那么就释放key和value的内存空间 dictFreeKey(d, he); //调用dictFreeKey释放 dictFreeVal(d, he); zfree(he); } d-\u0026gt;ht[table].used--; return he; } prevHe = he; he = he-\u0026gt;next; //当前key不是要查找的key，再找下一个 } ...... } /* Remove an element, returning DICT_OK on success or DICT_ERR if the * element was not found. */ int dictDelete(dict *ht, const void *key) { return dictGenericDelete(ht,key,0) ? DICT_OK : DICT_ERR; } /* Remove an element from the table, but without actually releasing * the key, value and dictionary entry. The dictionary entry is returned * if the element was found (and unlinked from the table), and the user * should later call `dictFreeUnlinkedEntry()` with it in order to release it. * Otherwise if the key is not found, NULL is returned. * * This function is useful when we want to remove something from the hash * table but want to use its value before actually deleting the entry. * Without this function the pattern would require two lookups: * * entry = dictFind(...); * // Do something with entry * dictDelete(dictionary,entry); * * Thanks to this function it is possible to avoid this, and use * instead: * * entry = dictUnlink(dictionary,entry); * // Do something with entry * dictFreeUnlinkedEntry(entry); // \u0026lt;- This does not need to lookup again. */ dictEntry *dictUnlink(dict *ht, const void *key) { return dictGenericDelete(ht,key,1); } 子操作二 基于异步删除的数据淘汰 dbAsyncDelete /* Delete a key, value, and associated expiration entry if any, from the DB. * If there are enough allocations to free the value object may be put into * a lazy free list instead of being freed synchronously. The lazy free list * will be reclaimed in a different bio.c thread. */ #define LAZYFREE_THRESHOLD 64 int dbAsyncDelete(redisDb *db, robj *key) { /* Deleting an entry from the expires dict will not free the sds of * the key, because it is shared with the main dictionary. */ if (dictSize(db-\u0026gt;expires) \u0026gt; 0) dictDelete(db-\u0026gt;expires,key-\u0026gt;ptr); /// 在过期 key 的哈希表中同步删除被淘汰的键值对 /* If the value is composed of a few allocations, to free in a lazy way * is actually just slower... So under a certain limit we just free * the object synchronously. */ dictEntry *de = dictUnlink(db-\u0026gt;dict,key-\u0026gt;ptr); /// 在全局哈希表中异步删除被淘汰的键值对 if (de) { robj *val = dictGetVal(de); size_t free_effort = lazyfreeGetFreeEffort(val); /* If releasing the object is too much work, do it in the background * by adding the object to the lazy free list. * Note that if the object is shared, to reclaim it now it is not * possible. This rarely happens, however sometimes the implementation * of parts of the Redis core may call incrRefCount() to protect * objects, and then call dbDelete(). In this case we\u0026#39;ll fall * through and reach the dictFreeUnlinkedEntry() call, that will be * equivalent to just calling decrRefCount(). */ if (free_effort \u0026gt; LAZYFREE_THRESHOLD \u0026amp;\u0026amp; val-\u0026gt;refcount == 1) { /// 计算释放被淘汰键值对内存空间的开销///当被淘汰键值对是包含超过 64 个元素的集合类型时 atomicIncr(lazyfree_objects,1); bioCreateBackgroundJob(BIO_LAZY_FREE,val,NULL,NULL); /// 会调用 bioCreateBackgroundJob 函数，来实际创建后台任务执行惰性删除 dictSetVal(db-\u0026gt;dict,de,NULL); } } /* Release the key-val pair, or just the key if we set the val * field to NULL in order to lazy free it later. */ if (de) { dictFreeUnlinkedEntry(db-\u0026gt;dict,de); if (server.cluster_enabled) slotToKeyDel(key); return 1; } else { return 0; } } /* Return the amount of work needed in order to free an object. * The return value is not always the actual number of allocations the * object is compoesd of, but a number proportional to it. * * For strings the function always returns 1. * * For aggregated objects represented by hash tables or other data structures * the function just returns the number of elements the object is composed of. * * Objects composed of single allocations are always reported as having a * single item even if they are actually logical composed of multiple * elements. * * For lists the function returns the number of elements in the quicklist * representing the list. */ size_t lazyfreeGetFreeEffort(robj *obj) { if (obj-\u0026gt;type == OBJ_LIST) { quicklist *ql = obj-\u0026gt;ptr; return ql-\u0026gt;len; } else if (obj-\u0026gt;type == OBJ_SET \u0026amp;\u0026amp; obj-\u0026gt;encoding == OBJ_ENCODING_HT) { dict *ht = obj-\u0026gt;ptr; return dictSize(ht); } else if (obj-\u0026gt;type == OBJ_ZSET \u0026amp;\u0026amp; obj-\u0026gt;encoding == OBJ_ENCODING_SKIPLIST){ zset *zs = obj-\u0026gt;ptr; return zs-\u0026gt;zsl-\u0026gt;length; } else if (obj-\u0026gt;type == OBJ_HASH \u0026amp;\u0026amp; obj-\u0026gt;encoding == OBJ_ENCODING_HT) { dict *ht = obj-\u0026gt;ptr; return dictSize(ht); } else if (obj-\u0026gt;type == OBJ_STREAM) { size_t effort = 0; stream *s = obj-\u0026gt;ptr; /* Make a best effort estimate to maintain constant runtime. Every macro * node in the Stream is one allocation. */ effort += s-\u0026gt;rax-\u0026gt;numnodes; /* Every consumer group is an allocation and so are the entries in its * PEL. We use size of the first group\u0026#39;s PEL as an estimate for all * others. */ if (s-\u0026gt;cgroups) { raxIterator ri; streamCG *cg; raxStart(\u0026amp;ri,s-\u0026gt;cgroups); raxSeek(\u0026amp;ri,\u0026#34;^\u0026#34;,NULL,0); /* There must be at least one group so the following should always * work. */ serverAssert(raxNext(\u0026amp;ri)); cg = ri.data; effort += raxSize(s-\u0026gt;cgroups)*(1+raxSize(cg-\u0026gt;pel)); raxStop(\u0026amp;ri); } return effort; } else { return 1; /* Everything else is a single allocation. */ } } 子操作二 基于同步删除的数据淘汰 dbSyncDelete 参考\r#\rredis惰性删除 lazy free 源码剖析，干货满满 "},{"id":1,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/distributedDatabase/","title":"分布式 数据库 总结","section":"分布式","content":"\n参考\r#\r\u0026laquo;分布式数据库30讲\u0026raquo;\n{% post_link \u0026lsquo;distributedDatabaseGlobalTime\u0026rsquo; %} self\n"},{"id":2,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%BA%94%E7%94%A8/redisUseCase/","title":"Redis 使用场景UseCase","section":"Redis 应用","content":"\n缓存 分布式锁 计数器 限流 购物车 用户消息时间线timeline 消息队列 点赞、签到、打卡 商品标签 用户关注、推荐模型 排行榜 参考\r#\rRedis 16 个常见的使用场景\n"},{"id":3,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/redisRehash/","title":"Redis Rehash机制","section":"数据类型","content":"\n什么时候触发 rehash？\r#\r，_dictExpandIfNeeded 函数中定义了三个扩容条件。 下面的代码就展示了 _dictExpandIfNeeded 函数对这三个条件的定义，你可以看下。 条件一：ht[0]的大小为 0。 条件二：ht[0]承载的元素个数已经超过了 ht[0]的大小，同时 Hash 表可以进行扩容。 条件三：ht[0]承载的元素个数，是 ht[0]的大小的 dict_force_resize_ratio 倍，其中， dict_force_resize_ratio 的默认值是 5。\nrehash 扩容扩多大？\r#\r对 Hash表扩容的思路也很简单，就是如果当前表的已用空间大小为 size，那么就将表扩容到 size*2 的大小。\nrehash 如何执行？\r#\r其实这是因为，Hash 表在执行 rehash 时，由于 Hash 表空间扩大，原本映射到某一位置 的键可能会被映射到一个新的位置上，因此，很多键就需要从原来的位置拷贝到新的位 置。而在键拷贝时，由于 Redis 主线程无法执行其他请求，所以键拷贝会阻塞主线程，这 样就会产生 rehash 开销。\n而为了降低 rehash 开销，Redis 就提出了渐进式 rehash 的方法。\ndictRehash 的主要执行流程\r#\r"},{"id":4,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/Arch-Redis-Cluster/redisArch/","title":"Redis 架构","section":"Arch \u0026 Redis Cluster","content":"\n目录\r#\rRedis架构演进[1]\r#\r数据怕丢失 -\u0026gt; 持久化（RDB/AOF） 恢复时间久 -\u0026gt; 主从副本（副本随时可切） 故障手动切换慢 -\u0026gt; 哨兵集群（自动切换） 读存在压力 -\u0026gt; 扩容副本（读写分离） 写存在压力/容量瓶颈 -\u0026gt; 分片集群 分片集群社区方案 -\u0026gt; Twemproxy、Codis（Redis 节点之间无通信，需要部署哨兵，可横向扩容） 分片集群官方方案 -\u0026gt; Redis Cluster （Redis 节点之间 Gossip 协议，无需部署哨兵，可横向扩容） 业务侧升级困难 -\u0026gt; Proxy + Redis Cluster（不侵入业务侧） 主从架构\r#\r主从同步 异步方式来同步数据 最终一致性 分类 增量同步 （同步的是指令） 1.指令记录在master本地的内存 buffer 2.异步将 buffer 中的指令同步到从节点 类似 AOF 快照同步（同步的是内容，全量同步） bgsave存磁盘rdb，然后rdb发到slave，slave装载 优化: 2.8.18 版开始支持无盘复制 哨兵集群 sentinel\r#\rmaster-slave异步复制, 所以会丢消息\n参数: # 至少有一个slave复制 min-slaves-to-write 1 # slave节点最大10s的延迟 min-slaves-max-lag 10 Redis cluster\r#\r整体架构\n去中心化的; 所有数据划分为16384个slots，每个节点负责其中一部分slots; 容错\n主从容错，主升从。 PFail（Possibly Fail） -\u0026gt; 临时不可用 Fail -\u0026gt; 确定不可用， PFail Count\u0026gt; 集群的1/2 Gossip协议 集群节点采用 Gossip 协议来广播自己的状态以及自己对整个集群认知的改变; 可能下线 (PFAIL-Possibly Fail) \u0026amp;\u0026amp; 确定下线 (Fail)\nslot迁移 在迁移过程中，客户端访问的流程会有很大的变化。 迁移是会影响服务效率的，同样的指令在正常情况下一个 ttl 就能完成，而在迁移中得 3 个 ttl 才能搞定。\n网络抖动\n# 表示当某个节点持续 timeout 的时间失联时，才可以认定该节点出现故障 cluster-node-timeout 槽位迁移感知 2个error指令 1.MOVED错误： 用来纠正槽位 槽的负责权已经从一个节点转移到了另一节点 2. ASK错误， ASKING命令： 用来临时纠正槽位 只是两个节点在迁移槽的过程中使用的一种临时措施\n参考\r#\r一文搞懂 Redis 架构演化之路 腾讯 *** 《Redis 深度历险：核心原理与应用实践》 钱文品 3. 原理 8：有备无患 —— 主从同步 4. 原理 3：未雨绸缪 —— 持久化 5. 集群 1：李代桃僵 —— Sentinel 6. 集群 3：众志成城 —— Cluster\n"},{"id":5,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98/cacheConsistent/","title":"缓存 一致性","section":"缓存","content":"\n关键词\r#\r失效策略(缓存更新)、 缓存与数据库的一致性\n失效策略(缓存更新) [9]\r#\rRead/Write Through模式\r#\rRead Through : 被动失效 Write Through ： 主动失效\nCache Aside 模式\r#\rCache Aside 模式和上面的 Read/Write Through 模式非常像，它们处理读请求的逻辑是完全一样的，唯一的一个小差别就是，Cache Aside 模式在更新数据的时候，并不去尝试更新缓存，而是去删除缓存。 普遍使用这种方式\n场景： 订单服务收到更新数据请求之后，先更新数据库，如果更新成功了，再尝试去删除缓存中订 单，如果缓存中存在这条订单就删除它，如果不存在就什么都不做，然后返回更新成功。这 条更新后的订单数据将在下次被访问的时候加载到缓存中。使用 Cache Aside 模式来更新 缓存，可以非常有效地避免并发读写导致的脏数据问题。\nWrite Behind模式\r#\rlinux page cache: dirty page刷盘。 kafka使用page cache异步刷盘。\n失效策略\r#\r失效策略 被动失效 有空窗期问题 主动更新【1】 无空窗期问题 有并发更新问题。并发写，数据覆盖一致性问题【2】 锁控制,少有这样做的 客户端读写锁 服务端加锁 版本控制 单版本机制【3】 多版本机制 类似向量时钟， NoSQL使用 消息机制【3,4】 增量db数据通过消息来异步变更缓存的数据 缓存的应用模式\r#\r缓存的应用模式 Cache-Aside 业务代码直接维护缓存 有并发更新问题【2】 Cache-As-SoR Read-Through Write-Through 同步写 Write-Behind 异步写 Cache与数据库的一致性\r#\rCache与数据库的一致性 读操作 先读缓存，再读数据库 写操作 先写数据库，在写缓存 缓存与数据库的数据同步 一致性\r#\r在应用层， 可以根据业务场景对一致性的要求不同， 给数据分配不同的队列，即 一致性分级队列。 强一致性的场景如自己发布的评论， 自己应该及时看到。 而别人看到我的评论属于会话一致性， 一致性要求比较弱。 这样可以把一致性要求高的业务分配更多资源， 做到快速同步。\n缓存与数据库同步的异步化也是提高响应度的方式， 在write-behind的方式中，所有的数据的操作都在缓存中， 更新的数据并不会立即传到数据库。相反，在缓存中一旦进行更新操作，缓存就会跟踪脏记录列表，并定期将当前的脏记录集刷新到数据库中。\n在DAO层的hibernate针对一致性的要求提出了类似DBMS的事务级别的4个配置项。 非严格读写型(nonstrict-read-write)策略提供弱一致性，不保证缓存与数据库中数据的一致性。如果存在两个事务同时访问缓存中相同数据的可能，必须为该数据配置一个很短的数据过期时间，从而尽量避免脏读。对于极少被修改，并且允许偶尔脏读的数据，可以采用这种并发访问策略。 事务策略(transactional )只可用于托管环境，如有必要，它还保证完全的事务隔离级别直到可重复读。事务策略可以用在强一致性的场景中。\n过期策略\r#\r被动过期 （一致性低， 缓存超时）\r#\r对一致性要求较低的系统，可以采用常规的缓存超时策略，此类策略属于被动过期。 存放数据时，永不过期的数据不要与有过期策略的数据放在一起， 早期的版本memcache曾经有一个这样的bug， 永不过期的数据被有过期策略的数据踢走了。不要把所有的数据的过期时间设为同一个时间， 这样可能造成大规模的数据同时过期，hit rate变小， 对数据库的查询数瞬时变大，造成数据库的压力。\n主动过期（一致性高， 事件过期， 异步过期， 变化事件）\r#\r对一致性要求较高的系统，可以采用事件过期策略，此类策略属于主动过期。 某个组件的结构发生变化，或者某个业务对象状态发生变化，把组件id或业务对象id放入过期队列中， 缓存节点异步读取这些数据， 将对应cache的对象移除。亦可把变化封装成事件放入过期队列中， 由代理处理这个事件， 异步的移除相应的缓存。\n基于版本的过期方式\r#\r在存储空间较大的前提下，借鉴mvcc的概念，每次更改数据时增加一个副本，并带版本号元数据。 然后由一个代理定时的删除低版本的过期的数据。\n## 服务端缓存\n缓存服务端常用的有memcache。 Nosql的解决方案有Redies，Redies在作为cache时往往配置为无持久化的形式 。两者数据模型都是key-value的。 Redies比老牌的memcache能提供更好的性能， 更快的速度。 Memcache 没有自建的replicaion 机制, 可靠性需要在客户端以双写支持。 Redies可以看成自带持久化机制的Write-back缓存，在write-behind缓存中，数据的读取和更新通过缓存进行，与write-through缓存不同，更新的数据并不会立即持久化。相反，在缓存中一旦进行更新操作，缓存就会跟踪脏记录列表，并定期将当前的脏记录集刷新到外部存储中， 在Redies中这种机制叫做AOF\n参考\r#\r应用系统数据缓存设计 淘宝技术部 *** 失效 Local Cache的小TIP 阿里 放翁（文初） 阿里云分布式缓存OCS与DB之间的数据一致性 杨成虎 缓存失效竟然可以这么解？ serana_cai xxx 《亿级流量网站架构核心技术》 第9章 张开涛 xxx 缓存更新的套路 coolshell *** 《后端存储实战课 - MySQL如何应对高并发（一）：使用缓存保护MySQL》 李玥 浅谈缓存最终一致性的解决方案 腾讯 未 "},{"id":6,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisBigKey/","title":"Redis 大Key","section":"常见问题","content":"\nRedis 大Key\r#\r什么是 Bigkey [5]\r#\r【字符串类型】： 单个string类型的value值超过1MB，就可以认为是Bigkey。 【非字符串类型】：哈希、列表、集合、有序集合等， 它们的元素个数超过2000个，就可以认为是Bigkey。\n问题\r#\r问题1 [1]\n大key在数据迁移[遇到过]、扩容、删除[遇到过]时会有卡顿。 问题2 [3]\n执行大key命令的客户端本身，耗时明显增加，甚至超时 执行大key相关读取或者删除操作时，会严重占用带宽和CPU，影响其他客户端 大key本身的存储带来分布式系统中分片数据不平衡，CPU使用率也不平衡 大key有时候也是热key，读取操作频繁，影响面会很大 执行大key删除时，在低版本redis中可能阻塞线程 [遇到过] 建议 [6]\r#\rbigKey 容量\nstring类型控制在10KB以内 hash、list、set、zset元素个数不要超过5000 非字符串的bigkey，不要使用del删除，使用hscan、sscan、zscan方式渐进式删除\n防止bigkey过期时间自动删除问题(例如一个200万的zset设置1小时过期，会触发del操作，造成阻塞，而且该操作不会不出现在慢查询中(latency可查))，查找方法和删除方法\n查找 大Key\r#\r\u0026ndash;bigkeys参数, 用scan扫描大key [1][2][5] $ redis-cli --bigkeys # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest zset found so far \u0026#39;testzset\u0026#39; with 129 members [00.00%] Biggest hash found so far \u0026#39;h2\u0026#39; with 513 fields [00.00%] Biggest set found so far \u0026#39;si1\u0026#39; with 5 members [00.00%] Biggest hash found so far \u0026#39;h4\u0026#39; with 514 fields [00.00%] Biggest string found so far \u0026#39;key\u0026#39; with 9 bytes -------- summary ------- Sampled 9 keys in the keyspace! Total key length in bytes is 27 (avg len 3.00) Biggest string found \u0026#39;key\u0026#39; has 9 bytes Biggest set found \u0026#39;si1\u0026#39; has 5 members Biggest hash found \u0026#39;h4\u0026#39; has 514 fields Biggest zset found \u0026#39;testzset\u0026#39; has 129 members 1 strings with 9 bytes (11.11% of keys, avg size 9.00) 0 lists with 0 items (00.00% of keys, avg size 0.00) 2 sets with 8 members (22.22% of keys, avg size 4.00) 4 hashs with 1541 fields (44.44% of keys, avg size 385.25) 2 zsets with 132 members (22.22% of keys, avg size 66.00) 0 streams with 0 entries (00.00% of keys, avg size 0.00) 注意点 建议在slave节点执行，因为\u0026ndash;Bigkeys也是通过scan完成的，可能会对节点造成阻塞。 建议在节点本机执行，这样可以减少网络开销。 如果没有从节点，可以使用\u0026ndash;i参数，例如(\u0026ndash;i 0.1 代表100毫秒执行一次)。 \u0026ndash;Bigkeys只能计算每种数据结构的top1，如果有些数据结构有比较多的Bigkey，是查找不出来的。 \u0026ndash;bigkeys的不足 [7] 想查询大于10kb的所有key， \u0026ndash;bigkeys参数就无能为力了。需要用到memory usage命令来计算每个键值的字节数\nMEMORY USAGE key \u0026gt; SET cento 01234567890123456789012345678901234567890123 45678901234567890123456789012345678901234567890123456789 OK \u0026gt; MEMORY USAGE cento (integer) 153 离线分析RDB [3] 使用redis-rdb-tools离线分析工具来扫描RDB持久化文件\nredis-rdb-tools\n解决方案 [3]\r#\r可删除\n渐进式删除\u0026lt;4.0版本 scan命令 游标式迭代扫描 使用 hscan、sscan、zscan 方式渐进式删除 [4] 惰性删除\u0026gt;4.0版本 [7][8] unlink命令 异步惰性非阻塞删除 unlink \u0026lt;keyName\u0026gt; lazyfree-lazy-server-del：Yes(default no) replica-lazy-flush：Yes(default no) lazyfree-lazy-user-del: Yes(default no) （6.0 新增） 不可删除\n拆分 字符串类型 比如商品信息，根据的类别拆分 key。 集合类元素 按照日期拆分，key20220101、key20220102 压缩 Value 数据\n使用序列化、压缩算法将 Key 的大小控制在合理范围内，但是需要注意，序列化、反序列化都会带来一定的消耗 scan命令\r#\rscan命令[1] zset -\u0026gt; zscan ZSCAN：命令用于迭代 zset 中的元素（包括元素成员和元素分值）。 hash -\u0026gt; hscan HSCAN：命令用于迭代 hash 中的键值对。 set -\u0026gt; sscan SSCAN：命令用于迭代 set 中的元素。 key of hash -\u0026gt; scan SCAN：命令用于迭代 数据库键。 127.0.0.1:6379\u0026gt; keys * 1) \u0026#34;db_number\u0026#34; 2) \u0026#34;key1\u0026#34; 3) \u0026#34;myKey\u0026#34; 127.0.0.1:6379\u0026gt; scan 0 MATCH * COUNT 1 1) \u0026#34;2\u0026#34; 2) 1) \u0026#34;db_number\u0026#34; 127.0.0.1:6379\u0026gt; scan 2 MATCH * COUNT 1 1) \u0026#34;1\u0026#34; 2) 1) \u0026#34;myKey\u0026#34; 127.0.0.1:6379\u0026gt; scan 1 MATCH * COUNT 1 1) \u0026#34;3\u0026#34; 2) 1) \u0026#34;key1\u0026#34; 127.0.0.1:6379\u0026gt; scan 3 MATCH * COUNT 1 1) \u0026#34;0\u0026#34; 2) (empty list or set) Hash删除: hscan + hdel [6] 先删除hash里的每个field，最后再删除hash public void delBigHash(String host, int port, String password, String bigHashKey) { Jedis jedis = new Jedis(host, port); if (password != null \u0026amp;\u0026amp; !\u0026#34;\u0026#34;.equals(password)) { jedis.auth(password); } ScanParams scanParams = new ScanParams().count(100); String cursor = \u0026#34;0\u0026#34;; do { ScanResult\u0026lt;Entry\u0026lt;String, String\u0026gt;\u0026gt; scanResult = jedis.hscan(bigHashKey, cursor, scanParams); List\u0026lt;Entry\u0026lt;String, String\u0026gt;\u0026gt; entryList = scanResult.getResult(); if (entryList != null \u0026amp;\u0026amp; !entryList.isEmpty()) { for (Entry\u0026lt;String, String\u0026gt; entry : entryList) { jedis.hdel(bigHashKey, entry.getKey()); } } cursor = scanResult.getStringCursor(); } while (!\u0026#34;0\u0026#34;.equals(cursor)); //删除bigkey jedis.del(bigHashKey); } 参考\r#\r《Redis 深度历险：核心原理与应用实践》 钱文品 大海捞针—scan Redis中查找大key 解决了Redis大key问题 *** 解决了Redis大key问题 Redis 大 key 要如何处理？ Bigkey问题的解决思路与方式探索 vivo team *** 一份完整的阿里云 Redis 开发规范，值得收藏！ *** 尚硅谷Redis零基础到进阶，最强redis7教程，阳哥亲自带练（附redis面试题） V {% post_link \u0026lsquo;redisLazyFree\u0026rsquo; %} self "},{"id":7,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlTransactionAndLock/","title":"MySQL 事务-隔离性","section":"单机","content":"\nMyISAM vs. InnoDB\r#\r描述 MyISAM InnoDB 行锁(并发高，会死锁) × √ (默认支持)\nRecord lock: 锁记录\nGap lock: 锁范围，不锁记录\nNext-key lock： 锁范围+锁记录 表锁(并发低，不会死锁) √ √ 事务和崩溃恢复 × √ 外键 × √ MVCC × √ 在READ COMMITTED 和 REPEATABLE READ时有效 事务隔离级别 [4]\r#\r隔离级别(从高到低) 脏读 不可重复读\n（重点是修改） 幻影读\n（重点是新增或者删除） SERIALIZABLE × × × REPEATABLE-READ\n（InnoDB默认隔离级别） × × √ READ-COMMITTED × √ √ READ-UNCOMMITTED √ √ √ innodb对于行的查询使用next-key lock Next-locking keying、Gap锁为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时(单条记录)，将next-key lock降级为record key\n新的隔离级别\r#\rSI[Snapshot Isolation] Oracle 可串行化, PG和MySQL称为RR SSI[Serializable Snapshot Isolation] PostgreSQL 和 CockroachDB 已经支持 SSI RC和RR隔离级别 [chat]\r#\r下面是一个表格，归纳了以上文字中RC和RR隔离级别的特点：\n隔离级别 快照读 #1 当前读 #2 幻读 RC 不加锁 加锁[记录锁 是,间隙锁 否] 存在 RR 不加锁 加锁[记录锁 是, 间隙锁 是] 不存在 在RC隔离级别下，快照读和当前读都不会对记录加锁，因此不会阻塞其他事务的读操作。但是，由于RC隔离级别只对读取到的记录加锁，而不对读取的范围加锁，因此可能会出现幻读现象。幻读指的是，在一个事务中先后进行两次相同的查询操作，第二次查询会发现有新增的记录，这种现象是由于其他事务在事务中新增了这些记录所导致的。\n在RR隔离级别下，快照读和当前读都不会对记录加锁，但是会对读取的范围加锁，防止其他事务在该范围内插入新的记录。因此，在RR隔离级别下不存在幻读现象。\n需要注意的是，虽然RR隔离级别可以避免幻读现象，但是由于对读取范围加锁可能会导致性能问题，因此在实际应用中需要根据具体情况选择合适的隔离级别。 [ 当前读 加锁，快照读 不加锁 ]\nMVCC\r#\r原理 [2][3]\r#\rInnoDB 中的 RC(READ COMMITTED) 和 RR(REPEATABLE READ) 隔离事务是基于多版本并发控制（MVVC）实现高性能事务。 MVCC 对普通的 Select 不加锁，如果读取的数据正在执行 Delete 或 Update 操作，这时读取操作不会等待排它锁的释放，而是直接利用 MVCC 读取该行的数据快照（数据快照是指在该行的之前版本的数据，而数据快照的版本是基于 undo 实现的，undo 是用来做事务回滚的，记录了回滚的不同版本的行记录）。\nMySQL默认的事务隔离级别是RR(REPEATABLE READ), InnoDB引擎的Select操作使用一致性非锁定读（MVCC）。 对于一致性非锁定读， 即使读取的行已经被执行了select\u0026hellip;for update,也是可以读取的。\n实现\r#\r当前读/快照读 含义 例子 当前读 #2 读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 select ... lock in share mode(共享锁)，\nselect ...for update、\nupdate、insert、delete(排他锁) 快照读 #1 简单的select（不加锁）就是快照读，快照读，读取的是记录数据的可见版本，有可能是历史数据，不加锁，是非阻塞读。 Read Committed：每次select，都生成一个快照读。\nRepeatable Read：开启事务后第一个select语句才是快照读的地方。\nSerializable：快照读会退化为当前读。 MVCC实现 [0] 隐藏字段 DB_TRX_ID: 最近修改事务ID DB_ROLL_PTR: 回滚指针 DB_ROW_ID: 隐藏主键 undolog版本链 链表的头部是最新的旧记录，链表尾部是最早的旧记录 readview ReadView（读视图）是 快照读 SQL执行时MVCC提取数据的依据，记录并维护系统当前活跃的事务（未提交的）id。 不同的隔离级别，生成ReadView的时机不同： READ COMMITTED ：在事务中每一次执行快照读时生成ReadView。 REPEATABLE READ：仅在事务中第一次执行快照读时生成ReadView，后续复用该ReadView。 参考\r#\r黑马程序员 MySQL数据库入门到精通 P141-P144 mysql_note 笔记1 MySQL 索引 笔记2 ***\n《深入浅出MySQL：数据库开发、优化与管理维护》\n《03 | 事务隔离：为什么你改了我还看不见？ 》MySQL实战45讲 丁奇\n《33 | MySQL调优之事务：高并发场景下的数据库事务调优》 Java性能调优实战 刘超 deleted\n可能是全网最好的MySQL重要知识点\n《07 | 行锁功过：怎么减少行锁对性能的影响？》 MySQL实战45讲 丁奇 deleted\n《18 | 为什么这些SQL语句逻辑相同性能却差异巨大？》MySQL实战45讲 丁奇\nMYSQL死锁的检测与预防 deleted\n{% post_link \u0026lsquo;mysqlTransaction\u0026rsquo; %} self\n"},{"id":8,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/overview/redis/","title":"Redis 总结","section":"overview","content":"\n事务\r#\r- 原子性 一致性 隔离性 持久性 redis 一定的原子性，但不支持回滚 × √ 通过一定策略可以保证持久性 redis 没有进行回滚，不具备原子性.操作之后写AOF日志 aof可以保证，但从应用层看没有回滚和原子性，所以并不能保证一致性 单线程天然隔离 纯内存(×)RDB Bgsave(√) RDB 异步执行(×) mysql √ √ √ √ mysql undo log 锁 锁 redo log hash命令\r#\rredis hash的结构：一维数组+二维链表（和java的hashmap结构一样）\nredis rehash: 渐进式rehash Java rehash： 一次性将旧数组下挂接的元素全部转移到新数组下面\nIO模型和性能\r#\r非阻塞IO： read， write时不阻塞\n事件轮询和多路复用[8]\nredis性能 最低配置: 4GB， 2核， 链接数2w， QPS 16w\nredis性能高的原因\n高效的数据结构 多路复用IO模型 事件机制 总结:Reactor + 队列 [10] 大体上可以说 Redis 的工作模式是，reactor 模式配合一个队列，用一个 serverAccept 线程来处理建立请求的链接， 并且通过 IO 多路复用模型，让内核来监听这些 socket，一旦某些 socket 的读写事件准备就绪后就对应的事件压入队列中， 然后 worker 工作，由文件事件分派器从中获取事件交于对应的处理器去执行，当某个事件执行完成后文件事件分派器才会从队列中获取下一个事件进行处理。 可以类比在 netty 中，我们一般会设置 bossGroup 和 workerGroup 默认情况下 bossGroup 为 1，workerGroup = 2 * cpu 数量， 这样可以由多个线程来处理读写就绪的事件，但是其中不能有比较耗时的操作如果有的话需要将其放入线程池中，不然会降低其吐吞量。 在 Redis 中我们可以看做这二者的值都是 1。\n特性\r#\rRedis 2.6 lua, pubsub, Sentinel V1 Redis 2.8\nSentinel V2, ipv6 Redis 3.0 Redis Cluster Redis3.2 GEO Redis 4.0 psync2.0, lazy-free, modules RDB-AOF 混合持久化 Redis 5.0 Stream Redis 6.0 Thread I/O SSL, ACL Redis 7.0\nfunctions, ACL v2 sharded-pubsub client-eviction multi-part AOF 参考\r#\r《Redis 深度历险：核心原理与应用实践》 钱文品\n原理 4：雷厉风行 —— 管道 原理 5：同舟共济 —— 事务 原理 3：未雨绸缪 —— 持久化 鞭辟入里 ——— 线程IO模型 《Redis实战》 黄健宏 3.7 ,4.4, 6.2 Redis 数据结构和对象系统，记住这 12 张图就够啦！ 七问Redis，才知道我与技术大牛的差距在哪里 *** 事务，乐观锁watch，持久化， 内存优化，主从复制，过期删除策略 Mysql事务总结 self 美团针对Redis Rehash机制的探索和实践 *** 为什么 Redis 单线程能达到百万+QPS？ *** "},{"id":9,"href":"/www6vMiddleware/docs/message/Kafka/kafka/","title":"Kafka总结","section":"Kafka","content":"\n参考:\r#\rKafka剖析（一）：Kafka背景及架构介绍 郭俊 Kafka设计解析（六）- Kafka高性能关键技术解析 郭俊 《kafka权威指南》 薛命灯 第3，5章 Kafka文件存储机制那些事 幽灵之使 分布式发布订阅消息系统 - Kafka架构设计 官方文档翻译 未 "},{"id":10,"href":"/www6vMiddleware/docs/message/Overview/mq/","title":"消息中间件总结","section":"Overview","content":"\n消息积压\r#\r原则: 消费性能要高于生产的性能\r#\r1. 发送端性能优化\r#\r并发和批量\n2. 消费端性能优化\r#\r分区partion和consumer同步扩容\n参考\r#\rJumper, JMQ 代码 文档 Kafka vs RocketMQ——单机系统可靠性 以夕 xxx 分布式开放消息系统(RocketMQ)的原理与实践 CHEN川 *** 消息队列设计精要 点评 王烨 失效 Kafka设计解析（六）- Kafka高性能关键技术解析 郭俊 事务消息 -\u0026gt; 消息队列 RocketMQ 阿里云官方文档 消息队列 RocketMQ、Apache RocketMQ、消息队列 Kafka、Apache Kafka、RabbitMQ 产品对比 阿里云官方文档 RocketMQ消息堆积判断 rocketMQ消息堆积监控的java实现 c614756zhang RocketMQ原理（4）——消息ACK机制及消费进度管理 Jaskey Lam "},{"id":11,"href":"/www6vMiddleware/demo/chaptor1/my-first-doc/","title":"My First Doc","section":"章节一","content":"\r一级标题\r#\r二级标题\r#\r三级标题\r#\r"},{"id":12,"href":"/www6vMiddleware/demo/chaptor1/","title":"章节一","section":"Demoes","content":"\r一级标题\r#\r二级标题\r#\r三级标题\r#\r"},{"id":13,"href":"/www6vMiddleware/demo/chaptor3/","title":"章节三","section":"Demoes","content":"\r一级标题\r#\r二级标题\r#\r三级标题\r#\r"},{"id":14,"href":"/www6vMiddleware/demo/chaptor2/","title":"章节二","section":"Demoes","content":"\r一级标题\r#\r二级标题\r#\r三级标题\r#\r"},{"id":15,"href":"/www6vMiddleware/docs/message/Kafka/kafkaProducer/","title":"Kafka Producer生产者","section":"Kafka","content":"\nProducer 架构\r#\rOverviw 1 [1]\r#\r整个生产者客户端是由主线程和Sender线程协调运行的, 主线程创建消息, 然后通过 拦截器、元信息更新、序列化、分区器、缓存消息等等流程。 Sender线程在初始化的时候就已经运行了,并且是一个while循环。\nOverviw 2\r#\rProducer 分区策略[2]\r#\rDefaultPartitioner 默认分区策略 粘性分区Sticky Partitioner UniformStickyPartitioner 纯粹的粘性分区策略 RoundRobinPartitioner 分区策略 kafka 生产者 里的buffer[3]\r#\rkafka producer中配置的 buffer.memory （参数在文末有详细说明）参数是缓冲区的大小，这个缓存区大家也就是RecordAccmulator所用的内存大小。默认是32MB。\n参考\r#\r图解kafka生产者流程,超详细！ 石臻臻 kafka contributor Kafka生产者的3种分区策略 石臻臻 kafka contributor Kafka Producer全流程分析和思考 "},{"id":16,"href":"/www6vMiddleware/docs/message/Overview/mqCompare/","title":"MQ总结(Kafka, Rocketmq, Rabbitmq)","section":"Overview","content":"\n功能 RocketMQ Kafka RabbitMQ 可靠性* - 同步刷盘\n- 异步刷盘 异步刷盘，丢数据概率高 同步刷盘 横向扩展能力 支持 支持 - 集群扩容依赖前端 - LVS 负载均衡调度 消费模型* Push/Pull Pull Push/Pull 定时消息* 支持（只支持18个固定 Level） 不支持 支持 顺序消息* 支持 支持 不支持 消息堆积能力 百亿级别 影响性能 影响性能 影响性能 消息堆积查询 支持 不支持 不支持 消息回溯 支持 支持（位置，时间） 不支持 消息重试 支持 生产者有重试机制 支持 死信队列 支持 不支持 支持 性能（常规）* 非常好 十万级 QPS 非常好 百万级 QPS 一般 万级 QPS 性能（万级 Topic 场景） 非常好 十万级 QPS 低 低 性能（海量消息堆积场景） 非常好 十万级 QPS 低 低 全链路消息轨迹 不支持 不支持 不支持 MQ比较[3]\r#\r重点[3]\r#\r功能级别不具备一票否决权 选型时要特别注意中间件的性能与扩展性 需要注重团队技术栈与中间件编程语言的匹配度 参数\r#\rKafka、RabbitMQ、RocketMQ等消息中间件的对比 https://honeypps.com/mq/kafka-vs-rabbitmq/ 未 13 | 技术选型：如何根据应用场景选择合适的消息中间件？ 丁威 "},{"id":17,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%BA%94%E7%94%A8/redisDistKey/","title":"Redis 分布式锁","section":"Redis 应用","content":"\n一. 基于redis的分布式锁\r#\r锁的特性:\r#\r排它性 超时释放锁 redis expire 高可用，锁集群容错[图2]， 安全性[7]， 可重入锁, 避免死锁[8] 乐观锁, 悲观锁[10][图2] 参考：\r#\r分布式系统互斥性与幂等性问题的分析与解决 点评 蒋谞 漫画：什么是分布式锁？ 程序员小灰 《从Paxos到Zookeeper分布式一致性原理与实践》 倪超 6.1.7节 Redlock：Redis分布式锁最牛逼的实现 阿飞的博客 SOFAJRaft-RheaKV 分布式锁实现剖析 | SOFAJRaft 实现原理 SOFALab 米麒麟 未 Go to Page self 分布式服务总结 分布式锁\n通过栅栏(fencing)使得锁更安全, fencing token How to do distributed locking Martin Kleppmann Redis实现分布式锁，以及可重入锁思路\n唯一id I. uuid II. 分布式线程中标识唯一线程：MAC地址 + jvm进程ID + 线程ID Redis分布式锁实现秒杀业务(乐观锁、悲观锁) 最后 乐观锁: jedis的watch方法 from redis 《Redis 深度历险：核心原理与应用实践》 钱文品 3. 应用 1：千帆竞发 —— 分布式锁 4. 拓展 3：拾遗漏补 —— 再谈分布式锁\n"},{"id":18,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/Arch-Redis-Cluster/redisError/","title":"Redis Error-MOVED和ASK指令","section":"Arch \u0026 Redis Cluster","content":"\n{% details 相关代码 %} cluster.c\n/* Return the pointer to the cluster node that is able to serve the command. * For the function to succeed the command should only target either: * * 1) A single key (even multiple times like LPOPRPUSH mylist mylist). * 2) Multiple keys in the same hash slot, while the slot is stable (no * resharding in progress). * * On success the function returns the node that is able to serve the request. * If the node is not \u0026#39;myself\u0026#39; a redirection must be perfomed. The kind of * redirection is specified setting the integer passed by reference * \u0026#39;error_code\u0026#39;, which will be set to CLUSTER_REDIR_ASK or * CLUSTER_REDIR_MOVED. * * When the node is \u0026#39;myself\u0026#39; \u0026#39;error_code\u0026#39; is set to CLUSTER_REDIR_NONE. * * If the command fails NULL is returned, and the reason of the failure is * provided via \u0026#39;error_code\u0026#39;, which will be set to: * * CLUSTER_REDIR_CROSS_SLOT if the request contains multiple keys that * don\u0026#39;t belong to the same hash slot. * * CLUSTER_REDIR_UNSTABLE if the request contains multiple keys * belonging to the same slot, but the slot is not stable (in migration or * importing state, likely because a resharding is in progress). * * CLUSTER_REDIR_DOWN_UNBOUND if the request addresses a slot which is * not bound to any node. In this case the cluster global state should be * already \u0026#34;down\u0026#34; but it is fragile to rely on the update of the global state, * so we also handle it here. * * CLUSTER_REDIR_DOWN_STATE if the cluster is down but the user attempts to * execute a command that addresses one or more keys. */ clusterNode *getNodeByQuery(client *c, struct redisCommand *cmd, robj **argv, int argc, int *hashslot, int *error_code) { clusterNode *n = NULL; robj *firstkey = NULL; int multiple_keys = 0; multiState *ms, _ms; multiCmd mc; int i, slot = 0, migrating_slot = 0, importing_slot = 0, missing_keys = 0; /* Allow any key to be set if a module disabled cluster redirections. */ if (server.cluster_module_flags \u0026amp; CLUSTER_MODULE_FLAG_NO_REDIRECTION) return myself; /* Set error code optimistically for the base case. */ if (error_code) *error_code = CLUSTER_REDIR_NONE; /* Modules can turn off Redis Cluster redirection: this is useful * when writing a module that implements a completely different * distributed system. */ /* We handle all the cases as if they were EXEC commands, so we have * a common code path for everything */ if (cmd-\u0026gt;proc == execCommand) { /* If CLIENT_MULTI flag is not set EXEC is just going to return an * error. */ if (!(c-\u0026gt;flags \u0026amp; CLIENT_MULTI)) return myself; ms = \u0026amp;c-\u0026gt;mstate; } else { /* In order to have a single codepath create a fake Multi State * structure if the client is not in MULTI/EXEC state, this way * we have a single codepath below. */ ms = \u0026amp;_ms; _ms.commands = \u0026amp;mc; _ms.count = 1; mc.argv = argv; mc.argc = argc; mc.cmd = cmd; } /* Check that all the keys are in the same hash slot, and obtain this * slot and the node associated. */ for (i = 0; i \u0026lt; ms-\u0026gt;count; i++) { struct redisCommand *mcmd; robj **margv; int margc, *keyindex, numkeys, j; mcmd = ms-\u0026gt;commands[i].cmd; margc = ms-\u0026gt;commands[i].argc; margv = ms-\u0026gt;commands[i].argv; keyindex = getKeysFromCommand(mcmd,margv,margc,\u0026amp;numkeys); for (j = 0; j \u0026lt; numkeys; j++) { robj *thiskey = margv[keyindex[j]]; int thisslot = keyHashSlot((char*)thiskey-\u0026gt;ptr, sdslen(thiskey-\u0026gt;ptr)); if (firstkey == NULL) { /* This is the first key we see. Check what is the slot * and node. */ firstkey = thiskey; slot = thisslot; n = server.cluster-\u0026gt;slots[slot]; /* Error: If a slot is not served, we are in \u0026#34;cluster down\u0026#34; * state. However the state is yet to be updated, so this was * not trapped earlier in processCommand(). Report the same * error to the client. */ if (n == NULL) { getKeysFreeResult(keyindex); if (error_code) *error_code = CLUSTER_REDIR_DOWN_UNBOUND; return NULL; } /* If we are migrating or importing this slot, we need to check * if we have all the keys in the request (the only way we * can safely serve the request, otherwise we return a TRYAGAIN * error). To do so we set the importing/migrating state and * increment a counter for every missing key. */ if (n == myself \u0026amp;\u0026amp; server.cluster-\u0026gt;migrating_slots_to[slot] != NULL) { migrating_slot = 1; } else if (server.cluster-\u0026gt;importing_slots_from[slot] != NULL) { importing_slot = 1; } } else { /* If it is not the first key, make sure it is exactly * the same key as the first we saw. */ if (!equalStringObjects(firstkey,thiskey)) { if (slot != thisslot) { /* Error: multiple keys from different slots. */ getKeysFreeResult(keyindex); if (error_code) *error_code = CLUSTER_REDIR_CROSS_SLOT; return NULL; } else { /* Flag this request as one with multiple different * keys. */ multiple_keys = 1; } } } /* Migarting / Improrting slot? Count keys we don\u0026#39;t have. */ if ((migrating_slot || importing_slot) \u0026amp;\u0026amp; lookupKeyRead(\u0026amp;server.db[0],thiskey) == NULL) { missing_keys++; } } getKeysFreeResult(keyindex); } /* No key at all in command? then we can serve the request * without redirections or errors in all the cases. */ if (n == NULL) return myself; /* Cluster is globally down but we got keys? We can\u0026#39;t serve the request. */ if (server.cluster-\u0026gt;state != CLUSTER_OK) { if (error_code) *error_code = CLUSTER_REDIR_DOWN_STATE; return NULL; } /* Return the hashslot by reference. */ if (hashslot) *hashslot = slot; /* MIGRATE always works in the context of the local node if the slot * is open (migrating or importing state). We need to be able to freely * move keys among instances in this case. */ if ((migrating_slot || importing_slot) \u0026amp;\u0026amp; cmd-\u0026gt;proc == migrateCommand) return myself; /* If we don\u0026#39;t have all the keys and we are migrating the slot, send * an ASK redirection. */ if (migrating_slot \u0026amp;\u0026amp; missing_keys) { if (error_code) *error_code = CLUSTER_REDIR_ASK; return server.cluster-\u0026gt;migrating_slots_to[slot]; } /* If we are receiving the slot, and the client correctly flagged the * request as \u0026#34;ASKING\u0026#34;, we can serve the request. However if the request * involves multiple keys and we don\u0026#39;t have them all, the only option is * to send a TRYAGAIN error. */ if (importing_slot \u0026amp;\u0026amp; (c-\u0026gt;flags \u0026amp; CLIENT_ASKING || cmd-\u0026gt;flags \u0026amp; CMD_ASKING)) { if (multiple_keys \u0026amp;\u0026amp; missing_keys) { if (error_code) *error_code = CLUSTER_REDIR_UNSTABLE; return NULL; } else { return myself; } } /* Handle the read-only client case reading from a slave: if this * node is a slave and the request is about an hash slot our master * is serving, we can reply without redirection. */ if (c-\u0026gt;flags \u0026amp; CLIENT_READONLY \u0026amp;\u0026amp; (cmd-\u0026gt;flags \u0026amp; CMD_READONLY || cmd-\u0026gt;proc == evalCommand || cmd-\u0026gt;proc == evalShaCommand) \u0026amp;\u0026amp; nodeIsSlave(myself) \u0026amp;\u0026amp; myself-\u0026gt;slaveof == n) { return myself; } /* Base case: just return the right node. However if this node is not * myself, set error_code to MOVED since we need to issue a rediretion. */ if (n != myself \u0026amp;\u0026amp; error_code) *error_code = CLUSTER_REDIR_MOVED; return n; } {% enddetails %}\n代码展示了 getNodeByQuery 函数基本执行过程\n总结\r#\rRedis Cluster 会因为负载均衡或节点故障等原因而执行数据迁移，而\n这就会导致客户端访问的 key 并不在接收到命令的集群节点上。因此，集群节点在命令执\n行函数 processCommand 中，针对集群模式，就增加了额外的处理逻辑。这主要是包括\n调用 getNodeByQuery 函数查询访问的 key 实际所属的节点，以及根据查询结果调用\nclusterRedirectClient 函数执行请求重定向。\n事实上，对于分布式集群来说，Redis Cluster 设计实现的请求重定向机制是一个不错的参\n考示例。其中，MOVED 和 ASK 两种重定向情况，就充分考虑了数据正在迁移的场景，这\n种设计值得我们学习。而且，getNodeByQuery 函数在查询 key 所属的 slot 和节点时，\n也充分考虑了 Redis 的事务操作，在对命令访问 key 进行查询时，巧妙地使用了同一个数\n据结构 multiState，来封装事务涉及的多条命令和常规的单条命令，增加了代码的复用程\n度，这一点也非常值得学习。\n参考\r#\r27 | 从MOVED、ASK看集群节点如何处理命令？ 蒋德钧\n"},{"id":19,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/distributedDatabaseCompare/","title":"分布式 数据库 比较","section":"分布式","content":"\n分布式 数据库 比较 [1]\r#\rPolarDB-X TiDB CockroachDB Spanner 分布式事务 MVCC+TSO MVCC+TSO [Percolator][5][6] MVCC+HLC[2][3][4] MVCC+TrueTime API 存储引擎 InnoDB/x-engine\nHEX2(列存) RocksDBTiFlash(列存) RocksDB Colossus 高可用 计算无状态集群\n存储Multi-Paxos 计算无状态集群\n存储Raft 计算存储一体化 集群化 存储Raft 计算存储一体化 集群化 存储Multi-Paxos 数据分区 Hash/List/Range Range/Hash Range/Hash Range/Hash 全局索引 √ √ √ √ HTAP 强隔离 强一致\nMPP并行 强隔离 强一致\nSpark 混合负载\nMPP并行 混合负载\nMPP并行 生态兼容性 基本兼容MySQL 基本兼容MySQL 基本兼容PGSQL 非标准SQL 元数据DDL 全局一致 + Online 全局一致 + Online 全局一致 + Online 全局一致 + Online 全局日志 √ √ √（企业版） × 备份恢复 √ √ √ √ 参考\r#\r《云数据库架构》 2.3.1 CockroachDB分布式事务解密(一)：CockroachDB \u0026amp; HLC CockroachDB使用了一个软件实现的基于NTP时钟同步的混合逻辑时钟算法(Hybrid Logic Clock)——HLC追踪系统中事务的的hb关系(happen before)。 CockroachDB事务解密(二)：事务模型 Cockroach HLC {% post_link \u0026rsquo;tikvMVCCTransaction\u0026rsquo; %} {% post_link \u0026lsquo;distributedDatabaseGlobalTime\u0026rsquo; %} self {% post_link \u0026lsquo;\u0026lsquo;globalSecondaryIndex %} self "},{"id":20,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/overview/redisIO/","title":"Redis 的IO模型","section":"overview","content":"\n关键词： 单线程，事件， socket， 多路复用\r#\rRedis 的IO模型\r#\rRedis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。 但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执 行的。\n多路复用\r#\rLinux 中的 IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同 时存在多个监听套接字和已连接套接字。内核会一直监听这些套接字上的连接请求或数据 请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。\n为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针 对不同事件的发生，调用相应的处理函数。\nepoll的API\r#\rint epoll_create(int size); int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // 事件注册 int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout); // 事件分配、事件处理\nNetty IO模型\r#\rchannel\r#\rchannel（在socket上的通道） - event channel - channelPipeline（单线程）\nreactor\r#\raccept事件（boss线程） read，write事件（work线程）\n参考：\r#\r03 | 高性能IO模型:为什么单线程Redis能那么快? Netty源码解读（三）Channel与Pipeline https://www6v.github.io/www6vHomeHexo/2015/08/23/nettySummary/ epoll使用详解：epoll_create、epoll_ctl、epoll_wait、close\n"},{"id":21,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E8%B5%84%E6%BA%90%E5%9B%9E%E6%94%B6/redisDelete/","title":"Redis 回收策略","section":"资源回收","content":"\n回收策略\r#\r回收策略 redis kafka 基于时间 过期删除策略 1. 定时删除(对内存最友好， 对CPU时间最不友好) 2. 惰性删除(对CPU时间最友好， 对内存最不友好) 3.定期删除(整合和折中) 基于大小 内存淘汰策略 1. noeviction 2.lru 3. random 4. ttl 其他 x 近似LRU算法[11] 参考\r#\r七问Redis，才知道我与技术大牛的差距在哪里 ***\n经典面试题：Redis 内存满了怎么办？\n回收策略\r#\rRedis内存回收机制，把我整懵了\u0026hellip; Kafka日志清理之Log Deletion 《Redis 开发与运维》 8.2.3 未 "},{"id":22,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisOptimize/","title":"Redis 优化建议","section":"常见问题","content":"\n优化建议\r#\r优化建议[2]\r#\r对象内存优化\n客户端缓冲优化\n碎片优化\n子进程内存优化\n序列化 + 压缩 Renault（序列化： Json or Hession，压缩： Hession自带压缩 ）\n参考\r#\r加餐（六）| Redis的使用规范小建议 Redis 内存优化在 vivo 的探索与实践 vivo "},{"id":23,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/redisDataStructure/","title":"Redis数据结构","section":"数据类型","content":"\n数据结构和实现\r#\r数据结构 基础 优化 特点 String SDS List 双向列表 压缩列表 Hash hash表 压缩列表 渐进式rehash SortedSet 跳表 压缩列表 Set 整数数组 全局hash表 \u0026amp;\u0026amp; 渐进式 rehash\r#\r参考：\r#\r02 | 数据结构:快速的Redis有哪些慢操作?\n"},{"id":24,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlUpdate/","title":"MySQL中的SQL更新语句","section":"单机","content":"\nredo log \u0026amp;\u0026amp; bin log\r#\r有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为crash-safe。\n\\ redo log bin log where InnoDB引擎特有的 MySQL的Server层实现的 what 物理日志，记录的是“在某个数据页上做了什么修改” 逻辑日志，记录的是这个语句的原始逻辑 how 循环写的 追加写入的 update in Mysql\r#\rMySQL里的WAL(Write-Ahead Logging)技术，它的关键点就是先写日志，再写磁盘.\n更新流程还涉及两个重要的日志模块，redo log（重做日志）和 binlog（归档日志）。\n图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。\nredo log的写入拆成了两个步骤：prepare和commit，这就是\u0026quot;两阶段提交\u0026quot;, 让redo log和bin log之间的逻辑一致。\n参考\r#\r《MySQL实战45讲 - 日志系统：一条SQL更新语句是如何执行的？》 丁奇 "},{"id":25,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98/cacheMultiLayer/","title":"多级缓存(cache)","section":"缓存","content":"\nOverview\r#\r多级缓存 [5?]\r#\r二级缓存（本地缓存+远程缓存）\r#\r二级缓存（本地缓存+远程缓存） 远端Cache推全量或者部分的数据到本地cache，并设置过期时间【初始化】 查询流程 先从本地缓存拿，如果有数据且有效，就直接返回 如果没有命中 1.本地查询远端服务，并拿到结果 2.本地更新远端缓存 3.更新本地缓存 本地cache失效+更新流程,本地防穿透【1】 同步更新缓存 访问同一个key的业务线程只有一个线程穿透到远端Cache，其他线程等待穿透线程的返回结果[加锁] 异步更新缓存 1.过期时间到了后，产生过期事件，延长数据有效期，返回旧的数据 检查过期事件，后台线程池更新缓存数据，并重新设置有效时间 eg. Google LoadingCache 以上两种方式都有 远端cache，防穿透 远端cache访穿透采用永久缓存数据，每次查询都能查到值. 通过辅助信息判断逻辑过期, 再从远端服务异步拉数据刷新远端cache。 多级缓存查询 [2]\r#\r多级缓存更新 [2]\r#\r缓存分层 多级缓存\r#\r边缘cache\r#\r可用CDN实现，往往是服务器端缓存，存静态数据。 可以存Html页面， 脚本， 样式， 图片，页面片段等。\n页面级缓存\r#\r往往是本地缓存， 数据相对动态。\n计算结果的缓存\r#\r可以存储索引聚合数据，比如 BI里的数据聚合表。也可以存储耗时查询数据 ，比如搜索的结果。也可以存储业务相关数据， 比如对象模型的有向图可以整个缓存起来。在微博系统中，所有@你的微博是相对耗时， 可以 考虑作为逻辑缓存。\n数据源级缓存： 缓存数据源结果集\r#\r比如Hibernate缓存中的QueryCache用来缓存查询语句, 及查询结果集中对象的Id与Type. 当再次使用已缓存的Query时, 就可以通过对象的Id与Type在二级缓存中查找实际的对象.\nHibernate提供了短生命周期的缓存， 也叫事务级别的缓存。长生命周期的缓存，也叫应用级别的缓存。\n缓存分层之间的失效方式： 1. 映射关系 2. 日志 ＋ 重试\n## 缓存对象的粒度\n有一种缓存的误用是缓存大量的数据集合，而读取其中一部分。 在很多时候，我们往往会缓存一个对象的集合，但是，我们在读取的时候，只是每次读取其中一部分。 在更新缓存时， 读出整个集合， 改变其中一部分后， 在存回去， 这样序列化与反序列化的代价相当大\n针对这个情况， jboss cache提供了两种粒度的对象存储：核心缓存（粗粒度的），POJO 缓存（细粒度的）\n核心缓存会直接把您传递给它的数据存储在一个树型结构中。键／值对被存储在树的节点上，出于复制或持续性的需要它们都被序列化了。\nPOJO 缓存则采用比较复杂的机制——利用字节码编织来内省（introspecting）用户类，并向用户类的域添加侦听器，一旦域值有任何变化，侦听器会立刻 通知缓存。例如，如果要在POJO缓存中存储一个庞大、复杂的对象，会导致POJO缓存内省对象的字节码，最终只把该对象的原始域存储到树结构中。一旦域 值有所变化，缓存只复制这个改变了的域值而不会去复制整个用户类，这是高效的细粒度复制。\n在缓存了细粒度的对象后， 造成的一个问题是数据的冗余。 例如查询条件1的返回的是model1, model2, model3, 查询条件2返回的是model2, model3, model4. model2, model3在缓存里就存了两份， 造成了冗余。这时可以分离出一个索引层，索引层存储缓存对象的地址， 这样可以节约大量的存储空间。 例如可以存储model1- model4的索引， 再从缓存中取得到实际的model. 在数据库中， 这种方式叫look up table. 如果系统更复杂， 可以采取缓存的partition加多级索引的方式\n## 缓存与一致性\n缓存多副本之间的同步： 可分为replication和invalidaiton机制。 Replication机制表示一旦有数据的更新， 其余副本都会同步复制一份更新后的数据。Replication机制复制时slave会对master节点有拖累， 这时可以考虑采取invalidation机制。 Invalidation机制在jboss cache里已有实现, 一旦有更新， 广播消息， 失效所有其他的副本，让其重新去获得该值。 可通过这种方式缓存大对象以减少在实例中复制对象的代价。根据用户在一定时间段内上网地点固定不变的规律，用户始终都是访问同一个机房， 针对主节点的本地缓存在有更新时可以异步发invalidation消息，副本节点可以慢慢的再加载回这个大对象， 这样可以提高用户响应度。这种方式也可用在边缘缓存中。对于无法分组的数据， 比如在某时间段的用户认证数据需要保证副本同步，最好的方式是清除相应的副本， 让它在下次使用时初始化\n参考\r#\r多级缓存架构 《架构实战营-第25节课：计算架构模式之多级缓存架构》 V\n解析分布式系统的缓存设计\n《移动选购线缓存实践》 赵思奇 ***\njetcache：阿里这款多级缓存框架一定要掌握 未\n聊聊微服务架构中的多级缓存设计（建议收藏） 未\nJava分布式缓存一篇文章让你明白你多级缓存的分层架构原理分析 未\n"},{"id":26,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/tikvMVCCTransaction/","title":"TiKV Transaction-MVCC+TSO","section":"分布式","content":"\n原理\r#\rPercolator [1]\r#\r总体来说就是一个经过优化的 2PC 的实现，依赖一个单点的授时服务 TSO 来实现单调递增的事务编号生成，提供SI 的隔离级别。\nTiKV 的写事务分为两个阶段 [1]\r#\r1、Prewrite 阶段 MVCC 在对应传统 2PC 的第一阶段的 prewrite 流程。 首先选出一个 primary row 和其他的 secondary rows，然后对 primary row 进行上锁，再对 secondary rows 进行类似的上锁流程。如果任何一步出错，都会进行回滚。完成 prewrite 阶段后，进入 commit 阶段，当前时间戳为 commitTs，TSO 会保证 commitTs \u0026gt; startTs。\n2、Commit 阶段 MVCC 中的 Commit 流程，包括在 primary 上写入 meta，删除 Lock 标记，以及异步提交 secondaries。如果 primary row 提交失败，则整个事务回滚。如果成功，则标志着整个事务提交成功。\nTidb乐观锁 [2] Tidb悲观锁 [2] TiKV 的读事务 [1]\r#\r在事务中进行读操作的过程。 首先，需要检查行是否被锁定，如果被锁定，则需要等待或者清除锁。然后，需要读取最新的数据版本，方法是读取元数据并找到最大的时间戳。 锁分为两级，Primary和Secondary row，只有Primary row的锁被释放，事务才算提交成功。Secondary row的提交可以异步进行，但在此过程中可能需要清理锁。即使Secondary row提交失败，也可以通过锁找到Primary row，并根据元数据确定事务是回滚还是提交成功。\nTiKV 的事务默认隔离级别是 Repeatable Read（SI）, 也对外暴露显式的加锁的 API，用于为客户端实现 SELECT … FOR UPDATE 等隔离级别为 SSI 的语句。\n读写冲突处理 [3]\r#\r参考\r#\rTiKV 事务模型概览，Google Spanner 开源实现 *** 《13 | 隔离性：为什么使用乐观协议的分布式数据库越来越少? 》 分布式数据库30讲 《11｜隔离性：读写冲突时，快照是最好的办法吗？》 分布式数据库30讲 percolator的理解与开源实现分析 未 Percolator - 分布式事务的理解与分析 未 《云原生数据库 原理与实践》 8.1.3 未 TiDB 新特性漫谈：悲观事务 未 {% post_link \u0026lsquo;distributedDatabaseGlobalTime\u0026rsquo; %} self "},{"id":27,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/soaGracefulStart/","title":"优雅启动","section":"容错\u0026限流","content":"\n优雅启动 实现 [1]\r#\r调用方发起的RPC 调用流程是怎样的，调用方应用通过服务发现能够获取到服务提供方的 IP 地址，然后每次发送请求前，都需要通过负载均衡算法从连接池中选择一个可用连接。那这样的话，我们是不是就可以让负载均衡在选择连接的时候，区分一下是否是刚启动不久的应用？对于刚启动的应用，我们可以让它被选择到的概率特别低，但这个概率会随着时间的推移慢慢变大，从而实现一个动态增加流量的过程。\n首先对于调用方来说，我们要知道服务提供方启动的时间，这个怎么获取呢？我这里给出两 种方法，一种是服务提供方在启动的时候，把自己启动的时间告诉注册中心；另外一种就是 注册中心收到的服务提供方的请求注册时间。\n调用方通过服务发现获取服务提供方的IP地址，并通过负载均衡算法选择一个可用连接进行RPC调用。为了实现动态增加流量的过程，可以让负载均衡在选择连接时区分是否是刚启动不久的应用。可以通过以下两种方法获取服务提供方的启动时间：一种是服务提供方在启动时告知注册中心自己的启动时间，另一种是注册中心记录服务提供方的注册时间。[gpt 总结]\n延迟加载 [1]\r#\r上述问题的解决方法是将应用启动过程中注册服务的步骤延迟到应用启动完成后，以避免在应用启动未完成时接受请求。此外，可以在应用启动完成后，预先加载和初始化相关资源，如缓存数据，以降低请求处理错误的概率。 [gpt总结]\n参考\r#\r《14 | 优雅启动：如何避免流量打到没有启动完成的节点？》 "},{"id":28,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E8%B5%84%E6%BA%90%E5%9B%9E%E6%94%B6/redisLRU/","title":"Redis LRU算法","section":"资源回收","content":"\nLRU 算法的实现\r#\rRedis 对近似 LRU 算法的实现分成了三个部分\n全局 LRU 时钟值的计算\r#\r这部分包括，Redis 源码为了实现近似 LRU 算法的效果，是 如何计算全局 LRU 时钟值的，以用来判断数据访问的时效性；\n全局 LRU 时钟值就是通过 getLRUClock 函数计算得到的。 如果一个数据前后两次访问的时间间隔小于 1 秒，那么这 两次访问的时间戳就是一样的。 serverCron 函数作为时间事件的回调函数，本身会按照一定的频率周期性执行，其频率值 是由 Redis 配置文件 redis.conf 中的 hz 配置项决定的。hz 配置项的默认值是 10，这表 示 serverCron 函数会每 100 毫秒（1 秒 /10 = 100 毫秒）运行一次。 这样，在 serverCron 函数中，全局 LRU 时钟值就会按照这个函数的执行频率，定期调用 getLRUClock 函数进行更新，如下所示：\nint serverCron(struct aeEventLoop *eventLoop, long long id, void *clientData) ... unsigned long lruclock = getLRUClock(); //默认情况下，每100毫秒调用getLRUClock函数更 atomicSet(server.lruclock,lruclock); //设置lruclock变量 ... } #hz默认设为10，提高它的值将会占用更多的cpu，当然相应的redis将会更快的处理同时到期的许多key，以及更精确的去处理超时。 #hz的取值范围是1~500，通常不建议超过100，只有在请求延时非常低的情况下可以将值提升到100。 hz 10 键值对 LRU 时钟值的初始化与更新\r#\r这部分包括，Redis 源码在哪些函数中对每个键值对对应的 LRU 时钟值，进行初始化与更新；\n近似 LRU 算法的实际执行\r#\r这部分包括，Redis 源码具体如何执行近似 LRU 算法，也就是何时触发数据淘汰，以及实际淘汰的机制是怎么实现的。\n何时触发算法执行？ 算法具体如何执行？ 参考\r#\r《15 | 为什么LRU算法原理和代码实现不一样？》\n"},{"id":29,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/Arch-Redis-Cluster/redisCluster/","title":"Redis Cluster","section":"Arch \u0026 Redis Cluster","content":"\n目录\r#\rCluster 功能 [1.1]\r#\rRedis Cluster支持多个master，每个master又可以挂载多个slave\n读写分离 支持数据的高可用 故障转移， 高可用 由于Cluster自带Sentinel的故障转移机制，内置了高可用的支持， 无需再去使用哨兵功能\n由对应的集群来负责维护节点、插槽和数据之间的关系\nSlot\r#\rRedis集群有16384个哈希槽每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽.\n分片方式\r#\rcrc（key）% 16384 映射关系\r#\rkey （→ CRC） 分片 分片 (→ 映射关系) 实例\nslot分配\r#\r自动分配slot create() 手动分配slot meet() addSlot() 指令\r#\rMOVED指令， ASK 指令 最大槽数是16384的原因 [1.3]\r#\r[ redis 作者解答] why redis-cluster use 16384 slots? #2576 antirez\n[中文翻译] 正常的心跳数据包带有节点的完整配置，可以用幂等方式用旧的节点替换旧节点，以便更新旧的配置。 这意味着它们包含原始节点的插槽配置，该节点使用2k的空间和16k的插槽，但是会使用8k的空间（使用65k的插槽）。同时，由于其他设计折衷，Redis集群不太可能扩展到1000个以上的主节点。 因此16k处于正确的范围内，以确保每个主机具有足够的插槽，最多可容纳1000个矩阵，但数量足够少，可以轻松地将插槽配置作为原始位图传播。请注意，在小型群集中，位图将难以压缩，因为当N较小时，位图将设置的slot / N位占设置位的很大百分比。\n[解读]\n消息头太大， 发送的心跳包过于庞大 如果槽位为65536，发送心跳信息的消息头达8k，发送的心跳包过于庞大。 在消息头中最占空间的是myslots[CLUSTER_SLOTS/8]。当槽位为65536时，这块的大小是:65536÷8÷1024=8kb\n在消息头中最占空间的是myslots[CLUSTER_SLOTS/8]。当槽位为16384时，这块的大小是:16384∶8∶1024=2kb\n因为每秒钟，redis节点需要发送一定数量的ping消息作为心跳包，如果槽位为65536，这个ping消息的消息头太大了，浪费带宽。\n主节点数量上限为1000，无需65536个slot redis的集群主节点数量基本不可能超过个1000个。 集群节点越多，心跳包的消息体内携带的数据越多。如果节点过1000个，也会导致网络拥堵。因此redis作者不建议redis cluster节点数量超过1000个。那么，对于节点数在1000以内的redis cluster集群，16384个槽位够用了。没有必要拓展到65536个。 一致性\r#\rRedis集群不保证强一致性 redis集群不保证强一致性，这意味着在特定的条件下，Redis集群可能会丢掉一些被系统收到的写入请求命令 参考\r#\r10.Redis集群(cluster) 1.1Redis集群介绍 1.2 Redis Cluster 1.3 Redis Cluster "},{"id":30,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%BA%94%E7%94%A8/redisSLO/","title":"Redis SLO","section":"Redis 应用","content":"\nSLO\r#\r命中率 Hit Ratio 每秒命中数量 每秒未命中数量 吞吐 Throughput 操作数 延迟 命令执行平均耗时 容量 expiring/not expiring keys expired/evicted keys 内存使用率 Memory Utilization 活动连接数 Active Connections 参考\r#\r部署一个redis exporter监控所有的Redis实例 6 Crucial Redis Monitoring Metrics You Need To Watch "},{"id":31,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisReliability-1/","title":"Redis雪崩、击穿、穿透","section":"常见问题","content":"\n问题\u0026amp;方案 [1]\r#\r“有损”方案\r#\r服务熔断、服务降级、请求限流这些方法都是属于**“有损”方案**，在保证数据库和整体系统稳定的同时，会对业务应用带来负面影响。 预防式方案\r#\r建议是，尽量使用预防式方案 针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群； 针对缓存击穿，在缓存访问非常频繁的热点数据时，不要设置过期时间； 针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。 缓存击穿\r#\r缓存击穿 原因\r#\r热点数据过期 大量并发请求\n解决方案\r#\r唯一DB请求，共享结果 分布式锁\n缓存穿透\r#\r原因\r#\r指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大不存在的数据。 提交不在数据库中的查询，会击穿缓存，直接到达数据库。\n解决方案\r#\r回种空值 使用bloomfilter 缓存穿透-狗桩效应[3]\r#\r原因\r#\r当有一个极热点的缓存项，它一旦失效会有大量请求穿透到数据库，这会对数据库造成瞬时极大的压力，我们把这个场景叫做**“dog-pile effect”（狗桩效应）**\n解决方案\r#\r后台线程定时加载 在代码中，控制在某一个热点缓存项失效之后启动一个后台线程，穿透到数据库，将数 据加载到缓存中，在缓存未加载之前，所有访问这个缓存的请求都不再穿透而直接返回。\n设置分布式锁 通过在 Memcached 或者 Redis 中设置分布式锁，只有获取到锁的请求才能够穿透到数 据库。\n参考\r#\r《26丨缓存异常（下）：如何解决缓存雪崩、击穿、穿透难题？》 【直播回放】海量并发微服务框架设计 V 《15 | 缓存的使用姿势（三）：缓存穿透了怎么办？》 "},{"id":32,"href":"/www6vMiddleware/docs/message/Overview/mqOrdering/","title":"消息系统 顺序消息","section":"Overview","content":"\n顺序消息\r#\r严格顺序消息(场景:数据库复制),单个生产者/消费者[jmq,Rocketmq]\r#\rjmq方案，任意时刻都有两个消息副本 [3] 单个生产者生产，其余生产者会抛出异常[jmq] 单个消费者消费，其余消费者会抛出异常[jmq] 正常流程:消息队列分配在两组以上的BROKER组，每个BROKER组由MASTER-SLAVE组成。消息同步写入单个broker的MASTER。 异常处理流程：Master宕机后，slave只能消费读。生产failover到下一个可用的Broker组，等宕机的broker组的slave消费完，然后消费被挑选到的可用的Broker组的slave 协调者 controller：BROKER组的集群信息在协调者上保存为一个单向的链表，消费者和发送者各有一份独立的链表数据。 rocketmq方案 顺序消息 rocketmq 顺序消息 [6][9][10] 从业务层面来保证消息的顺序而不仅仅是依赖于消息系统.比如，订单号相同的消息会被先后发送到同一个队列中 从业务层面来保证消息的顺序, 而不仅仅是依赖于消息系统 Eg. 订单号相同的消息会被先后发送到同一个队列中 partition 内部有序, 局部有序[kafka]\r#\rpull模型+单线程生产/消费[metaq]\r#\r参考\r#\rjmq顺序消息\r#\r高可用保证消息绝对顺序消费的BROKER设计方案 丁俊 rocketmq顺序消息\r#\r分布式开放消息系统(RocketMQ)的原理与实践 CHEN川 *** 消息的顺序问题 消息的重复问题 聊一聊顺序消息（RocketMQ顺序消息的实现机制） 杭州.Mark producer: selector, 相同orderid的发送到同一个topic; consumer: 多消费者加锁竞争消息 收发顺序消息 阿里云文档 "},{"id":33,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlReliability/","title":"MySQL的主从 高可用 容灾","section":"单机","content":"\nMySQL主从复制原理\r#\r主从复制-流程\r#\rMySQL主从复制\nMySQL master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看) MySQL slave 将 master 的 binary log events 拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 主从复制-类型 [6]\r#\r异步复制 半同步复制 MHA + 半同步复制 全同步复制 MGR + 全同步 主备切换 [1]\r#\r因为readonly设置对超级(super)权限用户是无效的，而用于同步更新的线程，就拥有超级权限。\n一个事务日志同步的完整过程是这样的： 在备库B上通过change master命令，设置主库A的IP、端口、用户名、密码，以及要从哪个 位置开始请求binlog，这个位置包含文件名和日志偏移量。 在备库B上执行start slave命令，这时候备库会启动两个线程，就是图中的io_thread和 sql_thread。其中io_thread负责与主库建立连接。 主库A校验完用户名、密码后，开始按照备库B传过来的位置，从本地读取binlog，发给B。 备库B拿到binlog后，写到本地文件，称为中转日志（relay log）。 sql_thread读取中转日志，解析出日志里的命令，并执行。 Master-Master(双M)循环复制问题 [1]\r#\r如果设置了双M结构，日志的执行流就会变成这样： 从节点A更新的事务，binlog里面记的都是A的server id； 传到节点B执行一次以后，节点B生成的binlog 的server id也是A的server id； 再传回给节点A，A判断到这个server id与自己的相同，就不会再处理这个日志。所以，死循 环在这里就断掉了。 主备延迟 [2]\r#\r主备延迟原因 备库所在机器的性能要比主库所在的机器性能差 备库的压力大 解决方案: I. 一主多从。除了备库外，可以多接几个从库，让这些从库来分担读的压力。 II. 通过binlog输出到外部系统，比如Hadoop这类系统，让外部系统提供统计类查询的能力。 大事务 解决方案: I. 不要一次性地用delete语句删除太多数据 II. 大表DDL场景, 处理方案就是，计划内的DDL，建议使用gh-ost方案. 备库的并行复制能力 [3] 主备切换的策略 [2]\r#\r由于主备延迟的存在，所以在主备切换的时候，就相应的有不同的策略。\n可靠性优先策略 - 数据不丢、安全可靠 可用性优先策略 - 服务可用 小结： 实际的应用中，我更建议使用可靠性优先的策略。 在满足数据可靠性的前提下，MySQL高可用系统的可用性，是依赖于主备延迟的。延迟的时间越小，在主库故障的时候，服务恢复需要的时间就越短，可用性就越高。\n高可用方案-Master高可用[5]\r#\rMMM\r#\r早期，不建议使用\nMHA - 单主 +\r#\rMHA-manager 管理Master 使用半同步复制 缺陷： 只关注到master，对slave关注不够 MySQL Group Replicatoin(MGR) - 单主 [6]+\r#\r单主(荐) 多主(不推荐) 5.7之后支持 raft协议算法，自动选主节点， 自动故障转移 全同步复制, 稳定性高, 强一致性 缺陷： 不支持gap lock(间隙锁)， 隔离级别需设置为read_commited 只能在GTID模式下， 并且日志格式未row格式 不支持对表进行锁操作(lock/unlock table) DDL语句不支持原子性 最多支持9个节点 MySQL Cluster - 多主\r#\r官方亲儿子 NDB engine， 存算分离 实现数据的强一致 缺陷：国内使用少， 配置复杂 Galera Cluster - 多主 +\r#\r三方提供 Master和数据Node部署在一起 WSREP协议来做数据同步 Percona XtraDB(PXC) -多主\r#\r早期 高可用方案 - 数据可靠性[5]\r#\rRaid10( Raid 1+0 )\r#\rSAN共享存储- 贵\r#\rDRBD磁盘复制-系统自带 +\r#\rMySQL Log和可靠性\r#\r{% post_link \u0026lsquo;mysqlLog\u0026rsquo; %}\n容灾 [7]\r#\r参考\r#\r《MySQL是怎么保证主备一致的？》 MySQL实战45讲 丁奇 《MySQL是怎么保证高可用的？》 MySQL实战45讲 丁奇 《备库为什么会延迟好几个小时？》MySQL实战45讲 丁奇 xxx 【IT老齐245】综合对比九种MySQL高可用方案 【IT老齐099】哎，MySQL高可用架构选型要慎重啊！ 腾讯云原生数据库 TDSQL-C异地容灾核心能力构建 MySQL主从复制原理剖析与应用实践 vivo team 未 "},{"id":34,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98/cacheSummary/","title":"缓存(cache)总结","section":"缓存","content":"\n总结\r#\r参考\r#\r应用系统数据缓存设计 淘宝技术部 *** 失效 Local Cache的小TIP 阿里 放翁（文初） xxx xxx xxx xxx cache 58沈剑 "},{"id":35,"href":"/www6vMiddleware/docs/message/Kafka/kafkaConsumer/","title":"Kafka消费者总结","section":"Kafka","content":"\nKafka消费者\r#\r总结\r#\rlag\r#\r消费者\r#\r批量消费 消费者的ZeroCopy:\n直接把消息从文件里发送到网络通道， 而不需要内核与用户态之间数据的来回复制。 Q\u0026amp;A\r#\r怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)\n“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？\n消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?\n有哪些情形会造成重复消费？\nKafka常见的导致重复消费原因和解决方案\n原因3:（重复消费最常见的原因）：消费后的数据，当offset还没有提交时，partition就断开连接。比如，通常会遇到消费的数据，处理很耗时，导致超过了Kafka的session timeout时间（0.10.x版本默认是30秒），那么就会re-blance重平衡，此时有一定几率offset没提交，会导致重平衡后重复消费。\n那些情景下会造成消息漏消费？\nKafka 可靠性总结\n聊聊 Kafka：Kafka 消息丢失的场景以及最佳实践\nKafkaConsumer是非线程安全的，那么怎么样实现多线程消费？\n简述消费者与消费组之间的关系\nKafka的旧版Scala的消费者客户端的设计有什么缺陷？\n参考\r#\rKafka设计解析（四）- Kafka Consumer设计解析 郭俊 Kafka的Lag计算误区及正确实现 朱小厮 《kafka权威指南》 薛命灯 第3，4 ，5章 Kafka Consumer机制优化-保证每条消息至少消费一次 幽灵之使 分区分配策略\nKafka分区分配策略（1）——RangeAssignor 朱小厮\nKafka分区分配策略（2）——RoundRobinAssignor和StickyAssignor 朱小厮\nKafka分区分配策略（3）——自定义分区分配策略 朱小厮\n图解Kafka消费者分区分配策略 石臻臻 kafka contributor *** 未\n"},{"id":36,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/soaGracefulClose/","title":"优雅关闭","section":"容错\u0026限流","content":"\n关闭流程\r#\r关闭流程的优雅处理可以通过以下步骤实现 [gpt 总结]\n服务提供方在关闭时设置一个请求挡板，告知调用方正在关闭并不能处理新的请求。 当服务提供方收到新的业务请求时，直接返回一个特定的异常（如ShutdownException）给调用方。 调用方收到异常响应后，将该节点从健康列表中挪出，并自动将请求重试到其他节点，保证业务无损。 除了等待被动调用外，可以加上主动通知流程，提高实时性并避免通知失败的情况。 通过捕获操作系统的进程信号，如使用Java语言中的Runtime.addShutdownHook方法，在关闭钩子中进行关闭标识的设置和服务对象的安全关闭。 在调用链中加入挡板处理器，当新的请求到来时，判断关闭标识，如果正在关闭，则抛出特定异常。 为了完成正在处理的请求，可以在服务对象上添加引用计数器，在开始处理请求前加一，完成处理后减一，根据引用计数器判断是否有正在处理的请求。 服务对象在关闭过程中拒绝新的请求，并根据引用计数器等待正在处理的请求全部结束后真正关闭。 为避免无法正常退出应用，可以在ShutdownHook中添加超时时间控制，当超过指定时间仍未结束，则强制退出应用。 通过以上步骤，实现了服务提供方的优雅关闭，保证业务正常处理并最大限度地完成正在处理的请求。\n参考\r#\r《13 | 优雅关闭：如何避免服务停机带来的业务损失？》\n"},{"id":37,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/Arch-Redis-Cluster/redisClusterSpec/","title":"Redis Cluster Spec","section":"Arch \u0026 Redis Cluster","content":"\n目录\r#\r设计目标和基本原理[bing]\r#\r高性能和线性扩展\r#\rRedis集群可以支持到1000个节点，使用异步复制和无代理的架构，避免了值合并的开销。\n可接受的写安全性\r#\rRedis集群尽力保留与大多数主节点连接的客户端的写操作，但在分区窗口期内可能会丢失已确认的写操作。\n可用性\r#\rRedis集群可以在大多数主节点可达且每个不可达主节点至少有一个可达副本的情况下存活，并且可以通过副本迁移来提高抗故障能力。\n哈希槽分配\r#\rRedis集群将16384个哈希槽分配给不同的主节点，每个主节点负责存储和服务一部分哈希槽。客户端可以通过计算键的CRC16值对16384取模来得到对应的哈希槽。也可以使用哈希标签来强制将多个键分配到同一个哈希槽，以支持多键操作。\n节点间通信\r#\rRedis集群节点之间使用一个TCP端口和一个二进制协议进行通信，称为集群总线。每个节点都与其他所有节点连接，使用gossip协议来传播集群的信息，如新节点、故障节点、配置变化等。\n客户端重定向\r#\rRedis集群节点不会代理请求，而是根据自己的哈希槽映射表，将客户端重定向到正确的节点。客户端可能收到MOVED或ASK错误，表示需要重新发送请求到另一个节点。MOVED表示哈希槽被永久地分配给了另一个节点，ASK表示哈希槽正在被迁移，需要发送ASKING命令后再发送请求。\n哈希槽迁移\r#\rRedis集群支持在运行时添加和删除节点，这是通过将哈希槽从一个节点移动到另一个节点来实现的。迁移过程中，源节点会将接收到的关于该哈希槽的请求重定向到目标节点，或者直接处理已存在键的请求。目标节点会接收到源节点通过MIGRATE命令发送过来的键，并在迁移完成后更新自己的配置。\n故障检测和副本提升\r#\rRedis集群使用心跳包和gossip协议来检测节点是否可达，并使用PFAIL和FAIL两种标志来表示故障状态。当一个主节点被大多数主节点标记为FAIL时，它的一个副本会发起选举并请求投票。如果获得了大多数主节点的授权，该副本会提升为主节点，并生成一个新的配置版本号（configEpoch）。\n副本选举和配置传播的机制[bing]\r#\r副本选举\r#\r当一个主节点失效时，它的一个副本会尝试发起选举，向其他主节点发送授权请求。如果获得了大多数主节点的回复，它就赢得了选举，并获得了一个新的唯一的配置版本号（configEpoch）。它会开始以主节点的身份广播心跳包，并通知其他节点更新配置。\n配置传播\r#\r当一个节点收到一个心跳包或UPDATE消息时，它会根据一些规则来更新自己的哈希槽映射表。如果一个哈希槽是未分配的，或者有一个新的节点使用更高的configEpoch来声明它，那么接收者会将该哈希槽绑定到新的节点。这样可以保证最后一次故障转移胜出，并且所有节点最终会达成一致。\n节点重置和遗忘\r#\r当一个节点需要从一个集群中移除或加入到另一个集群时，可以使用CLUSTER RESET命令来重置它的状态。这个命令有两种模式：软重置和硬重置。软重置会保留当前的configEpoch，而硬重置会将其设置为0。当一个节点被移除后，其他节点需要使用CLUSTER FORGET命令来删除它在节点表中的条目，并设置一个60秒的禁止期，防止因为gossip协议而重新添加它。\n发布订阅\r#\rRedis集群支持发布订阅功能，客户端可以向任何节点发送SUBSCRIBE或PUBLISH命令。集群会将发布的消息转发到所有其他节点。Redis 7.0及以后版本还支持分片发布订阅功能，其中分片频道根据与键相同的算法分配到哈希槽。分片消息必须发送到拥有该哈希槽的节点，集群会将发布的分片消息转发到该哈希槽所在的分片中的所有节点。\nCRC16算法\r#\r这篇文章最后给出了计算键对应哈希槽的CRC16算法的C语言实现代码。这个算法使用了1021作为多项式，并且不反转输入输出字节。\n参考\r#\rRedis cluster specification ***\n"},{"id":38,"href":"/www6vMiddleware/docs/message/Kafka/kafkaRebalance/","title":"Kafka消费者-Rebalance机制","section":"Kafka","content":"\nRebalance (What)\r#\rRebalance 定义\n在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者 原理\n重平衡的通知机制正是通过心跳线程来完成的 [7] 角色\nconsumer leader cordinator[8]\nRebalance 发生的时机有三个 (when) [1]\r#\r重平衡的 3 个触发条件：\n组成员数量发生变化。(最常遇到)\n订阅主题数量发生变化。\n订阅主题的分区数发生变化。 问题和解决方案\r#\rRebalance 的 弊端 [1]\r#\rRebalance 影响 Consumer 端 TPS\n在 Rebalance 期间，Consumer 会停下手头的事情，什么也干不了 如果你的 Group 下成员很多， Rebalance 会很慢。 Rebalance 效率不高\nGroup 下的 所有成员都要参与进来，而且通常不会考虑局部性原理 consumer rebalance 的问题\r#\rRebalance 过程也和这个类似，在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。 这是 Rebalance 为人诟病的一个方面。\n【消费者重平衡的时候， 所有的消费者是不能消费数据的。】 “不必要的”的Rebalance (Solution) [1]\r#\r第一类非必要 Rebalance 是因为未能及时发送心跳，导致 Consumer 被“踢出”Group 而引发的。 因此，你需要仔细地设置session.timeout.ms 和 heartbeat.interval.ms的 值。\n第二类非必要 Rebalance 是 Consumer 消费时间过长导致的。\nmax.poll.interval.ms参数值的设置显得尤为关键。\n总结\r#\rQ\u0026amp;A\r#\r消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器） 参考\r#\r《17 | 消费者组重平衡能避免吗? 》 胡夕 《15丨消费者组到底是什么？》 胡夕 《25 | 消费者组重平衡全流程解析》 胡夕 Kafka的Rebalance机制可能造成的影响及解决方案 线上Kafka突发rebalance异常，如何快速解决？ 为什么消费客户端频繁出现Rebalance？ 石臻臻 Kafka消费者客户端心跳请求 石臻臻 什么是Kafka消费组协调器 石臻臻 "},{"id":39,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/distributedDatabaseGlobalTime/","title":"分布式数据库-全局时钟","section":"分布式","content":"\nOverview [2]\r#\rTSO 和 HLC 的区别[N AI]\r#\rTSO (Timestamp Ordering) 和 HLC (Hybrid Logical Clock) 都是两种分布式系统中用于解决事件顺序问题的算法。 虽然两种算法都是基于时间戳的，但它们也有着一些区别。\n时间同步\r#\rTSO 需要依赖全局的时间同步服务，如 NTP (Network Time Protocol)，保证所有机器的时钟是一致的。而 HLC 可以在没有全局时钟同步服务的情况下，通过增加逻辑时钟来维护全局时钟的顺序。 这就使得 HLC 比 TSO 更加灵活和适用于更多的场景。\n时钟漂移\r#\rTSO 可能会受到时钟漂移的影响，当时钟出现漂移时，事件的顺序可能会受到影响。而 HLC 可以通过逻辑时钟来解决时钟漂移的问题。这使得 HLC 更加鲁棒，并且可以在更长的时间内保持正确的事件顺序。\n时间精度\r#\rTSO 采用 64 位的时间戳，精度为纳秒级别。而 HLC 采用 64 位的时间戳和 16 位的逻辑时钟，精度为微秒级别，比 TSO 更高。这意味着 HLC 可以更准确地记录事件发生的顺序。\nHLC\r#\rHLC的特点和缺点 [1]\r#\rHLC（Hybrid Logical Clock 混合逻辑时钟）结合了物理时间和逻辑时钟的优点，提供了下面3个特性：\n如果a happened-before b, 则hlc(a) \u0026lt; hlc(b)。不过反之不成立 HLC的时间戳占用的bit数不变 误差有上届。（其实就是NTP的最大误差，也就是所有机器的物理时间的最大误差） HLC本质上来说还是一个逻辑时钟，所以它只能提供partial ordering，而不能提供total ordering。所以2个节点中发生的独立事件a和b，如果他们的timestamp在误差的上届范围内，是无法排序的。\nHLC算法[2]\r#\r参考\r#\rCockRoachDB分布式事务 - HLC和MVCC的相映成趣 《05 | 全局时钟：物理时钟和逻辑时钟你Pick谁？》 分布式数据库30讲 王磊 "},{"id":40,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisDbConsistent/","title":"Redis和数据库之间的一致性","section":"常见问题","content":"\n“数据的一致性”含义\r#\r缓存中有数据，那么，缓存的数据值需要和数据库中的值相同； 缓存中本身没有数据，那么，数据库中的值必须是最新值。\n对于“读写缓存”的情况\r#\r如果我们采用同步写回策略，那么可以保证缓存和数据库中的数据一致。 所以，我们要在业务应用中使用事务机制，来保证缓存和数据库的更新具有原子性， 也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写。\n对于”只读缓存”的情况\r#\r参考：\r#\r25丨缓存异常（上）：如何解决缓存和数据库的数据不一致问题？\n"},{"id":41,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlIndex/","title":"MySQL的索引和优化","section":"单机","content":"\n索引-结构\r#\r索引分类[7]\r#\r分类 含义 特点 关键字 主键索引 针对于表中主键创建的索引 默认自动创建，只能有一个 PRIMARY 唯一索引 避免同一个表中某数据列中的值重复 可以有多个 UNIQUE 常规索引 快速定位特定数据 可以有多个 全文索引 全文索引查找的是文本中的关键词，而不是比较索引中的值 可以有多个 FULLTEXT 分类 含义 特点 聚集索引(Clustered Index) 将数据存储与索引放一块，索引结构的叶子节点保存了行数据 必须有，而且只有一个 二级索引(Secondary Index) 将数据与索引分开存储，索引结构的叶子节点关联的是对应的主键 可以存在多个 聚集索引选取规则: 如果存在主键，主键索引就是聚集索引 如果不存在主键，将使用第一个唯一（UNIQUE）索引作为聚集索引。 如果表没有主键，或没有合适的唯一索引，则InnoDB会自动生成一个rowid作为隐藏的聚集索 引。 索引结构和存储引擎 [3]\r#\r索引的数据结构： B+树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数\nindex MyISAM InnoDB Memory B-Tree\n（balanced 平衡的） 支持 支持 支持 Hash 不支持 不支持 支持 R-Tree 空间索引 支持 不支持 不支持 Full-text 支持 支持 不支持 复合索引的数据结构\r#\rcreate table people {\rlast_name,\rfirst_name,\rdob,\rgender,\rkey(last_name, first_name, dob)\r} 索引- 使用\r#\r索引的使用场景\r#\r索引的使用场景 例子 匹配全值 index (a,b,c) a=1 and b=2 and c=3 范围查找 index a\u0026gt;1 and b\u0026lt;3 匹配最左前缀 index(a，b，c) a OR a，b OR a、b、c OR a，c 会使用 b、c 不使用 仅对索引列进行查询（覆盖索引） index a a=1 匹配列前缀 index （a， b） a like \u0026lsquo;WEER%\u0026rsquo; Index Condition Pushdown（ICP） 减少回表IO 索引的失效 [12][7]\r#\r非复合索引\n索引失效(不会使用index的场景) 例子 解释 在索引列上进行运算操作 substring(phone,10,2) 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 模糊查询, 头部模糊匹配 like \u0026ldquo;%NI\u0026rdquo; 字符串类型字段使用时，不加引号[隐式转换] lastname=1 不使用索引 lastname=\u0026lsquo;1\u0026rsquo; 使用索引 隐式类型转换， 隐式字符编码转换，等价于在索引字段上做函数操作而导致了全索引扫描。 or连接条件 index a a=3 or c=6 or d=9 如果or前的条件中的列有索引，而后面的列中没有索引，那么涉及的索引都不会被用到. 当or连接的条件，左右两侧字段都有索引时，索引才会生效。 复合索引[7]\n最左前缀原则 如果索引关联了多列（联合索引），要遵守最左前缀法则，最左前缀法则指的是查询从索引的最左列开始，并且不跳过索引中的列。如果跳跃某一列，索引将部分失效（后面的字段索引失效）。 范围查询 联合索引中，出现范围查询(\u0026gt;,\u0026lt;)，范围查询右侧的列索引失效。 explain select * from tb_user where profession = \u0026#39;软件工程\u0026#39; and age \u0026gt;= 30 and status = \u0026#39;0\u0026#39;; 索引-优化\r#\r索引维护\r#\r页分裂， 性能会受影响， 整体空间利用率降低大约50%。 页合并，页分裂的逆过程。\n自增主键\r#\r自增主键的插入数据模式，正符合了递增插入的场景。每次插入一条 新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。\n除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如 字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？ 由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的 叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是 8个字节。显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。 这样，非主键索引占用的空间最小。\n所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。 [自增主键使得索引值是顺序插入的，而不是随机插入的， insert时性能更高。 顺序插入同时也减少了页分裂]\n覆盖索引(优化手段)\r#\r如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值 已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面， 索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引.\n覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段. [不需要回表， 不需要回到聚集索引里查询]\n索引下推 ICP [14]\r#\r索引-性能分析[7]\r#\r查看执行频次\r#\rSHOW GLOBAL STATUS LIKE \u0026#39;Com_______\u0026#39;; 慢查询日志\r#\rshow profiles\r#\r## 查看每一条SQL的耗时情况: mysql\u0026gt; show profiles; explain\r#\rtype：表示连接类型，性能由好到差的连接类型为 NULL、system、const、eq_ref、ref、range、index、all possible_key：可能应用在这张表上的索引，一个或多个 Key：实际使用的索引，如果为 NULL，则没有使用索引 Key_len：表示索引中使用的字节数，该值为索引字段最大可能长度，并非实际使用长度，在不损失精确性的前提下，长度越短越好 rows：MySQL认为必须要执行的行数，在InnoDB引擎的表中，是一个估计值，可能并不总是准确的 参考\r#\r《深入浅出MySQL：数据库开发、优化与管理维护》\nMySQL索引背后的数据结构及算法原理\n理解MySQL——索引与优化\nxxx\n剖析Mysql的InnoDB索引 ***\n可能是全网最好的MySQL重要知识点 已失效\n黑马程序员 MySQL数据库入门到精通 P75-P82 P72 mysql_note 笔记1 MySQL 索引 笔记2\nxxx\nali canal\n《MySQL实战45讲 - 深入浅出索引（上）》 丁奇\n《MySQL实战45讲 - 深入浅出索引（下）》 丁奇\n《Java性能调优实战 - 34 | MySQL调优之索引：索引的失效与优化》 刘超 还要再整理\nMySQL索引（二）B+树在磁盘中的存储\nB+树索引并不能直接找到行，只是找到行所在的页，通过把整页读入内存，再在内存中查找。 聚集索引的存储在物理上并不是连续的，每个数据页在不同的磁盘块，通过一个双向链表来进行连接。\n五分钟搞懂MySQL索引下推\nMySQL索引原理及慢查询优化 美团 未 ***\n业界难题-“跨库分页”的四种方案 58沈剑 未\n"},{"id":42,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/%E7%BC%93%E5%AD%98/cache/","title":"缓存(cache)机制","section":"缓存","content":"\n缓存数据分布模式\r#\r分片模式 （sharding）\r#\r数据正交分散到大量机器， 可以做到线性的可伸缩性， 但是实现可用性比较困难。还有一个好处是可以通过集群做负载均衡来实现数据的管理。 根据CAP理论， 在高可用的场景下， 数据分区的容忍性需要牺牲一定的一致性。 相应各个副本的同步策略也有不同， 主要可分为同步和异步方式。由客户端做一致性hash把数据分片存放到服务端。\n复制模式\r#\r数据在集群中存在多个副本。 在本地缓存的集群中，数据会复制到所有的节点中，没有网络延迟和等待时间的集群成员都是可用的。 多副本通过低延迟访问来提供高性能。 在修改数据时需要复制新的版本数据到所有的副本， 在高并发修改的场景下会限制系统的可伸缩性， 副本数不会调的太高。\n本地缓存\r#\r在同一机房内的， 需要适量考虑本地cache， 数据压缩传输等以节省内网数据传输量 。 跨机房的缓存有地域区分，用户往往访问同一机房， 这样可以做本地的cache。 机房之间通过队列的方式进行异步和压缩传输，以提高用户请求的相应度。\n缓存与场景\r#\r1. 非共性数据缓存 eg. 微博， 博客个人首页\r#\r问题：缓存所有的数据性价比不高，　命中率不高 解决方案：\nI. 热点缓存。\r#\r只缓存那些热点的数据。可以缓存在线的用户，　缓存热销的商品，　缓存热点用户的数据。热点规则表示如何匹配到一个热点，即这个查询请求是否请求了热点数据。 根据2/8原则，小部分的数据占用了大部分的访问量。 这也就是twitter page cache 是40%，而不是90%的原因。 II. 非热点数据，\r#\r可以采用nosql技术（redies），可以把它看成可持久化的缓存。 原理类似虚拟内存，理论上不受内存大小的限制。使用NoSQL来做缓存，我们可以把一些不常访问、不怎么更新的数据也缓存起来。比如论坛、新闻的老数据、数据列表的靠后的页面，虽然用户访问不多，但是搜索引擎爬虫会访问，也可能导致系统负载上升。 从外存拿数据减少了计算的开销 ，由于其数据库结构的简单，从磁盘获取一次数 据也比从数据库一次耗时的查询划算很多。\nIII. read-only缓存\r#\r缓存是read-only的， 如果有cache数据的更新， 把cache置为失效的。 如果有多个副本，这样做能够减少replication更新数据的开销， 只需要发送置失效的消息即可。\n2. 高并发更新场景\r#\rI. 悲观锁方案\r#\r高并发更新， 缓存会超时的场景可以使用mutex锁。如 首页top 10, 由数据库加载到memcache缓存n分钟 微博中名人的content cache, 一旦不存在会大量请求不能命中并加载数据库 在加载数据库之前先增加一个mutex key作为锁， 成功之后再去做加载数据库， 如果加锁失败则sleep，之后重试读取原cache数据。为了防止死锁，锁也需要设置过期时间。\nII. 乐观锁方案\r#\rMVCC是后验性的，读不阻塞写，写也不阻塞读，等到提交的时候才检验是否有冲突，由于没有锁，所以读写不会相互阻塞，从而大大提升了并发性能。修改过的副本带着版本号元数据， 多个副本在合并时， 根据版本检测冲突， 并合并数据。\nMemcache 通过客户端cas命令实现乐观锁。 Jboss在3.0实现了mvcc。 MVCC 提供了非阻塞 (non-blocking) 读操作 ( 它并不会去阻塞 wirter threads) ，在避免死锁的同时也提供了更高级的并发机制。它采用了 fail-fast 机制，如果写操作得到了一个 write lock ，那么它们也是依次进行，不允许重叠。\nRedies作为缓存的最佳实践\r#\r对于全局公用的，构建成本比较低的数据， 可以采用一致性hash， 无复制， 无持久化的方案。 如果缓存crash了，可以快速重新构建。 对于与用户相关的， 一致性要求比较低的， 构建成本较低的， 可以采用多对一的复制方式，多个小容量的节点复制到同一个大容量的节点， 但不提供持久化， 提供较高的可用性。 对于与用户相关的，一致性要求比较高的， 构建成本比较高，但存储占用量不高的场景下， 需要持久化， 并且一对一的复制方式， 提供最高的可用性。 案例： Twitter缓存体系 Twitter: 逻辑缓存\n－ page cache api － fragment cache 1. 原始数据的冗余 2. 结构上的冗余 数据源cache － vector cache － Row cache\nPage， fragment － 全局与局部的分离， api， 业务逻辑 Vector, row cache – 索引与内容的分离 Google gfs cache 缓存类型： Read-through 读贯穿 Write-trrough 写贯穿 Write-behind\n参考\r#\rNoSQL架构实践（三）——以NoSQL为缓存 大型网站架构系列之五,缓存策略设计概要 失效 Memcache mutex设计模式 深入理解JBoss Cache3.0——Naga * 极端事务处理模式：Write-behind缓存 多版本并发控制(MVCC)在分布式系统中的应用 "},{"id":43,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/Arch-Redis-Cluster/redisHA/","title":"Redis 集群  容灾（同城多活）","section":"Arch \u0026 Redis Cluster","content":"\n目录\r#\r1. 背景\r#\rRedis 集群自身已经具备了高可用的特性，即使几个Redis节点异常或者挂掉，Redis 集群也会实现故障自动转移，对应用方来说也可以在很短时间内恢复故障。 但是，如果发生了机房故障(断电、断网等极端情况)，Redis集群节点全部挂掉（过半主节点挂掉），会造成集群服务不可用，对于核心业务来说是不可接受的。 为了应对机房故障情况，保障在这种极端情况下，核心业务仍然可以正常访问 Redis 服务，本文将给出适合我司的 Redis 跨机房高可用解决方案。 2. 目标\r#\r保障单机房整体故障时 Redis 缓存服务正常运行。 单机房故障 RTO：30s，RPO：1s 3. 解决方案\r#\r3.1 核心能力\r#\r客户端流量路由：支持按一定的策略，把流量分流到不同机房；机房故障后，流量自动流向其他机房。 服务端故障转移：支持机房故障后，当前机房原来主节点的从节点，通过选举自动倒换成新的主节点。 管理平台正常服务：任一机房故障，Renault 管理平台使用不受影响。 3.2 工作模式\r#\r组件在机房高可用场景下一般有多种工作模式，典型的有单集群模式及多集群模式。本节描述组件在各种工作模式下的部署方式及工作机制。\n部署方案 方案说明 跨机房混合部署 将Redis集群主节点平均分配到各个机房，主从节点在不同机房 各机房独立部署集群 + 数据单向同步 各机房均独立部署集群，集群为热备模式，写请求均写入同一个集群，然后同步到其他集群 各机房独立部署集群 + 数据双向同步 每个机房部署一套Redis集群，同步核心业务写请求 数据同步方法：\n数据同步方案 方案说明 客户端双写 客户端同时写入到各个集群 客户端代理 Netflix开源实现的Dynomite，通过代理层接受数据后写入各个需要同步的节点\n改造难度大\n客户端需要优化添加Dyno Client服务端每个节点需要部署Dyno Node\nread/write性能较原生差异较大，主要体现在write上，之间的差异随着node节点数越多越严重 伪从节点 基于Redis的Master-Slave复制协议，实现低延时、高可用的Redis多数据中心、跨公网数据复制携程开源系统：https://github.com/ctripcorp/x-pipe阿里RedisShake同步工具 写事件监听+MQ跨集群消息同步 读写在本机房，监听写事件 + MQ消息同步到其他机房需要开启事件通知（PUB），修改Redis配置文件中的 notify-keyspace-events 配置（默认的redis并没有开启这个功能）需要独立服务订阅写事件（SUB），并同步到其他集群依赖MQ组件 不考虑客户端代理、发布订阅写事件\n下面详细比较如下四种方案\n方案一：跨机房混合部署 方案二：伪从节点+单向同步 方案三：伪从节点+双向同步 方案四：客户端双写+写监听服务+MQ消息队列 3.2.1 方案一：跨机房混合部署\r#\r3.2.1.1 部署方式\r#\rRedis集群主节点平均分配到各个机房，每个机房都有一个分片的副本；单个机房主节点数据占比不能过半。\n[pic]\n跨机房部署注意事项\n从节点选举需要过半主节点投票，因此不适合双机房部署，至少需要3机房 业务请求访问响应时间会不稳定，同机房请求延迟在0.1ms，跨机房请求在1-3ms 3.2.1.2 工作机制\r#\r流量路由\n默认读主节点（ReadFrom：Master） 读从节点（ReadFrom: Replication） 随机读主从（ReadFrom: Any） 优先读本地机房（ReadFrom：LocalDC）— TODO：新增路由策略 故障切换\n机房故障后，Redis 集群高可用机制，会将集群在30s内自动倒换并恢复正常访问 同城机房间网络传输响应延迟2ms内，几乎不影响集群故障判定 Redis 集群故障转移后，客户端30s内自动刷新集群拓扑关系 故障恢复\n机房恢复后，故障节点 Redis 会以从节点角色自动启动，并全量同步主节点数据（数据同步流量风暴 \u0026ndash; 会在机房间路由器端口上限速控制） 迁移方案\n服务核心业务 Redis 集群将会改造成跨三机房部署模式，不影响客户端正常使用 业务根据实际情况，可将读请求路由策略修改为优先从本地机房读 Redis 故障恢复机制\n投票选主：只有持有槽的主节点才会处理故障选举消息，获得N/2+1以上选票的从节点将为新主。\n替换主节点：取消复制 → clusterDelSlot/clusterAddSlot把槽委派给自己 → 向集群广播通知变为主节点并接管了故障主节点的槽信息。\n全量同步过程 从服务器连接主服务器，发送SYNC命令； 主服务器接收到SYNC命名后，开始执行BGSAVE命令生成RDB文件，并使用缓冲区记录此后执行的所有写命令； 主服务器BGSAVE执行完后，向所有从服务器发送RDB文件； 从服务器收到快照文件后丢弃所有旧数据，载入收到的RDB快照； 主服务器快照发送完毕后，开始向从服务器发送缓冲区中的写命令； 从服务器完成对快照的载入，开始接收命令请求，并执行来自主服务器缓冲区的写命令。 全量同步数据评估，假如100GRedis集群，三机房，一主二从 根据Renault公共集群统计，8G使用容量的Redis，生成RDB文件大小约3G左右，生成时间75s RDB传输，单一机房故障恢复时，将有10台Redis执行主从同步，约有30G流量从其他两个机房流入。 RDB加载，3G RDB数据加载时间约90s 3.2.2 方案二：伪从节点+单向同步\r#\r3.2.2.1 部署方式\r#\r双机房部署，业务读写默认集群，开发部署 Renault 复制服务（RRS, Renault Replicate Service），伪装从节点实时将默认集群数据同步到备用集群。\n[pic]\n3.2.2.2 工作机制\r#\r流量路由\n双机房独立部署，均访问默认集群（热），故障后访问备用集群（冷） 故障切换\n默认集群故障时，SDK熔断默认集群请求，并将流量切换到备用集群 故障恢复\n机房故障恢复后，原默认集群将被reset，RRS 服务将反向同步 迁移方案\n服务核心业务 Redis 集群将会搭建备用集群，并实时同步默认集群数据 业务客户端需要升级 SDK，支持自动故障切换功能 DC2机房业务客户端自动故障切换：\n[pic]\n故障恢复后（反向同步）：\n[pic]\n3.2.3 方案三：伪从节点+双向同步\r#\r3.2.3.1 部署方式\r#\r双机房独立部署，均访问本地集群；开发部署 Renault 复制服务（RRS, Renault Replicate Service），伪装从节点实时双向同步。\n[pic]\n3.2.3.2 工作机制\r#\r流量路由\n双机房独立部署，业务优先访问本地机房集群（TODO：需要开发一种集群选择策略） RRS 服务双向同步数据（需要解决双向同步成环问题） 故障切换\nDC1机房集群故障时候，上层流量自动切换到DC2机房，Redis层不需要处理 故障恢复\nDC1机房集群故障恢复，首先通过RRS服务从DC2全量同步数据，之后增量同步 迁移方案\n服务核心业务 Redis 集群将会搭建双活集群，并通过 RRS 服务实时双向同步 业务客户端需要升级 SDK，支持启动时选择哪个集群 数据库层面的多活，双向同步存在以下困难：\n两边都改了如何解决冲突？\n跨机房双写无法保证缓存的一致性，需要应用侧可以容忍对应的缓存不一致场景 RRS复制中断或者故障如何处理？\n数据同步如何防环？\n方法1：通常需要应用方配置使用，按业务类型分流（不能为通用解决方案） 方法2：数据层面添加字段标识数据源（带来一定开销，并且对于数值计算类数据不能添加标识） 方法3：x-pipe - 定制Redis，在内容分发上做处理，服务端能够识别不同的链接类型，在同步数据之初便加以控制在内容分发上做处理，服务端能够识别不同的链接类型，从而做到有的放矢，在同步数据之初便加以控制 3.2.4 方案四：客户端双写+写监听服务+MQ消息队列\r#\r3.2.4.1 部署方式\r#\r双机房/三机房每个独立部署，应用客户端均访问本地集群；借助MQ通过异步双写机制双写同步到其他集群。\n[pic]\n3.2.4.2 工作机制\r#\r流量路由\n多机房独立部署，业务优先访问本地机房集群（同方案三） MQ中间件+写事件监听服务做双写同步 故障切换\nDC1机房集群故障时候，上层流量自动切换到DC2机房，Redis层不需要处理（同方案三） 故障恢复\nDC1机房集群故障恢复，首先通过RRS服务从DC2全量同步数据，之后增量同步（同方案三） 迁移方案\n服务核心业务 Redis 集群将会搭建多活集群，并通过 MQ 发布订阅方式实现多集群间双向同步 业务客户端需要升级 SDK，支持启动时选择哪个集群 双机房/三机房每个独立部署，多活部署主要问题：\n两边都改了如何解决冲突？\n跨机房双写无法保证缓存的一致性，需要应用侧可以容忍对应的缓存不一致场景，应用如果依赖缓存强一致性，则不合适该方案。\nDC1和DC2两边都写了同一个Key，最终互相覆盖 如何保证消息的顺序？如何保证消息成功发送及消费？\n3.3 成本比较\r#\r部署成本 改造成本 使用成本 方案一：跨机房混合部署 成本-低适用于三机房及以上2副本，增加50%容量 改造成本-低只用按照要求部署集群SDK路由-优先读本地机房 性能有下降本机房访问不受影响，跨机房访问延迟1-3ms 方案二：伪从节点+单向同步 成本-中新增备用集群，增加100%容量部署同步服务 改造成本-中需要开发集群间数据同步服务（已有待完善）读写分离 性能有下降本机房访问不受影响，跨机房访问延迟1-3ms读可优化成读本地机房，性能不受影响 方案三：伪从节点+双向同步 成本-中各机房均新增集群，双机房增加100%容量，三机房增加200%容量部署同步服务 改造成本-高双向复制成环很难解决，需要定制Redis两边都写，冲突问题难解决需要开发集群间数据同步服务（已有待完善） 性能不受影响读写本地机房 方案四：客户端双写+写监听+MQ消息同步 成本-高各机房均新增集群，双机房增加100%容量，三机房增加200%容量部署写监听服务部署MQ消息集群 改造成本-中依赖MQ中间件 性能稍有下降同步写跨机房访问，性能会严重下降异步写性能稍有下降应用能容忍两边不一致场景 方案选定：\n目前同城多活选用方案一（跨机房混合部署） 未来异地多活再考虑方案三和四 3.4 场景建议\r#\r方案一适应场景\n适用于三机房及以上【两机房故障时候无法成功选举】 要能接受写缓存时间在1-2ms内【不可避免跨集群写数据，目前sh1读写tx1时延约1.3ms，sh1读写sh1时延约0.2ms】 配置优先读本地机房的话，能接收读数据时延【主从数据同步毫秒级时延】 其他场景建议\n不接受写缓存慢场景、不接受读从节点数据延迟场景\n服务端：每个机房都要独立部署Redis集群，并且双向同步数，推荐方案三 SDK改造：需要支持优先读写本地Redis集群路由策略 业务端：升级SDK版本 4. 里程碑\r#\r描述组件为了达成机房高可用需要做的事情，包括事项、优先级、预期完成时间、负责人等信息。\n对于中间件，一般有如下重要时间点：\n服务端机房高可用方案设计完成 服务端机房高可用方案在测试集群验证通过 客户端机房高可用方案设计开发完成 业务集群机房高可用方案验证：明确 RPO 和 RTO 方案一跨机房混合部署改造事项：\n管理平台：\n搭建核心集群 机器添加机房标识 (Done)， 机架标识，是否属于同一机架， 机房高可用校验 （Done） 主从节点数至少3个，且分布在不同机房 单机房主节点数不能过半 集群调整，确保核心集群Redis实例机房按大集群要求部署， 核心应用识别及迁移 核心缓存标识 - 源于用户？核心应用标识？ 新增核心缓存将绑定到核心集群，非核心缓存绑定到公共集群 缓存客户端监控（CAT QPS）集成到管理平台 - 便于缓存迁移时，从技术角度判定是否需要迁移 核心缓存迁移 已迁移（arch-100%，cl-100%） 待迁移（md、mkt、yw）（需要推动升级客户端并迁移） 客户端：\n读写分离，优先读本机房实例 ， 熟悉Lettuce 读写分离相关源码，确定可行方案（方案可行 - 选择NEAREST或者自定义ReadFrom） NEAREST：Read from any node of the cluster with the lowest latency. https://lettuce.io/core/release/reference/ 自定义ReadFrom，优先从本机房读，本机房无实例则从Master节点读 （done） 用户在 Apollo 配置读取数据方式（Master/Slaver/Any/Nearest/LocalZone） Redis集群：\n故障演练， repl-timeout 60s适当大小； 完整操作机房Redis实例下线、Redis实例恢复操作 缓存恢复时的数据同步抑制方法， （Done，需要时在交换机端口上限速） 机房故障后，强制下线故障机房Redis实例 机房恢复后，重新部署Redis实例，逐一添加到集群，并调整合理主从关系 ZK \u0026amp; Apollo ，\n切换演练 在线客户端列表对比（老zk的会清零，新zk临时节点数目和原来的一致） 将线上ZK从zk1切换到zk3 2021-11-30 Redis 集群机房高可用方案整体设计完成\n2021-12-31 客户端路由开发、跨集群测试验证通过\n2022-2-28 推广接入试点，及真实数据故障演练\n2022-3-31 业务集群机房高可用方案验证\n5. 相关文档\r#\r途虎机房高可用方案 同城多活资源成本需求调研 6. 参考资料\r#\r美团KV存储架构及实践 *** 华为容灾多活策略 Redis异地多活行业方案 携程Redis多数据中心复制管理系统 携程异地多活-MySQL实时双向（多向）复制实践 Redis Cluster多机房高可用实现\u0026ndash;基于客户端 同城双活-Redis篇 饿了么实时双向复制工具 CKV+异地容灾探索和实践 *** 未\n干货 | 携程Redis跨IDC多向同步实践 未\n干货 | 五大实例详解，携程 Redis 跨机房双向同步实践 未\n阿里云数据库全新功能Redis读写分离，全维度技术解析 未\n企业打开Redis的正确方式，来自阿里云云数据库团队的解读 未 Figure 2：Redis异地多活架构方案示意图\nhttps://github.com/ctripcorp/x-pipe 未\n"},{"id":44,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisSlowResponse/","title":"Redis 慢查询排查","section":"常见问题","content":"\n慢查询 排查\r#\r获取 Redis 实例在当前环境下的基线性能。\n是否用了慢查询命令？ 如果是的话，就使用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。\n是否对过期 key 设置了相同的过期时间？ 对于批量删除的 key，可以在每个 key 的过期时间上加一个随机数，避免同时删除。\n是否存在 bigkey？ 对于 bigkey 的删除操作，如果你的 Redis 是 4.0 及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是 Redis 4.0 以前的版本，可以使用SCAN 命令迭代删除；对于 bigkey 的集合查询和聚合操作，可以使用 SCAN 命令在客户端完成。\nRedis AOF 配置级别是什么？业务层面是否的确需要这一可靠性级别？ 如果我们需要高性能，同时也允许数据丢失，可以将配置项 no-appendfsync-on-rewrite 设置为 yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。 如果既需要高性能又需要高可靠性，最好使用高速固态盘作为 AOF 日志的写入盘。\nRedis 实例的内存使用是否过大？发生 swap 了吗？ 如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现Redis 和其他内存需求大的应用共享机器的情况。\n在 Redis 实例的运行环境中，是否启用了透明大页机制？ 如果是的话，直接关闭内存大页机制就行了。\n是否运行了 Redis 主从集群？ 如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。\n是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？ 使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。\n参考\r#\r19 | 波动的响应延迟：如何应对变慢的Redis？（下） Diagnosing latency issues\n"},{"id":45,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/distributedDatabaseJoinQuery/","title":"数据库  关联查询","section":"分布式","content":"\nTiDB 的 Join 算法[2][5]\r#\rTiDB 的 Join 算法包括如下几类：\nHash Join Merge Join Index Hash Join Index Merge Join TiDB 目前表 Join 的方式 [3][4]\nSort Merge Join Index Nested Loop Join Hash Join 参考\r#\r《20 | 关联查询：如何提升多表Join能力？ 》 TiDB 查询优化及调优系列（二）TiDB 查询计划简介 查看 Join 的执行计划 TiDB 源码阅读系列 TiDB 查询优化及调优系列（四）查询执行计划的调整及优化原理 JOIN 查询的执行计划 比较 "},{"id":46,"href":"/www6vMiddleware/docs/message/Kafka/kafkaReliability/","title":"Kafka 可靠性总结","section":"Kafka","content":"\nKafka高可靠配置\r#\r位置 配置项 可靠性 topic的配置 replication.factor\u0026gt;=3,即副本数至少是3个 复制因子\nreplication.factor(topic级别) default.replication.factor(broker级别) - 2\u0026lt;=min.insync.replicas\u0026lt;=replication.factor 最少同步副本min.insync.replicas 3副本（总）\n+ 3副本，一般最少同步2副本 + 最少同步2副本时，如2副本挂了，这时不能写，只能读.\n设置为1，单副本挂了，就会丢数据【3】 broker的配置 leader的选举条件unclean.leader.election.enable=false unclean.leader.election -\u0026gt; broker级别 1.允许不同步的副本成为首领 ，有数据不可靠的风险.\n2.不允许不同步的副本成为首领 ，降低了可用性. 3. 强烈建议不要开启它，还可以通过其他的方式来提升可用性 producer的配置 request.required.acks=-1(all)【6】 producer.type=sync【7】 如何确保消息不会丢失\r#\r生产阶段\r#\r在编写发送消息代码时，需要注意，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失。\n捕获消息发送的错误，并重发消息。 异步发送时，则需要在回调方法里进行检查。这个地方是需要特别注意的，很多丢消息的原因就是，使用了异步发送，却没有在回调中检查发送结果。 存储阶段\r#\r通过配置刷盘和复制相关的参数，让消息写入到多个副本的磁盘上，来确保消息不会因为某个 Broker 宕机或者磁盘损坏而丢失。\nEg. 在 RocketMQ 中，需要将刷盘方式 flushDiskType 配置为 SYNC_FLUSH 同步刷盘\nEg. 表. kafka高可靠配置 topic的配置 消费阶段\r#\r在处理完全部消费业务逻辑之后，再发送消费确认。 检测消息丢失的方法\r#\r可以利用消息队列的有序性来验证是否有消息丢失。在 Producer 端，我们给每个发出的消息附加一个连续递增的序号，然后在 Consumer 端来检查这个序号的连续性。\nQ\u0026amp;A\r#\r怎么样才能确保Kafka极大程度上的可靠性？ Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）\nKafka 可靠性总结 参考:\r#\rKafka设计解析（六）- Kafka高性能架构之道 郭俊 kafka数据可靠性深度解读 朱忠华 《Kafka权威指南》 第6 章可靠的数据传递 薛命灯 《消息队列高手课 - 如何确保消息不会丢失？》 李玥 "},{"id":47,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlTransaction/","title":"MySQL事务-总结","section":"单机","content":"\nMySQL Log和事务\r#\r{% post_link \u0026lsquo;mysqlLog\u0026rsquo; %}\n参考\r#\r拨开云雾见天日：剖析单机事务原理 CHEN川 *** 多版本并发控制(MVCC)在分布式系统中的应用 Todd 阿里云分布式缓存OCS与DB之间的数据一致性 杨成虎 乐观锁和 MVCC 的区别？ mysql可重复读和幻读实例 CWeeYii MySQL脏读、虚读、幻读 Eternity味道 MySQL 中事务的实现原理 失效 "},{"id":48,"href":"/www6vMiddleware/docs/RMDB/%E5%88%86%E5%B8%83%E5%BC%8F/globalSecondaryIndex/","title":"全局二级索引-GSI","section":"分布式","content":"\n场景\r#\r[Claude] 分布式数据库的全局二级索引主要有以下几个使用场景: 加速多表关联查询:用全局二级索引构建外键索引,可以有效地加速涉及多表关联的查询。 全表扫描查询:当需要做全表扫描时,使用全局二级索引可以跳过不需要的行,大幅降低扫描成本。 频繁查询的列上:对于数据库中经常需要根据某列进行查询的情况,建立该列的全局二级索引,可以大幅提高查询效率。 需要排序的查询:当需要查询结果按某个列排序时,全局二级索引可以加速排序。 模糊查询优化:通过部分键值匹配,全局二级索引可以优化like查询等模式查询。 分页查询优化:结合 Cursor 标记,全局二级索引可以有效的支持分页查询。\n总的来说,对于需要高效查询、排序、混合查询等复杂查询的分布式数据库而言,引入全局二级索引可以大大提高性能。尤其是处理大规模数据时,二级索引发挥的作用更加明显。\n全局二级索引能够解决什么问题 [1] 如果查询的维度与逻辑表的拆分维度不同，会产生跨分片查询。跨分片查询的增加会导致查询卡慢、连接池耗尽等性能问题。GSI能够通过增加拆分维度来减少跨分片查询，消除性能瓶颈。\n增加拆分维度 [4] 例如，对于在线商城的订单表，假设按照买家用户维度拆分，那么对于卖家查询（例如，查询某个卖家的本月所有订单）就需要扫描所有分区。但是借助全局二级索引，可以仅仅扫描相应卖家所在的索引表分区，快速找到所需的订单信息。\n全局唯一约束 [4] 例如，假设用户表是一张分布式表，按照用户ID分区。若要求用户手机号需要全局唯一，那么本地索引无法满足，必须构建一个按手机号作为索引键（同时也是分区键）的唯一索引。\n全局索引 vs 局部索引 [1][2]\r#\r全局二级索引 [DDIA 基于关键词的二级索引分区] 数据行和对应的索引行保存在不同分片上\n分类 [3] 全局非分区索引（Global Non-Partitioned Index） 全局分区索引（Global Partitioned Index） 局部索引 [DDIA 基于文档的二级索引分区] 如果数据行和对应的索引行保存在相同分片上\nPolarDB 全局索引\r#\rPolarDB-X [1]\nXA多写，保证主表与索引表数据强一致。[性能会不会慢] Online Schema Change，添加GSI不锁主表。 PolarDB-X GSI [4] 每个GSI对应一张分布式索引表，和其他分布式表一样，按照指定的分区规则水平拆分为多张物理表。PolarDB-X使用分布式事务维护主表和索引表之间数据强一致。\n参考\r#\r全局二级索引 PolarDB 设计数据密集型应用-C6-分区和二级索引 DDIA 二级索引 OB 全局二级索引 PolarDB PolarDB-X 全局二级索引 未 PolarDB-X 数据分布解读（三） ：TPCC与透明分布式 未 "},{"id":49,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisHotkey/","title":"Redis 热点Hotkey","section":"常见问题","content":"\nHotKey\r#\r寻找热点Key [2]\n客户端 [1] 缺点: 只能统计单个客户端 代理 缺点: 代理端的开发部署成本 服务端 缺点: monitor命令, 短时间用 机器 packetbeat 抓包 缺点: 运维部署和机器成本 解决热点key [2]\n拆分复杂数据结构 迁移热点key 本地缓存加通知机制 [3] 使用二级缓存 [1] 参考\r#\r有赞透明多级缓存解决方案（TMC） 《Redis 开发与运维》 12.5 京东 Redis 热点数据探测工具镜像 京东毫秒级热key探测框架设计与实践，已实战于618大促 热点 Key 问题的发现与解决 阿里 文档 已失效 通常的解决方案主要集中在对客户端和Server端进行相应的改造。 I. 服务端本地缓存. II. 服务端分布式缓存。\n阿里云方案 (整体看是用中间件 负载均衡 水平扩展)\nI. 读写分离方案解决热读 写 热备; 读 存储水平扩展 II. 热点数据解决方案 该方案通过主动发现热点并对其进行存储来解决热点Key的问题。\n"},{"id":50,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlLog/","title":"MySQL Logs","section":"单机","content":"\nMySQL Log和事务[0]\r#\rredo log\r#\r有了redolog之后，当对缓冲区的数据进行增删改之后，会首先将操作的数据页的变化，记录在redo log buffer中。在事务提交时，会将redo log buffer中的数据刷新到redo log磁盘文件中。过一段时间之后，如果刷新缓冲区的脏页到磁盘时，发生错误，此时就可以借助于redo log进行数据恢复，这样就保证了事务的持久性。 因为在业务操作中，我们操作数据一般都是随机读写磁盘的，而不是顺序读写磁盘。 而redo log在往磁盘文件中写入数据，由于是日志文件，所以都是顺序写的。顺序写的效率，要远大于随机写。 这种先写日志的方式，称之为** WAL（Write-Ahead Logging）**。 undo log\r#\r回滚日志，用于记录数据被修改前的信息 , 作用包含两个 : **提供回滚(保证事务的原子性) 和MVCC(多版本并发控制) **。\nundo log和redo log记录物理日志不一样，它是逻辑日志。可以认为当delete一条记录时，undolog中会记录一条对应的insert记录，反之亦然，当update一条记录时，它记录一条对应相反的update记录。\n总结\r#\rLog 性质 记录内容 ACID redo log 物理日志[记录内容] wal 保证事务的持久性[D] undo log 物理日志[记录内容] 被修改前的信息，提供回滚 保证事务的原子性[A] ​ binlog 逻辑日志[记录操作]\nMySQL Log和可靠性\r#\rbinlog的三种格式[1]\r#\rstatement\nrow格式\nmixed格式: statement or row格式\n因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。 但row格式的缺点是，很占空间。比如你用一个delete语句删掉10万行数据，用statement的话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果用row格式的binlog，就要把这10万条记录都写到binlog中。这样做，不仅会占用更大的空间，同时写binlog也要耗费IO资源，影响执行速度。 所以，MySQL就取了个折中方案，也就是有了mixed格式的binlog。mixed格式的意思是，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式，否则就用statement格式。 也就是说，mixed格式可以利用statment格式的优点，同时又避免了数据不一致的风险.因此，如果你的线上MySQL设置的binlog格式是statement的话，那基本上就可以认为这是一个不合理的设置。你至少应该把binlog的格式设置为mixed。\nredo log 和 undo log [4]\r#\rredo Log\nWAL日志，保证事务持久性 buffer楼盘之前数据库意外宕机， 可以进行数据的恢复 undo log\n保证事务原子性， 回滚或者事务异常，可以回滚到历史版本 实现MVCC的必要条件 事务开始. 记录A=1到undo log. 修改A=3. 记录A=3到redo log.( 先写内存， 后同步到磁盘中) 记录B=2到undo log. 修改B=4. 记录B=4到redo log.( 先写内存， 后同步到磁盘中) 将redo log写入磁盘。 事务提交 参考\r#\r黑马程序员 MySQL数据库入门到精通 P138 - P140 mysql_note 笔记1 MySQL 索引 笔记2 ***\n《MySQL是怎么保证主备一致的？》 MySQL实战45讲 丁奇\n《云数据库架构》 1.1.5 1.1.7 - 阿里云\n"},{"id":51,"href":"/www6vMiddleware/docs/message/Kafka/kafkaIndex/","title":"Kafka 索引","section":"Kafka","content":"\nKafka索引\r#\r稀疏索引 LogSegment 构成\r#\r$ tree /tmp/kafka-logs/t1-1/ /tmp/kafka-logs/t1-1/ ├── 00000000000000000000.index ├── 00000000000000000000.log ## 位移索引 ├── 00000000000000000000.timeindex ## 时间戳索引 └── leader-epoch-checkpoint Index类型\r#\r位移索引 [3] 假设要查找偏移量为230的消息，查找过程如下： 首先找到baseOffset=217的日志段文件（这里使用了跳跃表的结构来加速查找） 计算相对偏移量relativeOffset=230-217=13 在索引文件中查找不大于13的最大相对偏移量对应的索引项，即[12,456] 根据12对应的物理地址456，在日志文件.log中定位到准确位置 从日志文件物理位置456继续向后查找找到相对偏移量为13，即绝对偏移量为230，物理地址为468的消息 时间戳索引 [3] 假设要查找时间戳为1540的消息，查找过程如下（这里时间戳只是一个示意值）： 将要查找的时间戳1540和每个日志段的最大时间戳逐一对比，直到找到最大时间戳不小于1540的日志段。（日志段的最大时间戳：获取时间戳索引文件最后一个索引项的时间戳，如果大于0，取该值；否则取日志段的最近修改时间） 找到对应的日志段后，在时间戳索引文件中使用二分查找找到不大于目标时间戳1540的最大索引项，即图中的[1530,12]，获取对应的相对偏移量12 在该日志段的偏移量索引文件中找到相对偏移量不大于12的索引项，即图中的[12，456] 在日志文件中从物理位置456开始查找时间戳不小于1540的消息 位移索引 vs 时间戳索引 [2] 改进版二分查找算法 [1]\r#\r在位移索引 和 时间戳索引中都使用二分查找算法\n示例 现在，最新索引项保存在 Page #13 中。如果要查找最新索引项，原版二分查找算法将会 依次访问 Page #0、7、10、12 和 13。此时，问题来了：Page 7 和 10 已经很久没有被 访问过了，它们大概率不在页缓存中，因此，一旦索引开始征用 Page #13，就会发生 Page Fault，等待那些冷页数据从磁盘中加载到页缓存。根据国外用户的测试，这种加载过程可能长达 1 秒。\n日志模块 Q\u0026amp;A\r#\r简述Kafka的日志目录结构 Kafka中有那些索引文件？ 如果我指定了一个offset，Kafka怎么查找到对应的消息？ 如果我指定了一个timestamp，Kafka怎么查找到对应的消息？ 聊一聊你对Kafka的Log Retention的理解 聊一聊你对Kafka的Log Compaction的理解 聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层） 参考\r#\r《04 | 索引（上）：改进的二分查找算法在Kafka索引的应用》 《05丨索引（下）：位移索引和时间戳索引的区别是什么？》 深入理解Kafka服务端之索引文件及mmap内存映射 *** "},{"id":52,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlDeadLock/","title":"MySQL  锁和死锁","section":"单机","content":"\n锁\r#\r行锁， 锁优化 [3]\r#\r在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放.[todo 加个例子]\n行锁是通过索引实现的，如果不通过索引条件检索数据，那么 InnoDB 将对表中所有的记录进行加锁。\n行锁的具体实现算法有三种：record lock、gap lock 以及 next-key lock。\nrecord lock是专门对索引项加锁； gap lock 是对索引项之间的间隙加锁； next-key lock 则是前面两种的组合，对索引项以其之间的间隙加锁。 只在可重复读或以上隔离级别下的特定操作才会取得 gap lock 或 next-key lock，在Select 、Update 和 Delete 时，除了基于唯一索引的查询之外，其他索引查询时都会获取gap lock 或 next-key lock，即锁住其扫描的范围。 隐式锁和显示锁\r#\r显示锁 SELECT \u0026hellip; LOCK IN SHARE MODE(加共享锁); SELECT \u0026hellip; FOR UPDATE(加排他锁);\n死锁\r#\r死锁和死锁检测 [5]\r#\r当出现死锁以后，有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout来设置。 innodb_lock_wait_timeout的默认值是50s。 实际中不用这种策略。\n另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事 务得以继续执行。将参数 innodb_deadlock_detect 设置为on，表示开启这个逻辑。\n带来的问题：每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。\n一种解决思路是控制并发度：并发控制要做在数据库服务端。如果有中间件，可以考虑在中间件实现；如果-团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新，-在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。\n另一种解决思路是在应用层上优化:你可以考虑通过将一行改成逻辑上的多行来减少锁冲突。 比如，一个账户1条记录变10条记录。\n预防死锁 [7]\r#\r减少长事务 大事务拆成小事务 保证加锁顺序一直 业务允许的情况下，降低隔离级别 RR几倍下会有间隙锁，会提高死锁发生的概率 死锁的排查和解决 [7]\r#\r通过日志系统及时通知死锁事件 通过ELK做通知 结合业务代码与死锁日志 进行分析 通过 pt-deadlock-logger 监控死锁 查看最近一次的死锁日志\nshow engine innodb status 案例\r#\rCase [1]\r#\r原因\r#\r死锁是在并发环境下，两个或多个事务互相等待对方持有的资源而无法继续执行的情况。在上文中，死锁的产生是因为两个事务A和事务B都持有间隙(4,+∞）的gap锁，并且两个事务都在等待对方释放锁，导致循环等待而造成死锁。\n解决方案\r#\rinnodb_lock_wait_timeout 超时时间 - 通用 避免死锁最直观的方法就是在两个事务相互等待时，**当一个事务的等待时间超过设置的某一 阈值，就对这个事务进行回滚，另一个事务就可以继续执行了。**这种方法简单有效，在 InnoDB 中，参数 innodb_lock_wait_timeout 是用来设置超时时间的。\n替换 幂等性校验 - 非通用 我们还可以使用其它的方式来代替数据库实现幂等性校验。例如，使用 Redis 以及 ZooKeeper 来实现，运行效率比数据库更佳。\n参考\r#\r《35 | 记一次线上SQL死锁事故：如何避免死锁？》 刘超 《33 | MySQL调优之事务：高并发场景下的数据库事务调优》 刘超 《07 | 行锁功过：怎么减少行锁对性能的影响？》 MySQL实战45讲 丁奇 MYSQL死锁的检测与预防 "},{"id":53,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/redisHitRate/","title":"Redis  命中率","section":"常见问题","content":"\n命中率不高\r#\r会增加接口响应时间\n提高redis命中率 [3][4]\r#\r缓存粒度 缓存粒度越小，命中率越高 合理调整缓存有效期的时间 避免缓存同时失效 预加载 防止缓存击穿和穿透[5] 增加存储容量 容量不足时会触发Redis内存淘汰机制 清空策略 [6] FIFO(first in first out) LFU(less frequently used) LRU(least recently used) 参考\r#\r如何提高redis缓存命中率 关于如何提高缓存命中率（redis） {% post_link \u0026lsquo;redisReliability-1\u0026rsquo; %} self 缓存那些事 "},{"id":54,"href":"/www6vMiddleware/docs/message/Kafka/kafkaZeroCopy/","title":"Kafka-ZeroCopy","section":"Kafka","content":"\nIndex 中的mmap\r#\r在 AbstractIndex 中，这个 MappedByteBuffer 就是名为 mmap 的变量。 接下来，我用注释的方式，带你深入了解下这个 mmap 的主要流程。\n@volatile protected var mmap: MappedByteBuffer = { // 第1步：创建索引文件 val newlyCreated = file.createNewFile() // 第2步：以writable指定的方式（读写方式或只读方式）打开索引文件 val raf = if (writable) new RandomAccessFile(file, \u0026#34;rw\u0026#34;) else new Rando try { if(newlyCreated) { if(maxIndexSize \u0026lt; entrySize) // 预设的索引文件大小不能太小，如果连一个索引 throw new IllegalArgumentException(\u0026#34;Invalid max index size: \u0026#34; + m // 第3步：设置索引文件长度，roundDownToExactMultiple计算的是不超过maxInde // 比如maxIndexSize=1234567，entrySize=8，那么调整后的文件长度为1234560 raf.setLength(roundDownToExactMultipl 这些代码最主要的作用就是创建 mmap 对象。AbstractIndex 其他大部分的操作都是和 mmap 相关。\nAbstractIndex：这是 Kafka 所有类型索引的抽象父类，里面的 mmap 变量是实现索引机制的核心。\nmmap+write [4][5]\r#\r4次context切换 消费者中的sendfile() [6]\r#\rkafka的mmap是对写的优化，还是读的优化？ 参考\r#\r动画讲解：Kafka为什么快之零拷贝技术 \u0026ndash; mmap\n{% post_link \u0026lsquo;zeroCopy\u0026rsquo; %} self\n动画讲解：Kafka为什么快之零拷贝技术-sendfile()函数 ***\nKafka Zero-Copy 使用分析 transferTo() 未\nlinux零拷贝原理，RocketMQ＆Kafka使用对比 * 未\nRocketMQ入门（上） 未\n"},{"id":55,"href":"/www6vMiddleware/docs/RMDB/%E5%8D%95%E6%9C%BA/mysqlMasterSlaveDelay/","title":"MySQL 主从延迟","section":"单机","content":"\n案例 [1]\r#\r案例一：主库DML请求频繁\r#\r解决思路 如果是MySQL 5.7以下的版本，可以做分片(sharding)，通过水平扩展(scale out)的方法打散写请求，提升写请求写入binlog的并行度。 MySQL 5.7以上的版本， 在MySQL 5.7，使用了基于逻辑时钟(Group Commit)的并行复制。 而在MySQL 8.0，使用了基于Write Set的并行复制。 案例二：主库执行大事务\r#\r解决思路 拆分大事务语句到若干小事务中，这样能够进行及时提交，减小主从复制延时。 案例三：主库对大表执行DDL语句\r#\r解决思路 避免业务高峰，尽量安排在业务低峰期执行 ； set sql_log_bin=0后，分别在主从库上手动执行DDL（此操作对于某些DDL操作会造成数据不一致，请务必严格测试） 参考\r#\r高可用数据库主从复制延时的解决方案 《26 | 备库为什么会延迟好几个小时？》 未 "},{"id":56,"href":"/www6vMiddleware/docs/message/Kafka/kafkaController/","title":"Kafka Controller-控制器","section":"Kafka","content":"\n选举控制器的规则:\r#\r第一个成功创建 /controller 节点的 Broker 会被指定为控制器。\n作用\r#\r主题管理(创建、删除、增加分区)\n分区重分配\nkafka-reassign-partitions\nPreferred 领导者选举\nPreferred 领导者选举主要是 Kafka 为了避免部分 Broker 负载过重而提供的一种换 Leader 的方案。\n集群成员管理(新增 Broker、Broker 主动关闭、Broker 宕机)\n包括自动检测新增 Broker、Broker 主动关闭及被动宕机。\n这种自动检测是依赖于前面提到的 Watch 功能和 ZooKeeper 临时节点组合实现的。\n数据服务\n控制器上保存了最全的集群 元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存 中的缓存数据。\n参考\r#\r直击Kafka的心脏——控制器 \u0026laquo;26 | 你一定不能错过的Kafka控制器\u0026raquo; 胡夕\n"},{"id":57,"href":"/www6vMiddleware/docs/message/Kafka/kafkaReplica/","title":"Kafka Replication-副本机制","section":"Kafka","content":"\nPartition\r#\r首领 leader ， 首领副本 跟随者 follower， 跟随者副本 首选首领 Kafka的副本机制\r#\rAR = ISR + OSR。\nISR 不只是追随者副本集合，它必然包括 Leader 副本。\nISR中副本有主从之分，但是读写都是主副本， 从副本只负责拉取主副本的数据。 好处（一致性方面）\n方便实现**“Read-your-writes”** 方便实现单调读（Monotonic Reads） Kafka 判断 Follower 是否与 Leader 同步的标准,不是看\u0026quot;相差的消息数\u0026quot;，而是看\u0026quot;落后的时间\u0026quot;。\n落后的时间就是 Broker 端参数 replica.lag.time.max.ms 参数值。\n这个参数的含义是Follower 副本能够落后 Leader 副本的最长时间间隔，当前默认值是 10 秒。\nUnclean 领导者选举（Unclean Leader Election）\nKafka 可靠性总结 self\nQ\u0026amp;A\r#\r失效副本是指什么？有那些应对措施？\n怎么样判定一个分区是否有副本是处于同步失效状态的呢？从Kafka 0.9.x版本开始通过唯一的一个参数replica.lag.time.max.ms（默认大小为10,000）来控制，当ISR中的一个follower副本滞后leader副本的时间超过参数replica.lag.time.max.ms指定的值时即判定为副本失效，需要将此follower副本剔出除ISR之外。\nKafka解析之失效副本 优先副本是什么？它有什么特殊的作用？ 多副本下，各个副本中的HW和LEO的演变过程 为什么Kafka不支持读写分离？ 参考：\r#\rKafka核心技术与实战 - 23丨Kafka副本机制详解 胡夕 "},{"id":58,"href":"/www6vMiddleware/docs/message/Kafka/kafkaElection/","title":"Kafka 中的选主","section":"Kafka","content":"\nKafka中的选主[1]\r#\r/ 组件 详细 kafka Controller leader 依赖zk选主, kafka只有一个Controller Partition leader[2] leader在ISR中 Consumer leader的选举 消费组内的消费者选举出一个消费组的leader zookeeper zk自身的选主 Zab协议(原子广播+奔溃恢复) 其他系统依赖zk选主 使用zk的临时节点， session结束， 临时节点消失 Q\u0026amp;A\r#\r~~Kafka中有那些地方需要选举？~~这些地方的选举策略又有哪些？ 参考\r#\rKafka科普系列 | 原来Kafka中的选举有这么多？ 朱小厮\nKafka科普系列 | 原来Kafka中的选举有这么多\n你想知道的所有关于Kafka Leader选举流程和选举策略都在这(内含12张高清大图,建议收藏) 石臻臻 *** 未\n"},{"id":59,"href":"/www6vMiddleware/docs/message/Kafka/kafkaQ-A/","title":"Kafka  Q\u0026A","section":"Kafka","content":"\n基础\r#\rKafka的用途有哪些？使用场景如何？\nKafka中的ISR、AR又代表什么？ISR的伸缩又指什么\nKafka中的HW、LEO、LSO、LW等分别代表什么？\ntopic\r#\r当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？\ntopic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？\n创建topic时如何选择合适的分区数？\nKafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？\nKafka有哪几处地方有分区分配的概念？简述大致的过程及原理\nProducer\r#\rKafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？ Kafka生产者客户端中使用了几个线程来处理？分别是什么？ 特性\r#\r聊一聊Kafka的延时操作的原理\nKafka科普系列 | 轻松理解Kafka中的延时操作\n这里就涉及到了Kafka延迟操作的概念。Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给follower副本。\n延迟操作不只是拉取消息时的特有操作，在Kafka中有多种延时操作，比如延时数据删除、延时生产等。\nKafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）\nKafka中是怎么体现消息顺序性的？\nKafka的那些设计让它有如此高的性能？\n补齐\r#\rKafka中怎么实现死信队列和重试队列？\nKafka中怎么做消息审计？\nKafka中怎么做消息轨迹？\n监控\r#\rKafka中有那些配置参数比较有意思？聊一聊你的看法\nKafka有哪些指标需要着重关注？\n其他\r#\r在使用Kafka的过程中遇到过什么困难？怎么解决的？\n还用过什么同质类的其它产品，与Kafka相比有什么优缺点？\nKafka有什么优缺点？\nKafka总结\n"},{"id":60,"href":"/www6vMiddleware/docs/message/Kafka/kafkaTransaction/","title":"Kafka 幂等性和事务","section":"Kafka","content":"\nKafka 幂等性\r#\r为了实现生产者的幂等性， 引入了producer id（PID）和 序列号（sequence number）\nPID: producer 初始化的时候分配\n序列号： producer每发送一条消息，就会将\u0026lt;PID, 分区\u0026gt;对应的序列号的值+1.\n局限性： Kafka 幂等性只能保证单个producer 回话（session）中单分区的幂等\nKafka 事务\r#\rKafka 幂等性不能跨多个分区运作，而事务可以保证对多个分区写入操作的原子性。 事务性实现的关键\r#\r事务要求producer 开启幂等特性 enable.idempotence = true transactionalId：\n一个Producer 在 Fail 恢复后能主动 abort 上次未完成的事务（接上之前未完成的事务），然后重新开始一个事务，这种情况应该怎么办？\n之前幂等性引入的 PID 是无法解决这个问题的，因为每次 Producer 在重启时，PID 都会更新为一个新值：\nKafka 在 Producer 端引入了一个 transactionalId 来解决这个问题，这个 txn.id 是由应用来配置的； 架构和组件\r#\rtransactionalId和PID一一对应，transactionalId用户显示设置，PID由Kafka内部分配； 跨producer会话的消息幂等发送: 新的producer启动后，具有相同transactionalId的旧producer会立即失效； 跨producer会话的事务恢复: producer宕机后，新的producer可以保证未完成的旧事务要么commit，要么Abort。 TransactionCoordinator(coordinate 协调者) 事务日志 语义\r#\rKafka 的事务机制，更多的情况下被用来配合Kafka的幂等机制来实现 Kafka 的 Exactly Once 语义。 Kafka 的 Exactly Once 机制，是为了解决在**\u0026ldquo;consume - transform - produce\u0026rdquo;（流计算）**这样的计算过程中数据不重不丢，而不是我们通常理解的使用消息队列进行消息生产消费过程中的 Exactly Once。 ”consume - transform - produce“模式 总结\r#\r幂等性、事务都是0.11.0.0之后引入的特性, 以此来实现EOS（Exactly-Once semantics 精确一次性语义）\nQ\u0026amp;A\r#\rKafka中的幂等是怎么实现的 Kafka中的事务是怎么实现的（这题我去面试6家被问4次）\nKafka 幂等性和事务 参考:\r#\r消息队列高手课 - 25 | RocketMQ与Kafka中如何实现事务？ 李玥 Kafka Exactly-Once 之事务性实现 Matt\u0026rsquo;s Blog-柳年思水 \u0026laquo;深入理解Kafka：核心设计与实践原理\u0026raquo; 7.4节 "},{"id":61,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/redisBothAofAndRDB/","title":"Redis 混合持久化","section":"持久化","content":"\nRedis 混合持久化\r#\r数据恢复顺序和加载流程 [2][3]\r#\r在这种情况下， 当redis重启的时候会优先载入AOF文件来恢复原始的数据 ，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。\nRDB和AOF共存时会优先加载AOF文件\n【主从切换 优先加载 RDB -\u0026gt; 速度】 【redis重启 优先加载 AOF -\u0026gt; 数据完整性】\n开启\r#\raof-use-rdb-preamble no -\u0026gt; yes 原理 [3]\r#\rRDB镜像做全量持久化，AOF做增量持久化 先使用RDB进行快照存储，然后使用AOF持久化记录所有的写操作，当重写策略满足或手动触发重写的时候，将最新的数据存储为新的RDB记录。 这样的话，重启服务的时候会从RDB和AOF两部分恢复数据，既保证了数据完整性，又提高了恢复数据的性能。简单来说:混合持久化方式产生的文件一部分是RDB格式，一部分是AOF格式。\nAOF+RDB混合[1]\r#\r而混合使用 RDB 和 AOF，正好可以取两者之长，避两者之短，以较小的性能开销保证数据可靠性和性能。\n参考\r#\r《05丨内存快照：宕机后，Redis如何实现快速恢复？》 redis++：Redis持久化 rdb \u0026amp; aof 工作原理及流程图 （三） RDB-AOF混合持久化 尚硅谷Redis零基础到进阶，最强redis7教程，阳哥亲自带练（附redis面试题） "},{"id":62,"href":"/www6vMiddleware/docs/serviceGovernance/ConfigDiscovery/soaDiscovery/","title":"服务发现","section":"Config \u0026 Discovery","content":"\n机制\r#\rOverview\r#\r服务注册和发现\n模式\r#\rClient-side Discovery Server-side Discovery patterns 实现\r#\r需求 [1]\r#\rRPC 框架依赖的注册中心的服务数据的一致性其实并不需要满足 CP，只要满足 AP 即可。\nFramework\r#\retcd - CP nacos 基于raft协议 zk - CP eureka - AP 参考\r#\r《08 | 服务发现：到底是要CP还是AP？》 "},{"id":63,"href":"/www6vMiddleware/docs/serviceGovernance/security/soaAuth/","title":"服务治理-鉴权","section":"安全","content":"\n备选方案 [1]\r#\r分布式 Session OAuth2.0 JWT CAS OAuth2 和 JWT的关系[gpt4]\r#\rOAuth2和JWT都是用于实现网络应用中的授权和身份验证的技术。但是，它们在实现方式和使用场景上有所不同。\nOAuth2是一个授权框架，它允许第三方应用在用户的许可下访问其私有资源。例如，一个应用可以使用OAuth2获取用户的Facebook或Google账户信息，而无需用户提供他们的用户名和密码。\nJWT（JSON Web Token）则是一种开放标准（RFC 7519），它定义了一种紧凑且自包含的方式，用于在各方之间安全地传输信息作为JSON对象。这些信息可以被验证和信任，因为它们是数字签名的。\nOAuth2和JWT可以一起使用。例如，当一个应用使用OAuth2获取用户的授权时，它可能会接收到一个包含JWT的访问令牌。应用可以解码这个JWT，以获取关于用户的信息，如他们的用户名或电子邮件地址。同时，因为JWT是签名的，应用可以信任这些信息的准确性。\n总的来说，OAuth2和JWT都是实现网络应用授权和身份验证的重要工具，但它们在实现细节和使用方式上有所不同。\n参考\r#\r微服务之用户鉴权中心 {% post_link \u0026lsquo;securityOAuth2\u0026rsquo; %} self "},{"id":64,"href":"/www6vMiddleware/docs/serviceGovernance/Springcloud/springTransactionInvalid/","title":"Spring  Transaction  失效","section":"Spring Cloud","content":"\nSpring事务失效问题\r#\rspring事务失效 [1]\r#\r- 场景：普通方法调用事务方法时，事务会失效 - 解决：要在普通方法(一般是最外层)上加上@Transactional 代理不生效 [2]\r#\r非public修饰的方法 在AbstractFallbackTransactionAttributeSource类的computeTransactionAttribute方法中有个判断，如果目标方法不是public，则TransactionAttribute返回null，即不支持事务。\n被final、static关键字修饰的类或方法 spring事务底层使用了aop，也就是通过jdk动态代理或者cglib，帮我们生成了代理类，在代理类中实现的事务功能。但如果某个方法用final修饰了，那么在它的代理类中，就无法重写该方法，而添加事务功能。\n类方法内部调用 updateStatus方法拥有事务的能力是因为spring aop生成代理了对象，但是这种方法直接调用了this对象的方法，所以updateStatus方法不会生成事务\n解决方案 新加一个Service方法 在该Service类中注入自己 通过AopContent类 当前类没有被Spring管理\n多线程调用 spring的事务是通过数据库连接来实现的。当前线程中保存了一个map，key是数据源，value是数据库连接。 同一个事务，其实是指同一个数据库连接，只有拥有同一个数据库连接才能同时提交和回滚。如果在不同的线程，拿到的数据库连接肯定是不一样的，所以是不同的事务。\n(存储引擎)表不支持事务\n未开启事务 springboot通过DataSourceTransactionManagerAutoConfiguration类，已经默默的帮你开启了事务。 使用的还是传统的spring项目，则需要在applicationContext.xml文件中，手动配置事务相关参数。如果忘了配置，事务肯定是不会生效的。\n将注解标注在接口方法上\n错误使用@Transactional [2]\r#\r错误的传播机制 目前只有这三种传播特性才会创建新事务：REQUIRED，REQUIRES_NEW，NESTED。\n异常被内部catch 如果想要spring事务能够正常回滚，必须抛出它能够处理的异常。如果没有抛异常，则spring认为程序是正常的。\nrollbackFor属性设置错误\n@Transactional(rollbackFor = BusinessException.class) public void add(UserModel userModel) throws Exception { saveData(userModel); updateData(userModel); } 嵌套事务 可以将内部嵌套事务放在try/catch中，并且不继续往上抛异常。这样就能保证，如果内部嵌套事务中出现异常，只回滚内部事务，而不影响外部事务。\n手动抛了别的异常\n参考\r#\rSpring 踩坑之@Transactional 神奇失效 小鱼儿 spring事务（注解 @Transactional ）失效的12种场景 spring中12种@Transactional的失效场景(小结) Spring @Async/@Transactional 失效的原因及解决方案 未 "},{"id":65,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/redisReplica/","title":"Redis 主从复制","section":"持久化","content":"\n主从复制流程\r#\r主从库第一次同步的流程\r#\r主从库增量复制流程\r#\rrepl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己 已经读到的位置。\n缺点 [3]\r#\r主从延迟 复制偏移量：master_repl_offset | slave_repl_offset master挂了怎么办？ 默认情况下，不会在slave节点中自动选一个master 每次都要人工干预 需要Redis哨兵(sentinel) 参考\r#\r《06 | 数据同步：主从库如何实现数据一致？》 redis主从复制、主从延迟知几何 复制原理和工作流程 61_redis主从复制之工作流程总结 V 62_redis主从复制之痛点和改进需求 V "},{"id":66,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E6%95%85%E9%9A%9C%E4%BC%98%E5%8C%96/redisNodeId/","title":"Redis 集群扩容时NodeId问题","section":"Redis 故障\u0026优化","content":"\n1. 背景\r#\r有一次扩容 Redis 集群，往集群加入节点，这个操作了很多次的功能居然失败了。查看到异常日志：addAppClusterSharding:10.3.28.7:10240:10.3.28.70, result is false\n观察发现，Redis实例已启动，并且加入到集群，但是主从角色不正确\n本地调试发现，在从节点执行cluster replicate \u0026lt;master_node_id\u0026gt;报错，提示***，发现命令的\u0026lt;master_node_id\u0026gt;和Redis实例的\u0026lt;node_id\u0026gt;不一致\nRedis node_id是如何生成的，怎么会变化呢？ 临时查看最近修改代码， com.tuhu.renault.portal.redis.impl.RedisCenterImpl getNodeId方法 原来：\n从目标Redis实例获取自己的node_id\n改成了：\n可以从其他Redis实例获取（因为cluster nodes命令可以拿到集群所有节点信息）\n原因是：\n当目标节点挂了，会获取不到node_id，页面操作故障节点下线会失败（用于下线故障节点）\n修复方法：\n临时恢复从目标实例获取自己的node_id;下线故障节点时候从其他节点获取node_id\n下文简单梳理 Redis 集群扩容过程，主要关注 Redis node_id 变化及相关源码。\n2. Redis集群扩容过程梳理\r#\rRenault管理平台入口：\ncom.tuhu.renault.portal.controller.ClusterController doAddHorizontalNodes 方法\n主要包括三个步骤：（1）启动 Redis 实例；（2）meet Redis 实例；（3）确定主从角色。 2.1 启动Redis实例\r#\r根据模板生成Redis配置文件，并拷贝到目标机器 执行：redis-server /usr/local/renault/conf/redis-cluster-6379.conf 没有返回值，检测端口被占用则认为成功 2.2 Redis集群所有主节点meet待加入节点\r#\r确定Redis实例为\u0026quot;单身\u0026quot; 执行：cluster meet ，https://redis.io/commands/cluster-meet 返回“OK”即成功 2.3 确定主从角色\r#\r获取主节点NodeId 从节点实例执行：cluster replicate \u0026lt;node_id\u0026gt;，https://redis.io/commands/cluster-replicate 返回“OK”即成功 3. Redis集群扩容期间节点状态\r#\r3.1 扩容前集群节点状态\r#\rroot@work-arch-renault-redis-3 (16:06:02) ~ # redis-cli -p 6436 cluster nodes 798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642579573335 66 connected 0-5454 68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 myself,master - 0 1642579573000 69 connected 5455-10923 9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642579575338 67 connected 10924-16383 86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642579574337 66 connected ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642579573000 69 connected ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642579572335 67 connected 每行的组成结构：\u0026lt;id\u0026gt; \u0026lt;ip:port\u0026gt; \u0026lt;flags\u0026gt; \u0026lt;master\u0026gt; \u0026lt;ping-sent\u0026gt; \u0026lt;pong-recv\u0026gt; \u0026lt;config-epoch\u0026gt; \u0026lt;link-state\u0026gt; \u0026lt;slot\u0026gt; \u0026lt;slot\u0026gt; ... \u0026lt;slot\u0026gt; 每项的含义如下:\n1.id: 节点ID 2.ip:port：客户端与节点通信使用的地址 3.flags：逗号分割的标记位，可能的值如下 a. myself：当前连接的节点 b. master：节点是master c. slave：节点是slave d. fail?：节点处于PFAIL 状态。 当前节点无法联系，但逻辑上是可达的 (非 FAIL 状态) e. fail：节点处于FAIL 状态. 大部分节点都无法与其取得联系将会将改节点由 PFAIL 状态升级至FAIL状态 f. handshake：还未取得信任的节点，当前正在与其进行握手 g. noaddr：没有地址的节点 h. noflags：连个标记都没有 master_id：如果节点是slave，并且已知master节点，则这里列出master节点ID,否则的话这里列出”-“ ping-sent：最近一次发送ping的时间，这个时间是一个unix毫秒时间戳，0代表没有发送过 pong-recv：最近一次收到pong的时间，使用unix时间戳表示 config-epoch：节点的epoch值（从节点值随主节点） a. 每当节点发生失败切换时，都会创建一个新的，独特的，递增的epoch b. 如果多个节点竞争同一个哈希槽时，epoch值更高的节点会抢夺到 link-state：node-to-node集群总线使用的链接的状态，我们使用这个链接与集群中其他节点进行通信 a. 值可以是 connected 和 disconnected slot：哈希槽值或者一个哈希槽范围，代表当前节点可以提供服务的哈希槽值 3.2 Redis启动后“单身”状态\r#\rroot@work-arch-docker-3 (16:14:17) ~ # redis-cli -p 6379 cluster nodes 77199f3d3b63360117c352f51f0262627a53d215 :6379@16379 myself,master - 0 0 0 connected root@work-arch-docker-3 (16:14:30) ~ # root@work-arch-docker-3 (16:14:30) ~ # redis-cli -p 6380 cluster nodes 60445f6231213b2285f216eac53f67408bdeae28 :6380@16380 myself,master - 0 0 0 connected 3.3 Meet Redis实例1\r#\rroot@work-arch-docker-3 (16:15:16) ~ # redis-cli -p 6379 cluster nodes ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580117000 69 connected 86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580117750 66 connected ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580115000 67 connected 798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580115742 66 connected 0-5454 68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580115000 69 connected 5455-10923 9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580117000 67 connected 10924-16383 77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 myself,master - 0 1642580115000 0 connected 3.4 Meet Redis实例2\r#\rroot@work-arch-docker-3 (16:15:22) ~ # redis-cli -p 6380 cluster nodes 86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580129887 66 connected ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580132000 69 connected ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580130000 67 connected 798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580130590 66 connected 0-5454 68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580133595 69 connected 5455-10923 9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580132593 67 connected 10924-16383 77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 master - 0 1642580131592 0 connected 60445f6231213b2285f216eac53f67408bdeae28 10.100.140.152:6380@16380 myself,master - 0 1642580133000 73 connected 3.5 指定新增实例主从关系\r#\rroot@work-arch-docker-3 (16:16:55) ~ # redis-cli -p 6379 cluster nodes ba7481602474ce44fa964af043c9417670246002 10.100.140.232:6437@16437 slave 68eab9a2f3bae16afd41ae43970f866c01bebcf0 0 1642580214980 69 connected 86107b968a281aedbcddd9267b53dfb4bcaadee4 10.100.140.230:6437@16437 slave 798f8791b4e302df6c5e1418466e91899e199546 0 1642580215985 66 connected 60445f6231213b2285f216eac53f67408bdeae28 10.100.140.152:6380@16380 slave 77199f3d3b63360117c352f51f0262627a53d215 0 1642580213000 73 connected ad0cc373957d84ba9e37114feee68a8414408bf6 10.100.140.234:6436@16436 slave 9a1be71bd5699ecb62b4308212ba1363ea72020f 0 1642580212000 67 connected 798f8791b4e302df6c5e1418466e91899e199546 10.100.140.229:6436@16436 master - 0 1642580212000 66 connected 0-5454 68eab9a2f3bae16afd41ae43970f866c01bebcf0 10.100.140.231:6436@16436 master - 0 1642580213578 69 connected 5455-10923 9a1be71bd5699ecb62b4308212ba1363ea72020f 10.100.140.233:6435@16435 master - 0 1642580214000 67 connected 10924-16383 77199f3d3b63360117c352f51f0262627a53d215 10.100.140.152:6379@16379 myself,master - 0 1642580214000 0 connected 5. 延伸阅读\r#\r5.1 Redis 集群数据结构\r#\r初始化 cluster 中有三个重要数据结构：\nclusterState：记录当前节点视角下，集群目前的状态 clusterNode：记录节点的状态 clusterLink：为clusterNode的属性，记录了连接该节点所需的有关信息 源码文件：cluster.h cluster.c struct clusterState 展开源码 struct clusterNode 展开源码 struct clusterLink 展开源码 5.2 Redis 启动过程\r#\r5.2.1 启动流程\n5.2.2 启动日志 Redis实例1日志\n展开源码\nRedis实例2日志\n展开源码\n5.3 cluster meet处理过程\r#\r5.3.1 meet 命令处理 大致分为 3 个阶段：\nA 通过 meet msg 的 pong 回包，更改 A 对 B 的认识 B 通过 ping msg 的 pong 回包，更改 B 对 A 的认识 来自 A 的 ping or pong msg， B 更新自己看到的 A 的 slot 信息 5.3.2 Gossip消息结构（MEET、PING、PONG） MEET PING PNG message 展开源码\n5.3 揭晓 node_id是如何生成的？为什么会变化呢？\r#\r5.3.1 node_id 如何生成? 新建 redis 节点时候生成，几乎不会改变，除非执行：cluster reset hard\ncluster.c line462 随机40字节字符串，这个值在节点启动的时候，从节点配置文件获取或者创建 clusterLoadConfig /* Load or create a new nodes configuration. */ getRandomHexChars(node-\u0026gt;name, CLUSTER_NAMELEN); cluster.c line532 Only for hard reset: a new Node ID is generated. hard reset多用于测试 cluster reset 命令说明：http://www.redis.cn/commands/cluster-reset.html 5.3.2 Redis 集群扩容时候 node_id 为什么会变化? 其实 Redis 节点的 node_id 是没有变化的 扩容期间，meet 新节点后，立刻从其他节点获取该新节点的 nodei_id，可能拿到的是随机生成的 node_id (不认识的时候是随机生成node_id，认识之后再更新次node_id） "},{"id":67,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E5%85%B6%E4%BB%96/redisNVM/","title":"Redis NVM","section":"其他","content":"\n特点\r#\r能持久化保存数据 读写速度和 DRAM 接近 容量大 Intel Optane AEP 内存条(简称 AEP 内存)\r#\rMemory 模式 软件系统能使用到 的内存空间，就是 AEP 内存条的空间容量。 在 Memory 模式时，Redis 可以利用 NVM 容量大的特点，实现大容量实例，保存更 多数据。\nApp Direct 模式 使用了 App Direct 模式的 AEP 内存，也叫 做持久化内存(Persistent Memory，PM)。\nRedis 可以直接在持久化内存上进行数据读写，在这 种情况下，Redis 不用再使用 RDB 或 AOF 文件了，数据在机器掉电后也不会丢失。 而且，实例可以直接使用持久化内存上的数据进行恢复，恢复速度特别快。\n参考：\r#\r40 | Redis的下一步:基于NVM内存的实践\n"},{"id":68,"href":"/www6vMiddleware/docs/serviceGovernance/loadBalance/loadBalance/","title":"负载均衡-算法","section":"负载均衡","content":"\n负载均衡算法\r#\rLVS[1][2]\r#\r组件 类型 负载均衡算法 场景 LVS 静态 RR：roundrobin [常用] 轮询机制 WRR：Weighted RR [常用] 加权轮询，权重越大承担负载越大 SH：Source Hashing 源地址哈希 实现session sticky缺点：调度粒度大，对负载均衡效果差； DH：Destination Hashing 目标地址哈希 动态 LC：least connections 适用于长连接应用\nOverhead=activeconns*256+inactiveconns WLC：Weighted LC [常用] 默认调度方法,较常用（加上了权重）\nOverhead=(activeconns*256+inactiveconns)/weight SED：Shortest Expection Delay Overhead=(activeconns+1)*256/weight NQ：Never Queue， 第一轮均匀分配，后续SED****SED****算法改进 LBLC：Locality-Based LC 动态的 ****DH****连接算法 LBLCR：LBLC with Replication 带复制功能的LBLC Nginx [3]\r#\r组件 类型 负载均衡算法 场景 Nginx 静态 轮询 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除 加权轮询 指定轮询几率，weight 和访问比率成正比，用于后端服务器性能不均的情况 IP Hash 每个请求按访问 ip 的 hash 结果分配，这样每个访客固定访问一个后端服务器，可以解决 session 的问题 动态 Fair （第三方） 按后端服务器的响应时间来分配请求，响应时间短的优先分配 url_hash（第三方） 按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效 Least Connections HAproxy\r#\r组件 类型 负载均衡算法 场景 HAproxy 静态 轮询 加权轮询 IP Hash 动态 Least Connections Source LVS工作模式[11-16]\r#\rDR TUN模式 NAT模式 参考\r#\rLVS负载均衡集群服务搭建详解 LVS调度算法总结 LVS、Nginx 及 HAProxy 的区别 LVS集群的负载调度 *** 未 LVS工作模式\r#\rLVS三种模式的区别及负载均衡算法 LVS 介绍以及配置应用 *** 深入浅出 LVS 负载均衡（一）NAT、FULLNAT 模型原理 未 深入浅出 LVS 负载均衡（二）DR、TUN 模型原理 未 深入浅出 LVS 负载均衡（三）实操 NAT、TUNNEL 模型 未 深入浅出 LVS 负载均衡（四）实操 DR 模型、Keepalived DR 模型的高可用 未 "},{"id":69,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/throttle/ratelimitSentinel/","title":"流量治理-Sentinel","section":"限流","content":"\nSentinel\r#\r限流 API [4]\r#\r定义资源 资源：可以是任何东西，一个服务，服务里的方法，甚至是一段代码。 try (Entry entry = SphU.entry(\u0026#34;HelloWorld\u0026#34;)) { // Your business logic here. System.out.println(\u0026#34;hello world\u0026#34;); } catch (BlockException e) { // Handle rejected request. e.printStackTrace(); } // try-with-resources auto exit @SentinelResource(\u0026#34;HelloWorld\u0026#34;) public void helloWorld() { // 资源中的逻辑 System.out.println(\u0026#34;hello world\u0026#34;); } 定义规则 规则：Sentinel 支持以下几种规则： 流量控制规则、熔断降级规则、系统保护规则、来源访问控制规则和 热点参数规则。 private void initSystemRule() { List\u0026lt;SystemRule\u0026gt; rules = new ArrayList\u0026lt;\u0026gt;(); SystemRule rule = new SystemRule(); // 规则 rule.setHighestSystemLoad(10); rules.add(rule); SystemRuleManager.loadRules(rules); } FlowRuleManager.loadRules(List\u0026lt;FlowRule\u0026gt; rules); // 流控规则 DegradeRuleManager.loadRules(List\u0026lt;DegradeRule\u0026gt; rules); // 降级规则 SystemRuleManager.loadRules(List\u0026lt;SystemRule\u0026gt; rules); // 系统规则 AuthorityRuleManager.loadRules(List\u0026lt;AuthorityRule\u0026gt; rules); // 授权规则 限流 类型 [2]\r#\r直接失败 [滑动时间窗口] Warmup 预热 [令牌桶算法] 限流排队 [漏桶算法] 分布式限流 [1]\r#\r滑动窗口 [2]\r#\r核心代码 LeapArray.java\n/* * Get bucket item at given time from the array. * * (1) Bucket is absent, then just create a new bucket and CAS update to circular array. * (2) Bucket is up-to-date, then just return the bucket. * (3) Bucket is deprecated, then reset current bucket. */ while (true) { WindowWrap\u0026lt;T\u0026gt; old = array.get(idx); if (old == null) { /// 初始化一个窗口 /* * B0 B1 B2 NULL B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * bucket is empty, so create new and update * * If the old bucket is absent, then we create a new bucket at {@code windowStart}, * then try to update circular array via a CAS operation. Only one thread can * succeed to update, while other threads yield its time slice. */ WindowWrap\u0026lt;T\u0026gt; window = new WindowWrap\u0026lt;T\u0026gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); if (array.compareAndSet(idx, null, window)) { // Successfully updated, return the created bucket. return window; } else { // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); } } else if (windowStart == old.windowStart()) { /// 返回老的窗口 /* * B0 B1 B2 B3 B4 * ||_______|_______|_______|_______|_______||___ * 200 400 600 800 1000 1200 timestamp * ^ * time=888 * startTime of Bucket 3: 800, so it\u0026#39;s up-to-date * * If current {@code windowStart} is equal to the start timestamp of old bucket, * that means the time is within the bucket, so directly return the bucket. */ return old; } else if (windowStart \u0026gt; old.windowStart()) { /// 滚动: 重置老的窗口, 增加新的窗口 /* * (old) * B0 B1 B2 NULL B4 * |_______||_______|_______|_______|_______|_______||___ * ... 1200 1400 1600 1800 2000 2200 timestamp * ^ * time=1676 * startTime of Bucket 2: 400, deprecated, should be reset * * If the start timestamp of old bucket is behind provided time, that means * the bucket is deprecated. We have to reset the bucket to current {@code windowStart}. * Note that the reset and clean-up operations are hard to be atomic, * so we need a update lock to guarantee the correctness of bucket update. * * The update lock is conditional (tiny scope) and will take effect only when * bucket is deprecated, so in most cases it won\u0026#39;t lead to performance loss. */ if (updateLock.tryLock()) { try { // Successfully get the update lock, now we reset the bucket. return resetWindowTo(old, windowStart); /// 清零重置old窗口 } finally { updateLock.unlock(); } } else { // Contention failed, the thread will yield its time slice to wait for bucket available. Thread.yield(); } } else if (windowStart \u0026lt; old.windowStart()) { /// 时钟回拨 // Should not go through here, as the provided time is already behind. return new WindowWrap\u0026lt;T\u0026gt;(windowLengthInMs, windowStart, newEmptyBucket(timeMillis)); } } } Sentinel vs. Hystrix vs. resilience4j [3]\r#\rSentinel Hystrix resilience4j 隔离策略 信号量隔离（并发线程数限流） 线程池隔离/信号量隔离 信号量隔离 熔断降级策略 基于响应时间、异常比率、异常数等 异常比率模式、超时熔断 基于异常比率、响应时间 实时统计实现 滑动窗口（LeapArray） 滑动窗口（基于 RxJava） Ring Bit Buffer 动态规则配置 支持多种配置源 支持多种数据源 有限支持 扩展性 丰富的 SPI 扩展接口 插件的形式 接口的形式 基于注解的支持 支持 支持 支持 限流 基于 QPS，支持基于调用关系的限流 有限的支持 Rate Limiter 集群流量控制 支持 不支持 不支持 流量整形 支持预热模式、匀速排队模式等多种复杂场景 不支持 简单的 Rate Limiter 模式 系统自适应保护 支持 不支持 不支持 控制台 提供开箱即用的控制台，可配置规则、查看秒级监控、机器发现等 简单的监控查看 不提供控制台，可对接其它监控系统 多语言支持 Java / C++ Java Java 开源社区状态 活跃 停止维护 较活跃 参考\r#\r集群流量控制\n【图灵学院】2022最新B站独家分布式限流算法原理与应用讲解视频短合集 V\n常用限流降级组件对比 Sentinel vs. Hystrix\nsentinel （史上最全+入门教程） ***\n流控降级最佳实践 阿里 未\n{% post_link \u0026lsquo;ratelimit\u0026rsquo; %} self\n"},{"id":70,"href":"/www6vMiddleware/docs/serviceGovernance/Gray/apiGatewayGray/","title":"API 网关-灰度发布","section":"灰度发布","content":"\n灰度发布 策略 [2]\r#\r基于权重 百分比 version \u0026hellip; 基于springcloud gateway + nacos实现灰度发布[1]\r#\rspring: application: name: gateway-reactor-gray cloud: nacos: discovery: server-addr: localhost:8848 gateway: discovery: locator: enabled: true lower-case-service-id: true routes: - id: hello-consumer uri: grayLb://hello-consumer ## 灰度负载均衡 predicates: - Path=/hello/** public class GrayLoadBalancer implements ReactorServiceInstanceLoadBalancer { ... private Response\u0026lt;ServiceInstance\u0026gt; getInstanceResponse(List\u0026lt;ServiceInstance\u0026gt; instances,HttpHeaders headers) { if (instances.isEmpty()) { return getServiceInstanceEmptyResponse(); } else { return getServiceInstanceResponseWithWeight(instances); // } } /** * 根据版本进行分发 */ private Response\u0026lt;ServiceInstance\u0026gt; getServiceInstanceResponseByVersion(List\u0026lt;ServiceInstance\u0026gt; instances, HttpHeaders headers) { String versionNo = headers.getFirst(\u0026#34;version\u0026#34;); // System.out.println(versionNo); Map\u0026lt;String,String\u0026gt; versionMap = new HashMap\u0026lt;\u0026gt;(); versionMap.put(\u0026#34;version\u0026#34;,versionNo); ... } /** * 根据在nacos中配置的权重值，进行分发 */ private Response\u0026lt;ServiceInstance\u0026gt; getServiceInstanceResponseWithWeight(List\u0026lt;ServiceInstance\u0026gt; instances) { Map\u0026lt;ServiceInstance,Integer\u0026gt; weightMap = new HashMap\u0026lt;\u0026gt;(); for (ServiceInstance instance : instances) { Map\u0026lt;String,String\u0026gt; metadata = instance.getMetadata(); if(metadata.containsKey(\u0026#34;weight\u0026#34;)){ // weightMap.put(instance,Integer.valueOf(metadata.get(\u0026#34;weight\u0026#34;))); } } ... } public class GrayReactiveLoadBalancerClientFilter implements GlobalFilter, Ordered { ... @Override public Mono\u0026lt;Void\u0026gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { URI url = (URI)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR); String schemePrefix = (String)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_SCHEME_PREFIX_ATTR); if (url != null \u0026amp;\u0026amp; (\u0026#34;grayLb\u0026#34;.equals(url.getScheme()) || \u0026#34;grayLb\u0026#34;.equals(schemePrefix))) { // ServerWebExchangeUtils.addOriginalRequestUrl(exchange, url); ... return this.choose(exchange).doOnNext((response) -\u0026gt; { // ... }).then(chain.filter(exchange)); } else { return chain.filter(exchange); } } private Mono\u0026lt;Response\u0026lt;ServiceInstance\u0026gt;\u0026gt; choose(ServerWebExchange exchange) { // URI uri = (URI)exchange.getAttribute(ServerWebExchangeUtils.GATEWAY_REQUEST_URL_ATTR); GrayLoadBalancer loadBalancer = new GrayLoadBalancer(clientFactory.getLazyProvider(uri.getHost(), ServiceInstanceListSupplier.class), uri.getHost()); if (loadBalancer == null) { throw new NotFoundException(\u0026#34;No loadbalancer available for \u0026#34; + uri.getHost()); } else { return loadBalancer.choose(this.createRequest(exchange)); // } } } 参考\r#\r基于springcloud gateway + nacos实现灰度发布（reactive版） 相关的代码 Go to Page 灰度发布 self "},{"id":71,"href":"/www6vMiddleware/docs/serviceGovernance/API-Gateway/apiGatawaySpringGateway/","title":"API 网关-SpringCloud Gateway","section":"API Gateway","content":"\nFeatures [0]\r#\rBuilt on Spring Framework 5, Project Reactor and Spring Boot 2.0 Able to match routes on any request attribute. Predicates and filters are specific to routes. Circuit Breaker integration. Spring Cloud DiscoveryClient integration Easy to write Predicates and Filters Request Rate Limiting Path Rewriting 核心概念 [1][2]\r#\r路由（Route）\nid：路由标识，要求唯一，名称任意（默认值 uuid，一般不用，需要自定义） uri：请求最终被转发到的目标地址 order： 路由优先级，数字越小，优先级越高 predicates：断言数组，即判断条件，如果返回值是boolean，则转发请求到 uri 属性指定的服务中 filters：过滤器数组，在请求传递过程中，对请求做一些修改 谓词、断言（Predicate） 允许开发人员匹配 HTTP 请求中的内容，比如请求头或请求参数，最后根据匹配结果返回一个布尔值。参照 Java8 的新特性Predicate.\n过滤器（Filter） 可以在返回请求之前或之后修改请求和响应的内容。\n路由（Route）[1][2]\r#\r服务发现-集成nacos服务注册中心 [2]\r#\r服务路由配置 spring: cloud: gateway: routes: - id: gateway-provider_1 ## 使用了lb形式，从注册中心负载均衡的获取uri uri: lb://gateway-provider ## 配置断言 predicates: - Path=/gateway/provider/** filters: - AddResponseHeader=X-Response-Foo, Bar 自动路由配置 # enabled：默认为false，设置为true表明spring cloud gateway开启服务发现和路由的功能，网关自动根据注册中心的服务名为每个服务创建一个router，将以服务名开头的请求路径转发到对应的服务 spring.cloud.gateway.discovery.locator.enabled = true # lowerCaseServiceId：启动 locator.enabled=true 自动路由时，路由的路径默认会使用大写ID，若想要使用小写ID，可将lowerCaseServiceId设置为true spring.cloud.gateway.discovery.locator.lower-case-service-id = true 动态路由-整合 Apollo [2]\r#\r/** * Apollo路由更改监听刷新 */ @Configuration public class GatewayPropertRefresher implements ApplicationContextAware, ApplicationEventPublisherAware { ... /** * 监听路由修改 */ @ApolloConfigChangeListener(interestedKeyPrefixes = \u0026#34;spring.cloud.gateway.\u0026#34;) public void onChange(ConfigChangeEvent changeEvent) { refreshGatewayProperties(changeEvent); } /** * 刷新路由信息 */ private void refreshGatewayProperties(ConfigChangeEvent changeEvent) { logger.info(\u0026#34;gateway网关配置 刷新开始！\u0026#34;); preDestroyGatewayProperties(changeEvent); //更新配置 this.applicationContext.publishEvent(new EnvironmentChangeEvent(changeEvent.changedKeys())); //更新路由 refreshGatewayRouteDefinition(); logger.info(\u0026#34;gateway网关配置 刷新完成！\u0026#34;); } ... } 动态路由-整合nacos [3]\r#\r@Component @Slf4j public class NacosDynamicRouteService implements ApplicationEventPublisherAware { private String dataId = \u0026#34;gateway-router\u0026#34;; private String group = \u0026#34;DEFAULT_GROUP\u0026#34;; @Value(\u0026#34;${spring.cloud.nacos.config.server-addr}\u0026#34;) private String serverAddr; @Autowired private RouteDefinitionWriter routeDefinitionWriter; private ApplicationEventPublisher applicationEventPublisher; private static final List\u0026lt;String\u0026gt; ROUTE_LIST = new ArrayList\u0026lt;\u0026gt;(); @PostConstruct public void dynamicRouteByNacosListener() { try { ConfigService configService = NacosFactory.createConfigService(serverAddr); configService.getConfig(dataId, group, 5000); configService.addListener(dataId, group, new Listener() { @Override public void receiveConfigInfo(String configInfo) { clearRoute(); try { if (StringUtil.isNullOrEmpty(configInfo)) {//配置被删除 return; } List\u0026lt;RouteDefinition\u0026gt; gatewayRouteDefinitions = JSONObject.parseArray(configInfo, RouteDefinition.class); for (RouteDefinition routeDefinition : gatewayRouteDefinitions) { addRoute(routeDefinition); } publish(); } catch (Exception e) { log.error(\u0026#34;receiveConfigInfo error\u0026#34; + e); } } @Override public Executor getExecutor() { return null; } }); } catch (NacosException e) { log.error(\u0026#34;dynamicRouteByNacosListener error\u0026#34; + e); } } private void clearRoute() { for (String id : ROUTE_LIST) { this.routeDefinitionWriter.delete(Mono.just(id)).subscribe(); } ROUTE_LIST.clear(); } private void addRoute(RouteDefinition definition) { try { routeDefinitionWriter.save(Mono.just(definition)).subscribe(); ROUTE_LIST.add(definition.getId()); } catch (Exception e) { log.error(\u0026#34;addRoute error\u0026#34; + e); } } 谓词、断言（Predicate）[1][2]\r#\r过滤器（Filter）[1][2]\r#\r生命周期\nPRE POST 作用范围\nGatewayFilter 局部过滤器 默认预定义 限流 GlobalFilter 全局过滤器 自定义全局过滤器 统一鉴权过滤器 稳定性\r#\r熔断降级-Hystrix [3]\r#\rserver.port: 8082 spring: application: name: gateway redis: host: localhost port: 6379 password: 123456 cloud: gateway: routes: ## - id: rateLimit_route uri: http://localhost:8000 order: 0 predicates: ## - Path=/test/** filters: ## - StripPrefix=1 - name: Hystrix args: name: fallbackCmdA fallbackUri: forward:/fallbackA hystrix.command.fallbackCmdA.execution.isolation.thread.timeoutInMilliseconds: 5000 流控和降级-Sentinel [3]\r#\r高可用网关[1]\r#\rNginx负载均衡到部署的多个Gateway\n参考\r#\rspring-cloud-gateway\n2021最新(完整版)Gateway教学-第二代微服务网关组件SpringCloud-Gateway *** V\nSpring Cloud Gateway 服务网关的部署与使用详细介绍\nSpringCloud gateway （史上最全） 尼恩\n3W字吃透：微服务网关SpringCloud gateway底层原理和实操 尼恩 未\n"},{"id":72,"href":"/www6vMiddleware/docs/serviceGovernance/API-Gateway/apiGatawayApisix/","title":"API 网关-apisix","section":"API Gateway","content":"\napisix特性\r#\rCore api聚合 灰度发布 稳定性 服务熔断 故障注入 流量复制 云原生 多云，混合云 容器友好 随意扩缩容 apisix功能\r#\r动态配置，不用reload\n路由, ssl证书，上游，插件\u0026hellip; 插件化(40个)\n身份验证, 安全, 日志, 可观察性\u0026hellip; 对接Prom，zipkin， skywalking grpc代理和协议转换(rest \u0026lt;-\u0026gt; gprc) apisix只用了nginx的网络层 apisix使用场景\r#\r处理L4, L7层流量 代替nginx处理南北流量 代替envoy处理东西流量 k8s ingress controller 参考\r#\r【云原生学院 #3】基于 Apache APISIX 的全流量 API 网关 *** "},{"id":73,"href":"/www6vMiddleware/docs/serviceGovernance/API-Gateway/apiGateway/","title":"API Gateway网关","section":"API Gateway","content":"\n特性\r#\r路由 灰度发布 反向代理,负载均衡 鉴权 限流 监控 缓存 分类\r#\r入口网关 出口网关 框架\r#\r产品 技术 apisix self lua + Nginx Kong lua + Nginx Zuul Spring Cloud Netflix Gateway self Spring Cloud Traefik Golang 实现 [3]\r#\r扩展性\n责任链模式 - Zuul filter, Envoy filter 性能\n多路 I/O 复用模型 和 线程池 可用性\n线程池 服务隔离 API Gateway+BFF\r#\rAPI Gateway + BFF [3]\r#\r流量网关 + 业务网关\nBFF 聚合网关 [2]\r#\r参考\r#\r使用 API 网关构建微服务 微服务架构：BFF和网关是如何演化出来的？ 《27 | API网关：系统的门面要如何做呢？》 百亿规模API网关服务Shepherd的设计与实现 点评 未 Go to Page self "},{"id":74,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/redisAOF/","title":"Redis AOF","section":"持久化","content":"\nAOF\r#\r定义[1]\r#\r[AOF不是WAL，是先操作，后记录日志。]\n传统数据库的日志，例如 redo log(重做日志)，记录的是修改后的数据，而AOF里记录的是Redis收到的每一条命令，这些命令是以文本形式保存的。\nAOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志。\n而写后日志这种方式，就是先让系统执行命令，只有命令能执行成功，才会被记录到日志中，否则，系统就会直接向客户端报错。 所以，Redis 使用写后日志这一方式的一大好处是，可以避免出现记录错误命令的情况。\nAOF 还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。\nAOF 是 写前日志\n两个潜在的风险 [1]\r#\r如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数 据就有丢失的风险. AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就 会导致写盘很慢，进而导致后续的操作也无法执行了. 三种写回策略 [1]\r#\rAOF 配置项 - appendfsync 的三个可选值 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘； Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘； No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 关闭AOF落盘 [2]\r#\rAOF落盘会带来一定写性能损耗，如果将Redis实例应用于纯缓存场景中，对数据持久化没有需求，您可以按照本章节的说明，修改appendonly参数的值，关闭AOF落盘。\nAOF Rewrite过程\r#\r功能 压缩AOF文件的大小\nAOF Rewrite过程 [1] 非阻塞的重写\n一个拷贝，两处日志 [1] 触发机制 [4]\n手动触发 bgrewriteaof 命令 自动触发 AOF文件大小 参考\r#\r《04 | AOF日志:宕机了，Redis如何避免数据丢失? 》 蒋德钧 关闭AOF落盘 {% post_link \u0026lsquo;aofRewrite\u0026rsquo; %} 《Redis开发与运维》 第5章 "},{"id":75,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E4%BA%8B%E5%8A%A1/redisTransaction/","title":"Redis 事务","section":"事务","content":"\n事务的 ACID 属性是我们使用事务进行正确操作的基本要求。\nRedis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。\n不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，我们更加关注的还是原子性、 一致性和隔离性这三个属性。\n原子性的情况比较复杂，只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行。\n事务 是否能保证 一致性 C 能保证 隔离性 I 能保证 原子性 A 能保证(命令语法无误时) 持久性 D 不能保证 【redis cluster 没有事务】\n参考\r#\r31 | 事务机制:Redis能实现ACID属性吗?\n[redis]redis集群不支持事务multi、exec 失效\n"},{"id":76,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/rdb/","title":"Redis RDB源码","section":"持久化","content":"\n/// 对应了 Redis 的 save 命 令，会在 save 命令的实现函数 saveCommand(在 rdb.c 文件中)中被调用 /* Save the DB on disk. Return C_ERR on error, C_OK on success. */ int rdbSave(char *filename, rdbSaveInfo *rsi) { char tmpfile[256]; char cwd[MAXPATHLEN]; /* Current working dir path for error messages. */ FILE *fp; rio rdb; int error = 0; snprintf(tmpfile,256,\u0026#34;temp-%d.rdb\u0026#34;, (int) getpid()); fp = fopen(tmpfile,\u0026#34;w\u0026#34;); if (!fp) { char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \u0026#34;Failed opening the RDB file %s (in server root dir %s) \u0026#34; \u0026#34;for saving: %s\u0026#34;, filename, cwdp ? cwdp : \u0026#34;unknown\u0026#34;, strerror(errno)); return C_ERR; } rioInitWithFile(\u0026amp;rdb,fp); if (server.rdb_save_incremental_fsync) rioSetAutoSync(\u0026amp;rdb,REDIS_AUTOSYNC_BYTES); if (rdbSaveRio(\u0026amp;rdb,\u0026amp;error,RDB_SAVE_NONE,rsi) == C_ERR) {. /// 调用 rdbSaveRio 函数(在 rdb.c 文件中)来实际创建 RDB 文件 errno = error; goto werr; } /* Make sure data will not remain on the OS\u0026#39;s output buffers */ if (fflush(fp) == EOF) goto werr; if (fsync(fileno(fp)) == -1) goto werr; if (fclose(fp) == EOF) goto werr; /* Use RENAME to make sure the DB file is changed atomically only * if the generate DB file is ok. */ if (rename(tmpfile,filename) == -1) { char *cwdp = getcwd(cwd,MAXPATHLEN); serverLog(LL_WARNING, \u0026#34;Error moving temp DB file %s on the final \u0026#34; \u0026#34;destination %s (in server root dir %s): %s\u0026#34;, tmpfile, filename, cwdp ? cwdp : \u0026#34;unknown\u0026#34;, strerror(errno)); unlink(tmpfile); return C_ERR; } serverLog(LL_NOTICE,\u0026#34;DB saved on disk\u0026#34;); server.dirty = 0; server.lastsave = time(NULL); server.lastbgsave_status = C_OK; return C_OK; werr: serverLog(LL_WARNING,\u0026#34;Write error saving DB on disk: %s\u0026#34;, strerror(errno)); fclose(fp); unlink(tmpfile); return C_ERR; } /// 对应 了 Redis 的 bgsave 命令，会在 bgsave 命令的实现函数 bgsaveCommand(在 rdb.c 文 件中)中被调用 int rdbSaveBackground(char *filename, rdbSaveInfo *rsi) { pid_t childpid; long long start; if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) return C_ERR; server.dirty_before_bgsave = server.dirty; server.lastbgsave_try = time(NULL); openChildInfoPipe(); start = ustime(); if ((childpid = fork()) == 0) { /// 子进程的代码执行分支 int retval; /* Child */ closeClildUnusedResourceAfterFork(); redisSetProcTitle(\u0026#34;redis-rdb-bgsave\u0026#34;); retval = rdbSave(filename,rsi); /// 调用rdbSave函数创建RDB文件 if (retval == C_OK) { size_t private_dirty = zmalloc_get_private_dirty(-1); if (private_dirty) { serverLog(LL_NOTICE, \u0026#34;RDB: %zu MB of memory used by copy-on-write\u0026#34;, private_dirty/(1024*1024)); } server.child_info_data.cow_size = private_dirty; sendChildInfo(CHILD_INFO_TYPE_RDB); } exitFromChild((retval == C_OK) ? 0 : 1); /// 子进程退出 } else { /* Parent */ //父进程代码执行分支 server.stat_fork_time = ustime()-start; server.stat_fork_rate = (double) zmalloc_used_memory() * 1000000 / server.stat_fork_time / (1024*1024*1024); /* GB per second. */ latencyAddSampleIfNeeded(\u0026#34;fork\u0026#34;,server.stat_fork_time/1000); if (childpid == -1) { closeChildInfoPipe(); server.lastbgsave_status = C_ERR; serverLog(LL_WARNING,\u0026#34;Can\u0026#39;t save in background: fork: %s\u0026#34;, strerror(errno)); return C_ERR; } serverLog(LL_NOTICE,\u0026#34;Background saving started by pid %d\u0026#34;,childpid); server.rdb_save_time_start = time(NULL); server.rdb_child_pid = childpid; server.rdb_child_type = RDB_CHILD_TYPE_DISK; updateDictResizePolicy(); return C_OK; } return C_OK; /* unreached */ } /// 这是 Redis server 在采用不落盘方式传输 RDB 文件进行主从复制时，创建 RDB 文件的入 口函数 /// rdbSaveToSlavesSockets 函数是通过网络以字节流的形式，直接发送 RDB 文件的二进制 数据给从节点。 /* Spawn an RDB child that writes the RDB to the sockets of the slaves * that are currently in SLAVE_STATE_WAIT_BGSAVE_START state. */ int rdbSaveToSlavesSockets(rdbSaveInfo *rsi) { int *fds; uint64_t *clientids; int numfds; listNode *ln; listIter li; pid_t childpid; long long start; int pipefds[2]; if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) return C_ERR; /* Before to fork, create a pipe that will be used in order to * send back to the parent the IDs of the slaves that successfully * received all the writes. */ if (pipe(pipefds) == -1) return C_ERR; server.rdb_pipe_read_result_from_child = pipefds[0]; server.rdb_pipe_write_result_to_parent = pipefds[1]; /* Collect the file descriptors of the slaves we want to transfer * the RDB to, which are i WAIT_BGSAVE_START state. */ fds = zmalloc(sizeof(int)*listLength(server.slaves)); /* We also allocate an array of corresponding client IDs. This will * be useful for the child process in order to build the report * (sent via unix pipe) that will be sent to the parent. */ clientids = zmalloc(sizeof(uint64_t)*listLength(server.slaves)); numfds = 0; listRewind(server.slaves,\u0026amp;li); while((ln = listNext(\u0026amp;li))) { client *slave = ln-\u0026gt;value; if (slave-\u0026gt;replstate == SLAVE_STATE_WAIT_BGSAVE_START) { clientids[numfds] = slave-\u0026gt;id; fds[numfds++] = slave-\u0026gt;fd; replicationSetupSlaveForFullResync(slave,getPsyncInitialOffset()); /* Put the socket in blocking mode to simplify RDB transfer. * We\u0026#39;ll restore it when the children returns (since duped socket * will share the O_NONBLOCK attribute with the parent). */ anetBlock(NULL,slave-\u0026gt;fd); anetSendTimeout(NULL,slave-\u0026gt;fd,server.repl_timeout*1000); } } /* Create the child process. */ openChildInfoPipe(); start = ustime(); if ((childpid = fork()) == 0) { /* Child */ int retval; rio slave_sockets; rioInitWithFdset(\u0026amp;slave_sockets,fds,numfds); zfree(fds); closeClildUnusedResourceAfterFork(); redisSetProcTitle(\u0026#34;redis-rdb-to-slaves\u0026#34;); retval = rdbSaveRioWithEOFMark(\u0026amp;slave_sockets,NULL,rsi); if (retval == C_OK \u0026amp;\u0026amp; rioFlush(\u0026amp;slave_sockets) == 0) retval = C_ERR; if (retval == C_OK) { size_t private_dirty = zmalloc_get_private_dirty(-1); if (private_dirty) { serverLog(LL_NOTICE, \u0026#34;RDB: %zu MB of memory used by copy-on-write\u0026#34;, private_dirty/(1024*1024)); } server.child_info_data.cow_size = private_dirty; sendChildInfo(CHILD_INFO_TYPE_RDB); /* If we are returning OK, at least one slave was served * with the RDB file as expected, so we need to send a report * to the parent via the pipe. The format of the message is: * * \u0026lt;len\u0026gt; \u0026lt;slave[0].id\u0026gt; \u0026lt;slave[0].error\u0026gt; ... * * len, slave IDs, and slave errors, are all uint64_t integers, * so basically the reply is composed of 64 bits for the len field * plus 2 additional 64 bit integers for each entry, for a total * of \u0026#39;len\u0026#39; entries. * * The \u0026#39;id\u0026#39; represents the slave\u0026#39;s client ID, so that the master * can match the report with a specific slave, and \u0026#39;error\u0026#39; is * set to 0 if the replication process terminated with a success * or the error code if an error occurred. */ void *msg = zmalloc(sizeof(uint64_t)*(1+2*numfds)); uint64_t *len = msg; uint64_t *ids = len+1; int j, msglen; *len = numfds; for (j = 0; j \u0026lt; numfds; j++) { *ids++ = clientids[j]; *ids++ = slave_sockets.io.fdset.state[j]; } /* Write the message to the parent. If we have no good slaves or * we are unable to transfer the message to the parent, we exit * with an error so that the parent will abort the replication * process with all the childre that were waiting. */ msglen = sizeof(uint64_t)*(1+2*numfds); if (*len == 0 || write(server.rdb_pipe_write_result_to_parent,msg,msglen) != msglen) { retval = C_ERR; } zfree(msg); } zfree(clientids); rioFreeFdset(\u0026amp;slave_sockets); exitFromChild((retval == C_OK) ? 0 : 1); } else { /* Parent */ if (childpid == -1) { serverLog(LL_WARNING,\u0026#34;Can\u0026#39;t save in background: fork: %s\u0026#34;, strerror(errno)); /* Undo the state change. The caller will perform cleanup on * all the slaves in BGSAVE_START state, but an early call to * replicationSetupSlaveForFullResync() turned it into BGSAVE_END */ listRewind(server.slaves,\u0026amp;li); while((ln = listNext(\u0026amp;li))) { client *slave = ln-\u0026gt;value; int j; for (j = 0; j \u0026lt; numfds; j++) { if (slave-\u0026gt;id == clientids[j]) { slave-\u0026gt;replstate = SLAVE_STATE_WAIT_BGSAVE_START; break; } } } close(pipefds[0]); close(pipefds[1]); closeChildInfoPipe(); } else { server.stat_fork_time = ustime()-start; server.stat_fork_rate = (double) zmalloc_used_memory() * 1000000 / server.stat_fork_time / (1024*1024*1024); /* GB per second. */ latencyAddSampleIfNeeded(\u0026#34;fork\u0026#34;,server.stat_fork_time/1000); serverLog(LL_NOTICE,\u0026#34;Background RDB transfer started by pid %d\u0026#34;, childpid); server.rdb_save_time_start = time(NULL); server.rdb_child_pid = childpid; server.rdb_child_type = RDB_CHILD_TYPE_SOCKET; updateDictResizePolicy(); } zfree(clientids); zfree(fds); return (childpid == -1) ? C_ERR : C_OK; } return C_OK; /* Unreached. */ } 参考：\r#\r18 | 如何生成和解读RDB文件?\n《Redis源码剖析与实战》\n"},{"id":77,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/aofRewrite/","title":"Redis AOF Rewrite","section":"持久化","content":"\n一. AOF 重写函数与触发时机\r#\r我们就了解了 AOF 重写的四个触发时机，这里我也给你总结下，方便你回 顾复习。\n时机一:bgrewriteaof 命令被执行。\r#\r时机二:主从复制完成 RDB 文件解析和加载(无论是否成功)。\r#\r而对于 restartAOFAfterSYNC 函数来说，它会在主从节点的复制过程中被调用。简单来 说，就是当主从节点在进行复制时，如果从节点的 AOF 选项被打开，那么在加载解析 RDB 文件时，AOF 选项就会被关闭。然后，无论从节点是否成功加载了 RDB 文件， restartAOFAfterSYNC 函数都会被调用，用来恢复被关闭的 AOF 功能。\r那么在这个过程中，restartAOFAfterSYNC 函数就会调用 startAppendOnly 函数，并进 一步调用 rewriteAppendOnlyFileBackground 函数，来执行一次 AOF 重写。\r时机三:AOF 重写被设置为待调度执行。\r#\r时机四:AOF 被启用，同时 AOF 文件的大小比例超出阈值，以及 AOF 文件的大小绝对 值超出阈值。\r#\rauto-aof-rewrite-percentage:AOF 文件大小超出基础大小的比例，默认值为 100%，即超出 1 倍大小。 auto-aof-rewrite-min-size:AOF 文件大小绝对值的最小值，默认为 64MB。\n另外，这里你还需要注意，在这四个时机下，其实都不能有正在执行的 RDB 子进程和 AOF 重写子进程，否则的话，AOF 重写就无法执行了。\n二. AOF 重写的基本过程\r#\rint rewriteAppendOnlyFileBackground(void) { ... if ((childpid = fork()) == 0) { //创建子进程 ... //子进程调用rewriteAppendOnlyFile进行AOF重写 if (rewriteAppendOnlyFile(tmpfile) == C_OK) { size_t private_dirty = zmalloc_get_private_dirty(-1); ... exitFromChild(0); } else { exitFromChild(1); } } else{ //父进程执行的逻辑 ... server.aof_rewrite_scheduled = 0; server.aof_rewrite_time_start = time(NULL); server.aof_child_pid = childpid; //记录重写子进程的进程号 updateDictResizePolicy(); //关闭rehash功能 ... } } 不过，AOF 重写和 RDB 文件又有两个不同的地方: 一是，AOF 文件中是以“命令 + 键值对”的形式，来记录每个键值对的插入操作，而 RDB 文件记录的是键值对数据本身; 二是，在 AOF 重写或是创建 RDB 的过程中，主进程仍然可以接收客户端写请求。不 过，因为 RDB 文件只需要记录某个时刻下数据库的所有数据就行，而 AOF 重写则需要 尽可能地把主进程收到的写操作，也记录到重写的日志文件中。所以，AOF 重写子进程 就需要有相应的机制来和主进程进行通信，以此来接收主进程收到的写操作。\n"},{"id":78,"href":"/www6vMiddleware/docs/serviceGovernance/Springcloud/springCloud/","title":"SpringCloud","section":"Spring Cloud","content":"\n参考\r#\rSpringCloud 面试题 （持续更新、吐血推荐）\n"},{"id":79,"href":"/www6vMiddleware/docs/Redis+%E7%BC%93%E5%AD%98/Redis-%E5%9F%BA%E7%A1%80/%E6%8C%81%E4%B9%85%E5%8C%96/redisRDB/","title":"Redis RDB","section":"持久化","content":"\nRDB\r#\r功能 [1]\r#\r只需要把 RDB 文件直接读入内存，可以快速恢复数据库。这就避免了 AOF 需要顺序、逐一重新执行操作命令带来的低效性能问题。\nRedis 设计了 bgsave 和写时复制方式，尽可能减少了内存快照对正常读写的影响，但是，频繁快照仍然是不太能接受的。\n【 总结: bgsave 异步 + 写时复制（copy-on-write）】\n触发机制 [2]\r#\r手动触发 bgsave [3] save 废弃 [3] 自动触发 save的相关配置 从节点全量复制，主节点执行bgsave生成RDB并发送给从节点 其他 优点/缺点[2]\r#\r优点 加载RDB恢复数据远远快于AOF的方式 缺点 没办法做到实时持久化 建议 [1]\r#\r数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。 参考\r#\r《05丨内存快照：宕机后，Redis如何实现快速恢复？》 《redis开发与运维》 第5章 {% post_link \u0026lsquo;rdb\u0026rsquo; %} self "},{"id":80,"href":"/www6vMiddleware/docs/serviceGovernance/ConfigDiscovery/config/","title":"服务治理-分布式配置","section":"Config \u0026 Discovery","content":"\n需求\r#\r对实时性要求不高 对可用性要求高 产品\r#\r产品 存储 Disconf 百度 mysql Apollo 携程 mysql QConf 360 zookeeper 微博 redis 美图 etcd spring cloud config git 参考：\r#\rSpring Boot与Kubernetes云原生微服务实践 杨波 "},{"id":81,"href":"/www6vMiddleware/docs/serviceGovernance/loadBalance/nginxOptimize/","title":"Nginx优化","section":"负载均衡","content":"\nNginx 参数优化\r#\r反向代理\r#\rproxy_cache: 10M(重要) proxy_cache_path /data/nginx_cache/ levels=1:2 keys_zone=my_zone:10m inactive=300s max_size=5g; tls ssl\r#\rssl_session_cache builtin:1000 shared:SSL:10m; /// 一天内连接上的用户， 不需要再协商秘钥 ssl_session_cache builtin:1000 shared:SSL:10m; /// 1m -\u0026gt; 4000个https连接 ssl_session_timeout 10m; /// 10分钟 ssl_protocols TLSv1.2; /// 版本号 非安全请求重定向\r#\rNo redirect: 无重定向 Redirect： 301-302 -》 转到https站点 gzip\r#\rgzip on; gzip_comp_level 5; gzip_http_version 1.1; /// 注意：gzip 在1.1上有效， http2.0上是无效的 gzip_min_length 256; gzip_types application/atom+xml ... gzip_proxied any; gzip_vary on; worker\r#\rworker_connections 16384; /// 一个worker有 16384/2=8192 ‬个链接 . 两个事件， 一个读事件， 一个写事件。 越多的connections对应更多的内存消耗。 Default: worker_connections 512; 高级选项： worker connections的内存池（pools）， 更少的的内存碎片。一般是nginx自动分配的， 不用分配。 Default: connection_pool_size 256|512 Default: request_pool_size 4k; worker_processes 设置worker进程的数量 减少进程间切换\r#\r程间切换： cpu从一个进程或线程切换到另一个进程或线程。\n主动切换和被动切换 减少主动切换 被动切换：时间片耗尽。 减少被动切换： 增大进程优先级 Nice 静态优先级: -20 \u0026ndash; 19 Priority 动态优先级： 0-139 提升CPU缓存命中率\r#\r绑定worker到指定cpu L1,L2(cpu独享) L3（共享的） worker_cpu_affinity cpumask ...; worker_cpu_affinity auto [cpumask]; lua 分配的内存（暂时没有使用）\r#\rlua_shared_dict configuration_data 5M; lua_shared_dict certificate_data 16M; // 应用场景: 集群流控, 多个worker之间的内存的共享。 http的keeplive 长链接（一个tcp的链接，上面有多个http的请求）\r#\r注意: 非tcp keeplive\nkeepalive_disable; /// 没有设置 keepalive_timeout 75s; // 默认值 keepalive_requests 100; // 默认值 一个tcp请求中可发100个http请求 测试用例\r#\rURL：logsearch.sh.pre.urtc.com.cn\ncase1\r#\rinput： 1000用户并发, 短连接， 非keepalive的\nresult： 链接数 40000+\ntps 4000+\navg 百ms\ncase2\r#\rinput： 1500用户并发，短连接， 非keepalive的\nresult： 链接数 30000+，\ntps 4000+ ，\navg 200ms+\n参考\r#\rNginx全面配置 *** 未 "},{"id":82,"href":"/www6vMiddleware/docs/serviceGovernance/security/securityOAuth2/","title":"安全-OAuth2","section":"安全","content":"\n目录\r#\rOAuth 2.0 授权类型 [1][4]\r#\r授权码模式-用的多 Authorization Code 授权码 ***\n客户端模式 Client Credentials\nThe Client Credentials grant is used when applications request an access token to access their own resources, not on behalf of a user.\nxxx Refresh Token *** 动态token\n密码模式-Legacy Password Grant\n基于OAuth2 的微服务 参考架构 [3]\r#\rOverview\r#\r网关 令牌的校验和转换，将前端传递过来的 OAuth 2.0 访问令牌，通过调用 IDP 进 行校验，并转换为包含用户和权限信息的 JWT 令牌，再将 JWT 令牌向后台微服务传 递。 IDP 服务 IDP 是 Identity Provider 的简称，主要负责 OAuth 2.0 授权协议处理，OAuth 2.0 和 JWT 令牌颁发和管理，以及用户认证等功能。IDP 使用后台的 Login-Service 进行用户认 证。 选型: Spring Security OAuth or KeyCloak(RedHat) OAuth2 与微服务进行集成\r#\r第三方 Web 应用 + 授权码模式 各大开放平台是如何使用 OAuth 2.0 的 [2]\r#\r网关集成OAuth2.0 [5]\r#\rOIDC [2]\r#\r什么是 OIDC\r#\r什么是 OIDC？ EU：End User RP：Relying Party OP：OpenID Provider ID Token UserInfo Endpoint OIDC 是 OpenID Connect 的简称，OIDC=(Identity, Authentication) + OAuth 2.0。它在 OAuth2 上构建了一个身份层，是一个基于 OAuth2 协议的身份认证标准协议。OAuth2 是一个授权协议，它无法提供完善的身份认证功能，OIDC 使用 OAuth2 的授权服务器来为第三方客户端提供用户的身份认证，并把对应的身份认证信息传递给客户端.\nOAuth2 提供了Access Token来解决授权第三方客户端访问受保护资源的问题；OIDC 在这个基础上提供了ID Token 来解决第三方客户端标识用户身份认证的问题。\nOIDC 核心概念\r#\rOIDC 核心概念 主要术语 OIDC 工作流程 ID Token 认证 基于 Authorization Code 的认证请求 获取 ID Token Implicit Flow 和 Hybrid Flow UserInfo Endpoint OIDC 示例\r#\rOIDC 示例 + 请求示例： POST /auth/realms/ccm/protocol/openid-connect/token HTTP/1.1 Host: server.example.com Content-Type: application/x-www-form-urlencoded Authorization: Basic d2ViX2FwcDp3ZWJfYXBw grant_type=**authorization_code**\u0026amp;code=7138b4b3-8c2b-4016-ad98-01c4938750c6.c110ddc8-c6c1-4a95-bd9e-cd8d84b4dd70.1eabef67-6473-4ba8-b07c-14bdbae4aaed\u0026amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb + 响应示例： HTTP/1.1 200 OK Content-Type: application/json Cache-Control: no-store Pragma: no-cache { **\u0026#34;access_token\u0026#34;**: \u0026#34;SlAV32hkKG\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;refresh_token\u0026#34;: \u0026#34;8xLOxBtZp8\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, **\u0026#34;id_token\u0026#34;**: \u0026#34;eyJhbGciOiJSUzI1NiIsImtpZCI6IjFlOWdkazcifQ.ewogImlzc yI6ICJodHRwOi8vc2VydmVyLmV4YW1wbGUuY29tIiwKICJzdWIiOiAiMjQ4Mjg5 NzYxMDAxIiwKICJhdWQiOiAiczZCaGRSa3F0MyIsCiAibm9uY2UiOiAibi0wUzZ fV3pBMk1qIiwKICJleHAiOiAxMzExMjgxOTcwLAogImlhdCI6IDEzMTEyODA5Nz AKfQ.ggW8hZ1EuVLuxNuuIJKX_V8a_OMXzR0EHR9R6jgdqrOOF4daGU96Sr_P6q Jp6IcmD3HP99Obi1PRs-cwh3LO-p146waJ8IhehcwL7F09JdijmBqkvPeB2T9CJ NqeGpe-gccMg4vfKjkM8FcGvnzZUN4_KSP0aAp1tOJ1zZwgjxqGByKHiOtX7Tpd QyHE5lcMiKPXfEIQILVq0pc_E2DzL7emopWoaoZTF_m0_N0YzFC6g6EJbOEoRoS K5hoDalrcvRYLSrQAZZKflyuVCyixEoV9GfNQC3_osjzw2PAithfubEEBLuVVk4 XUVrWOLrLl0nx7RkKU8NXNHq-rvKMzqg\u0026#34; } 参考\r#\r10 分钟理解什么是 OAuth 2.0 协议 *** OAuth2.0 + OIDC 技术规范及应用场景 *** \u0026laquo;12 | 架构案例：基于OAuth 2.0/JWT的微服务参考架构\u0026raquo; 杨波 *** OAuth2.0的四种模式测试 07 网关集成OAuth2.0实现统一认证鉴权 代码 "},{"id":83,"href":"/www6vMiddleware/docs/serviceGovernance/Springcloud/springboot/","title":"SpringBoot","section":"Spring Cloud","content":"\nCore\r#\rSpring Boot定义\r#\rSpring Boot is designed to get you up and running as quickly as possible, with minimal upfront configuration of Spring. Spring Boot takes an opinionated view of building production-ready applications.\nFeatures (官方)\r#\rCreate stand-alone Spring applications Embed Tomcat, Jetty or Undertow directly (no need to deploy WAR files) Provide opinionated \u0026lsquo;starter\u0026rsquo; dependencies to simplify your build configuration Automatically configure Spring and 3rd party libraries whenever possible Provide production-ready features such as metrics, health checks, and externalized configuration Absolutely no code generation and no requirement for XML configuration 特性\r#\r自动配置 Auto Configuration 为Spring及第三方库提供自动配置; 简化了项目的构建配置;\n无需生成代码或进行xml配置; 约定优于配置(Convention Over Configuration) CoC Starter Dependency\nSpringboot CLI\n内嵌的服务器 方便地创建可独立运行的Spring应用程序; 直接内嵌的Tomcat， Jetty或者Undertow;\n生产级 提供生产级特性; Actuator（Runtime）\nAuto Configuration\r#\r底层装配技术 [3]\r#\rSpring 模式注解装配\nSpring @Enable 模块装配\n// 组件 @EnableXXX @Importer @ImportXXXSelector @Conditional @ConditionalOnClass @ConditionalOnBean ... // 开启自动配置 @EnableAutoConfiguration @SpringBootApplication Spring 条件装配装配 // 实现原理 - 有条件的加载机制 @ConditionalOnClass @ConditionalOnBean @ConditionalOnMissingBean @ConditionalOnProperty ... Spring 工厂加载机制 实现类： SpringFactoriesLoader 配置资源： META-INF/spring.factories 外部化配置加载顺序\r#\r...\r命令行参数（--server.port=9000）\r...\rSystem.getProperties()\r操作系统环境变量\r... jar包外的application-{profile}.properties 或 .yml\rjar包内的application-{profile}.properties 或 .yml\rjar包外的application.properties或 .yml\rjar包内的application.properties或 .yml Starter Dependency\r#\r直接面向功能 官方Starters spring-boot-starter-* \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.0.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;!-- 使用方 --\u0026gt; \u0026lt;!-- 统一管理依赖 --\u0026gt;\u0026lt;!-- spring cloud的依赖 --\u0026gt; \u0026lt;dependencyManagement\u0026gt;\u0026lt;/dependencyManagement\u0026gt; \u0026lt;!-- 定义方 --\u0026gt; Bill of Materials - bom BOM本质上是一个普通的POM文件 扩展自定义starter \u0026hellip; production-ready\r#\rActuator\r#\r目的： 监控并管理应用程序 访问方式： HTTP, JMX 依赖： spring-boot-starter-actuator Actuator Endpoint\r#\rhttp访问 /actuator/ 端口与路径 management.server.address= management.server.port= 内嵌的Web容器\r#\r可选容器列表\r#\rspring-boot-starter-tomcat spring-boot-starter-jetty spring-boot-starter-undertow spring-boot-starter-reactor-netty 端口\r#\rserver.port server.address 压缩\r#\rTomcat特性配置\r#\rserver.tomcat.max-connections=10000 server.tomcat.max-http-post-size server.tomcat.max-threads 参考\r#\r《玩转Spring全家桶》 67, 68, 71, 73, 75, 79 丁雪峰 V 《黑马程序员SpringBoot教程，6小时快速入门Java微服务架构Spring Boot》 V 《mksz252 - Spring Boot 2.0深度实践之核心技术篇》 第2章 走向自动装配 V *** SpringBoot面试题 (史上最全、持续更新、吐血推荐) 尼恩 未 spring + spring mvc + tomcat 面试题（史上最全） 尼恩 未 SpringBoot 基础知识 核心知识 【收藏版】 尼恩 未 "},{"id":84,"href":"/www6vMiddleware/docs/serviceGovernance/Overview/microservice/","title":"微服务 总结","section":"Overview","content":"\n目录\r#\r微服务 定义\r#\rIn short, the microservice architectural style [1] is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. \u0026ndash; [Martin Fowler]\nCore\r#\rAPI网关\r#\rserviceGovernanceSummary self\n服务容错\r#\rserviceGovernanceSummary self\n服务注册和发现\r#\rserviceGovernanceSummary self\n服务间调用\r#\rMicro Service Architecture Microservice 微服务的理论模型和现实路径\n服务契约\r#\rAPI，具体接口的 API 接入技术说明。 能力，服务能力的描述。 契约，提供这些能力所约定的一些限制条件说明。 版本，支持的最新和历史的版本说明。 调用协议\r#\r同步 HTTP REST（JAX-RS） RPC（Dubbo）\n异步消息 Kafka, RabbitMQ, Notify AMQP, MQTT, STOMP\n服务部署和发布\r#\r微服务部署：蓝绿部署、滚动部署、灰度发布、金丝雀发布\n部署模式 Single Service per Host Multiple Services per Host patterns Design\r#\r服务划分和组合\r#\r微服务不是指\u0026quot;微小\u0026quot;的服务, 而是如何\u0026quot;拆分\u0026quot;服务,然后\u0026quot;组合\u0026quot;服务.\nDDD 领域驱动设计, 上下文划分（context） 康威定律 服务分层\r#\r上层: 聚合服务（适配服务， 边界服务）\r#\r比如：pc和mobile服务对商品服务返回内容的裁剪。\r聚合商品服务和目录服务的内容。 下层: 基础服务（核心领域服务， 公共服务）\r#\r比如：电商的商品服务， 目录服务， 订单服务\rDesign-微服务设计模式\r#\rOverview\r#\rSidecar [11]\r#\r分离业务逻辑与路由，流控，熔断，幂等，服务发现，鉴权等控制组件。\n适用场景： 老系统改造扩展，Sidebar 进程与服务进程部署在同一个节点； 多语言混合分布式系统扩展；\nEg. k8s pod中日志采集sidecar\nThe Scale Cube 可伸缩性\r#\rThe Scale Cube\nX-Axis: Horizontal Duplication and Cloning of services and data Y-Axis: Functional Decomposition and Segmentation - Microservices (or micro-services) Z-Axis: Service and Data Partitioning along Customer Boundaries - Shards/Pods\nX-Axis: Replicate \u0026amp;\u0026amp; Load Balance Y-Axis: Servcie Z-Axis: Data Sharding\n微服务的优势和代价\r#\rMicroservicePremium Martin Fowler.\n生产率和复杂度之间的关系。\n在不复杂的系统中， 更适合monolithic的应用。 复杂度增长时， 微服务的生产率能持续保持，在生产率方面是可伸缩的。\n原则和缺点（挑战）\r#\r微服务架构——不是免费的午餐 有关微服务架构的争论：更简单还是更复杂？\n原则 优点 缺点 挑战 分布式服务组成的系统； 去中心化 可用性高 多服务运维难度 分布式系统的复杂性（容错，延迟，分布式事务） 按照业务而不是技术来划分组织 服务独立无依赖 系统部署依赖 事务、异步、测试面临挑战 做有生命的产品而不是项目 技术栈灵活 运营开销 Smart endpoints and dumb pipes（强服务个体和轻量级通信）; 可组合的服务 独立按需扩展和伸缩 服务间通信成本 隐式接口[接口变更成本] 自动化运维（DevOps） 系统集成测试 DevOps 要求 容错 可用性高 数据一致性 性能监控; 分布式系统的复杂性 快速演化 开发简单 重复工作 系统集成测试 SOA、微服务、云原生演进\r#\r关注点 SOA 微服务 云原生 研发过程 CMM/RUP Agile Agile 交付流程 手工/自动化 DevOps\nDevSecOps GitOps[12]\nAIOps\nNoOps(Serverless) 服务通信 Web Service（WSDL，Soap） REST/私有RPC协议（Dubbo） REST/gRPC,Envoy xDS， MSI协议等开放协议 功能扩展性-filter x AOP filter\nDubbo filter chain\nWEB filter/lisnter Envoy filter 功能扩展性-微内核 x Dubbo SPI K8s CRD, Operator 服务治理 ESB 微服务/API网关（SpringCloud），去中心化, sidecar 服务网格（ istio ， Linked） 分布式 应用运行环境 物理机/虚拟机 虚拟机/容器 Kubernete（操作系统）+ Serverless（Knative） 基础设施 IDC 公有云/私有云 无边界的云（多云/混合云、 云+边+端） 总结 重 轻, 快速 开放、融合 参考\r#\rIntroduction to Microservices 英文\nIntroduction to Microservices 中文 优缺点\n微服务（Microservice）那点事 ***\nPattern: Microservice Architecture ***\n一致性 self\n微服务：分解应用以实现可部署性和可扩展性 Chris Richardson\n《Linux/Unix设计思想》随笔 ——Linux/Unix哲学概述 未\n微服务学习资料汇总 ***\n微服务架构技术栈选型手册 未\n从 SOA 到微服务，企业分布式应用架构在云原生时代如何重塑？ 阿里 易立 ***\n云原生时代，分布式系统设计必备知识图谱（内含22个知识点） 杨泽强（竹涧） ***\n使用托管服务网格实现应用在多集群中的 GitOps 全自动化渐进式发布 郝树伟 阿里云容器服务\n​\n"},{"id":85,"href":"/www6vMiddleware/docs/serviceGovernance/loadBalance/nginx/","title":"Nginx总结","section":"负载均衡","content":"\nNginx总结\r#\rNginx架构\r#\r共享内存 Slab 分页 4K， 8K， 16K Nginx反向代理\r#\r类型\r#\r带权重的round-robin算法是基础 hash负载均衡算法 ip-hash算法 -\u0026gt; real-ip hash算法 -\u0026gt; 自定义可以hash的参数（比如?userName） 问题: 如果有upstream的机器宕机， hash算法还会路由到这台机器 解决方案：使用一致性hash(consistent),hash 环\nleast-connection算法， 如果所有节点的connection都一致， 会退化成为round-robin算法。 可扩展立方体\r#\rX-axis 基于round-robin或者least-connected算法分发请求 -\u0026gt; 相对简单 Y-axis 基于URL对功能进行分发。 -\u0026gt; 相对复杂 Z-axis 将用户IP地址或者其他信息映射到某个特定的服务或者集群 -\u0026gt; 相对简单 多种协议反向代理\r#\rtcp udp 透传 http -\u0026gt; memcached , scgi, fastcgi, uwsgi, grpc, http, websocket 反向代理流程\r#\r修改发送到upstream机器的请求的nginx指令。\n节点热更新\r#\rmaster节点热更新\r#\rworker节点热更新\r#\r域名转发到其他域名[2]\r#\rreturn 指令 rewrite proxy_pass 文件下载\r#\rnginx.conf\nlocation /userlab.dat {\rcharset gbk;\r# alias /home/hp/home/frontend/indicator/userlab.dat;\rroot /home/cms/indicator;\rif ($request_filename ~* ^.*?\\.(txt)$){\radd_header Content-Disposition \u0026#39;attachment\u0026#39;;\radd_header Content-Type: \u0026#39;APPLICATION/OCTET-STREAM\u0026#39;;}\rautoindex on;\rautoindex_exact_size off;\rautoindex_localtime on;\r} 参考\r#\r深入Nginx 思维导图\nnginx配置域名转发到其他域名的几种方法\n"},{"id":86,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/faultTolerant/soaTolerateFramework/","title":"容错框架","section":"容错","content":"\nHystrix实现和容错模式\r#\rHystrix实现和容错模式 熔断【熔断器模式】 三个状态 开 闭 半开 模块 熔断请求判断机制算法 维护10个bucket,每秒一个bucket,每个blucket记录成功,失败,超时,拒绝的状态。 超时【超时与重试模式】 失败（异常） 成功 拒绝 线程池拒绝【1】 信号量拒绝【2】 默认错误超过50%且10秒内超过20个请求进行中断拦截 熔断恢复 每隔5s允许部分请求通过，若请求都是健康的（RT\u0026lt;250ms）则对请求健康恢复 熔断报警和Metric上报 流控【限流模式】 控制速率 控制并发 隔离【舱壁隔离模式】 Hystrix实现 线程池隔离 【1】 信号量隔离【2】 回退【回退模式】 快速失败（Fail Fast ） 无声失败（Fail Silent ） 返回默认值（Fallback Static ） Resilience4j [1]\r#\r断路器（Circuit Breaker） 重试（Retry） 限时器（Time Limiter） 限流器（Rate Limiter） 隔板（BulkHead） 参考\r#\rHystrix\r#\r微服务熔断与隔离 楚岩 Hystrix技术解析 王新栋 \u0026laquo;亿级流量网站架构核心技术\u0026raquo; 5.8节 张开涛 Hystrix 使用与分析 zhangyijun Resilience4j\r#\rResilience4j 比 Hystrix 好在哪里？ "},{"id":87,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/throttle/ratelimit/","title":"限流-总结","section":"限流","content":"\n限流总结\r#\r限流算法\r#\r流控算法 原理 实现 实现复杂度 优势 缺点 计数器法 简单 缺点 临界问题,不能应对突发请求 滑动窗口 滑动时间窗口划成了多格，粒度细; 解决了计数器法的缺点; 基于时间窗口[5] 简单 令牌桶算法 Guava RateLimiter [7] 复杂 能够处理突发请求; 允许某些流量的突发，被业界采用地较多 漏桶算法 漏桶算法[6] 代码[0] 简单 队列算法 FIFO队列; 权重队列; Linux tc 队列长度很关键 分布式限流\r#\r分布式计数器\n实现 Redis(服务端)+Lua(客户端) 限流网关\n缺陷 服务之间的调用不一定走网关 参考\r#\r漏桶算法实现 限流系统如何发现系统的热点 中间件小哥 *** 接口限流算法总结 夜有所思，日有所梦 聊聊高并发系统之限流特技 张开涛 服务化体系之－限流 江南白衣 失效 《应用 6：断尾求生 —— 简单限流 》 Redis 深度历险：核心原理与应用实践 《应用 7：一毛不拔 —— 漏斗限流》 Redis 深度历险：核心原理与应用实践 有代码实现 Guava RateLimiter源码解析 林舍 manerfan 淘宝应用柔性架构的探索 自适应负载调节 "},{"id":88,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/faultTolerant/soaTimeout/","title":"超时和重试 总结","section":"容错","content":"\n关键词： 超时, 降级, 重试\n超时 和 重试\r#\r超时和延迟的原因: 服务端获得请求了，但超时了; 服务端没获得请求，请求失败了， 超时了; 重试方式: 指数级退避 Exponential Blackoff[3][4] 超时类型\r#\r类型 客户端调用超时 服务器端调用超时 提供端/消费端与注册中心之间超时 超时后策略\r#\r超时后策略 超时+快速失败 超时后不重试 超时+降级failback 返回托底（返回历史数据/静态数据/缓存数据）数据，等待页或者错误页 超时+熔断 超时后重试，重试不行后熔断服务 降级\r#\r降级 非核心服务在超时后可以自动降级 超时时间和超时重试次数 最佳实践\r#\r最佳实践 不设置超时 慢请求累积导致连锁反应，甚至造成应用雪崩 推荐值 稍大于压测或者线上监控看到的TP99的响应时间 超时时间太长-不恰当设置 导致本应成功的调用却失败了。超时的时候服务资源没有释放 超时时间太短- 不恰当设置 服务调用成功率降低 最重要：网络连接/读/写的超时 用户能忍受的最长超时时间 - 用户体验 最坏情况下的响应时间=重试次数*单次超时时间 依赖 service重启时大量超时的问题 服务预热功能。延迟发布 客户端的超时时间\u0026lt;服务端的超时 可以在服务端实施限流/降级 多级依赖关系 如A调用B，B调用C 超时设置应该是A\u0026gt;B\u0026gt;C,否则可能会一直重试，引起DDos攻击效果 重试\r#\r重试 客户端 心跳超时 关闭链路，然后由客户端发起重新连接的操作，保证链路能恢复到正常的状态 有负载均衡的中间件，要考虑配置心跳/存活检查 客户端调用超时 重试策略,保证服务调用成功 - failover 摘掉不存活节点 尝试其他分组服务 尝试其他机房服务 服务端 读服务天然适合重试 写服务大多不能重试 幂等 Zookeeper客户端会话超时 - 服务注册反注册 Zookeeper服务端，将该会话对应的Node删除，删除事件通知到所有监听该Node的消费者 重试次数太多 导致多倍请求流量，模拟了DDos攻击 参考\r#\r《亿级流量网站架构核心技术》 张开涛 Hedwig文档 网络重试中的 指数退避抖动 算法 Exponential Backoff And Jitter AWS 中的错误重试和指数退避 "},{"id":89,"href":"/www6vMiddleware/docs/serviceGovernance/threadModel/jsfThreadModel/","title":"京东服务框架JSF服务提供者线程模型","section":"线程模型","content":"\n京东服务框架JSF\r#\rJSF是京东基础架构组的服务化中间件产品，全名是Jingdong Service Framework（中文名：杰夫）。 JSF整体是依据netty来构建的，本文从代码层面简单介绍一下JSF服务端的线程模型。\n1.JSF的服务端线程模型整体上是 boss线程池 + worker线程池 + 业务线程池。boss线程池和worker线程池称为Reactor线程池。\r#\r三类线程池各自的参数详见下图1。\nworker线程池和业务线程池之间的关系详见下图2，在图中可以看到业务线程和worker线程是解耦的，请求放入业务线程后，IO线程即worker线程就返回了，业务线程和I/O线程隔离。 在没有解耦IO线程和业务ChannelHandler的情况时，如果在业务ChannelHandler中进行数据库等同步I/O操作，很有可能会导致IO线程中的pipeline链路被阻塞。\n2. 图3是boss线程池， 线程数为Max(4,cpu/2)，用户不可以配置\r#\r3. 图4是worker线程池， 线程数为Max(8,cpu+1)，用户可以配置\r#\r4.图5和图6是业务线程池的构建，cached线程池大小是20-200，默认queue的大小是0。 任务来了直接分配线程，直到线程池满，得不到执行线程抛异常。\r#\r图7中一个服务端口对应一个业务线程池。\n5. 在ChannelPipeline中ServerHandler根据服务端的配置获取对应的业务线程池，然后在ServerHandler的handlerRequest中提交业务任务，默认的任务是JSFTask。\r#\r具体实现如图8,9,10.\n可以看到，JSF服务提供者线程模型整体还是按照boss+worker+biz这种netty官方推荐的方式构建的。\n参考：\r#\r京东 jsf 源代码和文档 Netty案例集锦之多线程篇（续） 李林锋 "},{"id":90,"href":"/www6vMiddleware/docs/serviceGovernance/faultTolerant/faultTolerant/soaTolerate/","title":"分布式服务框架 容错机制","section":"容错","content":"\n关键词： 容错, 降级, 隔离, 超时, 重试, 高可用, 监控, 开关\nOverview\r#\r超时重试机制[self 1][self 2] 限流 熔断机制 隔离 降级（本地缓存） 流量调度、负载均衡 微服务熔断与隔离 降级\r#\r降级策略 场景 实现 降低一致性约束 [1] 关闭非核心业务 人工开关 （非核心服务）, 强制降级,简化功能 开关存放位置:配置文件,数据库, Redis/Zk 关闭非核心业务 自动开关(非核心服务), 超时降级 1. 统计失败次数降级-不稳定的api\n2. 限流降级-大促秒杀\n3. 实现-熔断器 超时和重试 Retry\r#\r超时和重试 网络连接/读/写的超时时间（重要） 服务 读服务 - 可重试 写服务 - 幂等可重试 服务 客户端超时与重试 服务端超时 - 业务超时 任务型 服务调用型 超时后策略 failover 其它分组 其它机房 failcache 托底默认数据 等待页/错误页 降级 超时时间 太短 调用成功率降低 太长 后续正常请求挤压 经验值 稍微大于tp99的响应时间 集群容错\r#\r集群容错 Fail over（重试其他节点） 超时异常 Fail fast（快速失败） Fail cache（重试故障节点）（hedwig） 网路异常 Fail back（回退） 隔离 BulkHead\r#\r隔离 线程池隔离(hystirx) vm隔离（资源隔离） 物理机隔离 集群隔离 分组隔离 机房隔离 框架\r#\r框架 淘宝Dubbo 一号店Hedwig 京东JSF 点评pegion 唯品会OSP 状态监测\r#\r状态监测 服务注册中心状态监测（hedwig） 服务提供者和消费者之间的链路有效性检测（pegion） 服务健康检查（打分） 反推回消费者的路由表 流量控制（算法） Rate Limiter\r#\r流量控制（算法）　限流算法 令牌桶（控制入口） 漏桶（控制出口） 计数器(hedwig) 接口的总请求数（hedwig客户端） 接口的时间窗口请求数（hedwig服务端） 平滑限流,整形（netty） 整体流控 静态流控(整体qps固定) 预先分配 动态配额分配置（推） 动态配额申请制（拉） 动态流控 分级流控-拒绝流量 连接控制 并发控制（线程的并发执行数） 参考\r#\rself\r#\r{% post_link \u0026lsquo;soaTolerate\u0026rsquo; %} self {% post_link \u0026lsquo;soaTimeout\u0026rsquo; %} self "},{"id":91,"href":"/www6vMiddleware/docs/serviceGovernance/Overview/soaFeature/","title":"分布式服务框架功能","section":"Overview","content":"\n负载均衡 RR Least Connections Least Time “Power of Two Choices” 参考\r#\rNGINX and the “Power of Two Choices” Load-Balancing Algorithm 【直播回放】海量并发微服务框架设计 重要公式\n"},{"id":92,"href":"/www6vMiddleware/docs/serviceGovernance/Springcloud/springTransaction/","title":"Spring事务","section":"Spring Cloud","content":"\n三种事务模型\r#\r三种事务模型 本地事务模型 事务全部交给数据库来管理 编程式事务模型 事务的提交和回滚操作完全交给开发人员 TransactionTemplate TransactionCallback中执行业务代码 事务代码和业务代码可以实现分离的原理【1】 声明式事务模型【AOP】 事务的提交和回滚操作全部交给Spring来管理 事务拦截器TransactionInterceptor 事务管理器transactionManager【2】 事务配置的提供者transactionAttributes【3】\n业务方法+传播属性 代理的对象target\n业务对象 参考\r#\r分布式事务系列（1.1）Spring事务管理器PlatformTransactionManager 乒乓狂魔 分布式事务系列（1.2）Spring的事务体系 乒乓狂魔 "}]